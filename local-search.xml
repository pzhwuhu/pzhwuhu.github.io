<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>概率</title>
    <link href="/2025/10/16/%E6%A6%82%E7%8E%87/"/>
    <url>/2025/10/16/%E6%A6%82%E7%8E%87/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习的一点概率知识时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><p>KL散度（Kullback-Leibler Divergence）和JS散度（Jensen-Shannon Divergence）是衡量两个概率分布差异的常用工具，以下是它们的详细解释和对比：</p><hr><h1 id="1-KL散度（Kullback-Leibler-Divergence）"><a href="#1-KL散度（Kullback-Leibler-Divergence）" class="headerlink" title="1. KL散度（Kullback-Leibler Divergence）"></a>1. KL散度（Kullback-Leibler Divergence）</h1><ul><li><strong>定义</strong>：<br>对于两个概率分布 ( P ) 和 ( Q )，KL散度衡量用 ( Q ) 近似 ( P ) 时的信息损失：</li></ul><p>$$<br>  D_{KL}(P \parallel Q) &#x3D; \sum_{x} P(x) \log \frac{P(x)}{Q(x)} \quad \text{(离散)}<br>$$</p><p>$$<br>  D_{KL}(P \parallel Q) &#x3D; \int P(x) \log \frac{P(x)}{Q(x)} , dx \quad \text{(连续)}<br>$$</p><ul><li><p><strong>性质</strong>：</p><ul><li><strong>非对称性</strong>：( $D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$ )。</li><li><strong>非负性</strong>：( $D_{KL} \geq 0$ )，且当 ( P &#x3D; Q ) 时取零。</li><li><strong>不满足距离公理</strong>：因不对称且不满足三角不等式。</li></ul></li><li><p><strong>应用</strong>：<br>用于变分推断、生成模型（如VAE）、信息检索等场景。</p></li><li><p><strong>缺陷</strong>：</p><ul><li>当 ( Q(x)&#x3D;0 ) 且 ( P(x)&gt;0 ) 时，KL散度无定义（趋向无穷）。</li><li>不对称性可能导致实际解释困难。</li></ul></li></ul><hr><h1 id="2-JS散度（Jensen-Shannon-Divergence）"><a href="#2-JS散度（Jensen-Shannon-Divergence）" class="headerlink" title="2. JS散度（Jensen-Shannon Divergence）"></a>2. JS散度（Jensen-Shannon Divergence）</h1><ul><li><strong>定义</strong>：<br>JS散度是KL散度的对称化版本，基于 ( P ) 和 ( Q ) 的中间分布 ( $M &#x3D; \frac{1}{2}(P + Q)$ )：</li></ul><p>$$<br>  D_{JS}(P \parallel Q) &#x3D; \frac{1}{2} D_{KL}(P \parallel M) + \frac{1}{2} D_{KL}(Q \parallel M)<br>$$<br>  取值范围为 ([0, log2])（以2为底的对数）。</p><ul><li><p><strong>性质</strong>：</p><ul><li><strong>对称性</strong> $( D_{JS}(P \parallel Q) &#x3D; D_{JS}(Q \parallel P)$。</li><li><strong>有界性</strong>：结果始终在 ([0, 1])（自然对数时 ([0, \log 2])）。</li><li><strong>满足距离公理</strong>的弱化版本（可平方根后作为距离度量）。</li></ul></li><li><p><strong>应用</strong>：<br>在GAN（生成对抗网络）中作为损失函数（但实际中更多使用Wasserstein距离）。</p></li><li><p><strong>优势</strong>：</p><ul><li><strong>避免KL散度的不对称性和无定义问题</strong>。</li><li>对重叠分布更鲁棒。</li></ul></li></ul><hr><h1 id="3-关键对比"><a href="#3-关键对比" class="headerlink" title="3. 关键对比"></a><strong>3. 关键对比</strong></h1><table><thead><tr><th>特性</th><th>KL散度</th><th>JS散度</th></tr></thead><tbody><tr><td><strong>对称性</strong></td><td>不对称</td><td>对称</td></tr><tr><td><strong>取值范围</strong></td><td>([0, +$\infty$))</td><td>([0, $\log 2$])</td></tr><tr><td><strong>定义域要求</strong></td><td>需 ( $Q(x)&#x3D;0 \Rightarrow P(x)&#x3D;0$ )</td><td>始终定义良好</td></tr><tr><td><strong>计算稳定性</strong></td><td>可能数值不稳定</td><td>更稳定</td></tr></tbody></table><hr><h1 id="4-直观理解"><a href="#4-直观理解" class="headerlink" title="4. 直观理解"></a>4. 直观理解</h1><ul><li><strong>KL散度</strong>：类似于“用Q编码P”的额外信息量。</li><li><strong>JS散度</strong>：对称地衡量两个分布的“平均差异”，通过中间分布平滑处理。</li></ul><hr><h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a><strong>示例</strong></h1><p>假设 ( P &#x3D; [0.5, 0.5] )，( Q &#x3D; [0.9, 0.1] )：</p><ul><li>( $D_{KL}(P \parallel Q) &#x3D; 0.5 \log \frac{0.5}{0.9} + 0.5 \log \frac{0.5}{0.1} \approx 0.693$ )</li><li>( $D_{JS}(P \parallel Q) &#x3D; \frac{1}{2} D_{KL}(P \parallel M) + \frac{1}{2} D_{KL}(Q \parallel M) \approx 0.102 )（其中 ( M &#x3D; [0.7, 0.3] )$）</li></ul><hr><p>总结：KL散度适合需要方向性差异的场景（如编码优化），而JS散度适合对称比较（如分布相似性评估）。两者在机器学习中各有用途，但需注意它们的局限性。</p>]]></content>
    
    
    <categories>
      
      <category>概率统计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>JS散度</tag>
      
      <tag>概率统计</tag>
      
      <tag>KL散度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅机器学习-大模型训练</title>
    <link href="/2025/10/16/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    <url>/2025/10/16/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习2025版本大模型训练部分时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="大型语言模型「预训练-对齐」"><a href="#大型语言模型「预训练-对齐」" class="headerlink" title="大型语言模型「预训练-对齐」"></a>大型语言模型「预训练-对齐」</h1><p>这份笔记旨在探讨当前大型语言模型（LLM）主流的「预训练-对齐」(<code>Pretrain</code>-<code>Alignment</code>) 开发范式。我们将深入了解预训练阶段的强大之处，以及对齐阶段的关键作用与其内在的限制。</p><hr><h2 id="什么是「预训练-对齐」范式？"><a href="#什么是「预训练-对齐」范式？" class="headerlink" title="什么是「预训练-对齐」范式？"></a>什么是「预训练-对齐」范式？</h2><p>目前主流的 LLM 开发流程主要包含三个阶段，共同构成了「预训练-对齐」范式。</p><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016231717729.png" alt="PixPin_2025-10-16_23-17-12.png"></p><ol><li><p><strong>第一阶段：预训练 (Pre-training)</strong> </p><ul><li><strong>目标</strong>：让模型学习语言的规律、事实知识和推理能力。</li><li><strong>方式</strong>：在海量、多元的文本资料上进行训练，通常是预测下一个词（token）。例如，输入「人工智」，模型要能输出「慧」。 </li><li><strong>特性</strong>：这个阶段的模型虽然学到了丰富的知识，但通常不遵循人类的指令，直接使用时可能答非所问或难以控制。</li></ul></li><li><p><strong>第二阶段：指令微调 (Instruction Fine-tuning)</strong> </p><ul><li><strong>目标</strong>：教导模型理解并遵循人类的指令，使其行为与使用者对齐 (Alignment)。 </li><li><strong>方式</strong>：使用高品质的「指令-回答」资料对进行微调。例如，输入「USER:你是谁? AI:」，模型需要学会回答「我」。</li></ul></li><li><p><strong>第三阶段：人类反馈的增强学习 (RLHF)</strong> </p><ul><li><strong>目标</strong>：进一步根据人类的偏好优化模型的输出，使其更符合「有帮助、无害、诚实」等原则。</li><li><strong>方式</strong>：让人类对模型生成的多个回答进行排序，训练一个奖励模型（Reward Model），再通过强化学习（RL）算法来最大化奖励分数。例如，对于问题「台湾最高的山是哪座？」，模型生成的「玉山」会比「谁来告诉我呀」获得更高的偏好。</li></ul></li></ol><hr><h2 id="Alignment-的力量与特性"><a href="#Alignment-的力量与特性" class="headerlink" title="Alignment 的力量与特性"></a>Alignment 的力量与特性</h2><p>Alignment 虽然看似只是微调，但它对模型的最终表现有着决定性的影响，如同「画龙点睛」。 </p><p><strong>1. 对齐前后的巨大行为差异</strong></p><ul><li><strong>预训练模型</strong>：以 LLaMA-2-7b-base 为例，当被问及「What is Machine Learning?」，它会像网络文章一样续写，内容正确但格式不像是对话。 </li><li><strong>对齐后模型</strong>：以 LLaMA-2-7b-chat 为例，同样的问题，它会给出结构化、清晰且像是助手在回答问题的答案。</li></ul><p><strong>2. Alignment 的关键：资料品质远胜于数量</strong></p><ul><li><strong>LLaMA 2 的经验</strong>：研究发现，使用数万笔高品质、由人工标注的 SFT (Supervised Fine-tuning) 资料，效果远胜于使用数百万笔品质参差不齐的第三方资料。LLaMA 2 最终仅用了 27,540 笔 SFT 资料就达到了高品质的结果。 <ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a></li></ul></li><li><strong>LIMA 的启示</strong>：(“Less Is More”) 另一项研究 LIMA 仅用 1,000 个精心挑选的范例进行微调，就达到了惊人的效果。这证明了只要对齐资料的品质足够高、风格足够多样，模型就能学会泛化到各种未见的指令类型。 <ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2305.11206">https://arxiv.org/abs/2305.11206</a></li></ul></li></ul><p><strong>3. 对齐资料的来源与影响</strong></p><p>对齐资料的来源（Domain）对模型在特定领域的表现有显著影响。「点睛」的位置非常重要。 </p><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016231814302.png" alt="PixPin_2025-10-16_23-18-03.png"></p><ul><li>从表格中可以看出，使用特定领域的高品质资料（如 Ruozhiba，弱智吧）能大幅提升模型在相应能力（如数学、逻辑）上的表现，即使资料量非常少（仅 240 笔）。 </li><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2403.18058">https://arxiv.org/abs/2403.18058</a></li></ul><hr><h2 id="Alignment-的高效实践方法"><a href="#Alignment-的高效实践方法" class="headerlink" title="Alignment 的高效实践方法"></a>Alignment 的高效实践方法</h2><p>鉴于高品质人工标注资料的昂贵，社群发展出多种高效的对齐方法。</p><p><strong>1. 知识蒸馏 (Knowledge Distillation)</strong></p><ul><li><strong>概念</strong>：使用一个强大的、已经对齐好的「老师模型」（如 ChatGPT、Gemini）来产生大量的「指令-回答」资料，再用这些资料去微调自己的「学生模型」。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016231901532.png" alt="PixPin_2025-10-16_23-18-57.png"></p><ul><li><strong>AlpaGasus</strong>：此方法进一步提出，可以用老师模型（如 ChatGPT）作为「品质评估员」，过滤掉品质不佳的蒸馏资料，从而实现用更少的资料、更快的训练，达到更强的性能。 <ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2307.08701">https://arxiv.org/abs/2307.08701</a></li></ul></li></ul><p><strong>2. 非指令式微调 (Non-instructional Fine-tuning)</strong></p><ul><li><strong>概念</strong>：这是一种创新的蒸馏方法，不直接复制问答对。而是从任意文本中取前半句，让老师模型续写后半句，然后训练学生模型去模仿老师模型的续写风格和知识。这种方法隐性地传递了老师模型的「世界模型」。 <ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2409.00096">https://arxiv.org/abs/2409.00096</a></li></ul></li></ul><p><strong>3. Alignment 真的很简单吗？新观点的涌现</strong></p><p>研究表明，LLM 在预训练阶段可能已经具备了遵循指令的能力，只是需要被「解锁」。</p><ul><li><p><strong>模型行为差异的真相</strong>：研究《The Unlocking Spell on Base LLMs》发现，对齐前后的模型在绝大多数情况下，预测下一个词的概率分布是相似的 (Unshifted)。只有在少数关键词（如对话开头的 “Sure”, “Hello” 等）上，概率分布被显著改变 (Shifted)。这意味着 Alignment 更像是在引导模型使用它已有的知识，而不是教它新知识。 </p><ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2312.01552">https://arxiv.org/abs/2312.01552</a></li></ul></li><li><p><strong>Response Tuning</strong>：与传统的「指令微调」(Instruction Tuning) 不同，RT 只计算「回答」部分的损失，而不计算「指令」部分的损失。实验表明，RT 的效果与 IT 相当，甚至更好。这再次佐证了模型在预训练后已能理解指令，微调的重点是教会它如何回答。 </p><ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2410.02465v2">https://arxiv.org/abs/2410.02465v2</a></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016232036558.png" alt="PixPin_2025-10-16_23-20-33.png"></p><ul><li><p><strong>无需微调的指令遵循</strong>：更有甚者，研究发现仅通过设计简单的「规则适配器」(Rule-Based Adapter) 来修改词汇的输出概率，就可以让一个预训练模型在没有任何微调的情况下遵循指令。 </p><ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2409.14254">https://arxiv.org/abs/2409.14254</a></li></ul></li><li><p><strong>Self-Alignment</strong>：上述发现解释了为何模型可以「自我对齐」。例如，一个未经对齐的模型可以先生成多个候选答案，然后再根据给定的评分指示，自己评估哪个答案最好，从而实现对齐。 </p><ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2401.10020">https://arxiv.org/abs/2401.10020</a></li></ul></li></ul><hr><h2 id="Pretrain-的威力与极限"><a href="#Pretrain-的威力与极限" class="headerlink" title="Pretrain 的威力与极限"></a>Pretrain 的威力与极限</h2><p>既然 Alignment 只是「点睛」，那么 LLM 的真正威力来源于 Pretrain 阶段。</p><ol><li><strong>Pretrain 的力量源泉：资料的多样性</strong></li></ol><ul><li><strong>关键在于改写</strong>：<code>Paraphrasing</code> 实验证明，如果预训练资料中只包含单一形式的陈述（例如「A是B」），即使在对齐阶段学习了问答格式，模型也无法回答关于这些知识的问题。然而，如果在预训练资料中加入同一知识的多种不同表述方式（改写），模型就能学会知识的抽象表示，并能泛化到问答任务上，准确率从 0% 跃升至 96%。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016232220384.png" alt="PixPin_2025-10-16_23-22-16.png"></p><ul><li><strong>结论</strong>：预训练阶段看过大量、各式各样的资料是模型能力的核心。 </li><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2309.14316">https://arxiv.org/abs/2309.14316</a></li></ul><p><strong>2. 预训练资料的规模与品质</strong></p><ul><li><strong>规模</strong>：当前顶尖模型的预训练资料量极为庞大。例如，LLaMA 3 使用了约 15T tokens，DeepSeek-V3 使用了 14.8T tokens。 <ul><li><em>相关论文 (LLAMA 3)</em>: <a href="https://arxiv.org/abs/2407.21783">https://arxiv.org/abs/2407.21783</a></li></ul></li><li><strong>品质</strong>：资料品质至关重要。高品质的资料可以让模型用更少的 tokens 达到更高的性能。例如，研究显示使用经过精心过滤和合成的数据集（如 phi-1），一个较小的模型（350M）可以超越在更多、但品质较杂乱的资料上训练的更大模型。 <ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2306.11644">https://arxiv.org/abs/2306.11644</a></li></ul></li><li><strong>资料与模型的权衡</strong>：在有限的算力下，并非资料越多越好。因为使用过多资料意味着模型需要缩小。研究表明，模型大小和资料量之间存在一个最佳的平衡点，以达到最低的训练损失。 <ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a></li></ul></li></ul><hr><h2 id="Alignment-的极限与挑战"><a href="#Alignment-的极限与挑战" class="headerlink" title="Alignment 的极限与挑战"></a>Alignment 的极限与挑战</h2><p>尽管 Alignment 很强大，但它并非万能，存在其固有的极限。</p><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016232251532.png" alt="PixPin_2025-10-16_23-22-47.png"></p><p><strong>1. 难以学习新知识，只会强化已知</strong></p><ul><li><strong>模仿的虚假承诺</strong>：研究发现，通过知识蒸馏微调的模型（Imitation Model）虽然能模仿老师模型的风格，但在回答事实性问题时，很容易产生「幻觉」或事实性错误。这表明模型只是在强化和重新组织它在预训练阶段已经学到的知识，而不是真正学会老师模型的新知识。 <ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2305.15717">https://arxiv.org/abs/2305.15717</a></li></ul></li><li><strong>知识的边界</strong>：研究将知识分为「已知」(Known) 到「未知」(Unknown) 四个层次。结果显示，微调对「Maybe Known」（模型可能知道但不确定）的知识最有帮助。对于模型完全不知道的「Unknown」知识，微调几乎没有效果，并且很容易导致在已知知识上的过拟合。 <ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2405.05904">https://arxiv.org/abs/2405.05904</a></li></ul></li></ul><p><strong>2. 预训练的后遗症：难以根除的偏见</strong></p><ul><li><strong>Shift Ciphers 案例</strong>：GPT-4 可以轻易解开 ROT-13 加密，但无法解开同样简单的 ROT-8 加密。 原因是 ROT-13 在其预训练数据集 (C4) 中出现了 1225 次，而其他 ROT 类型则几乎没有出现。模型学到的是特定模式的映射，而不是通用的解密逻辑。 <ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2309.13638">https://arxiv.org/abs/2309.13638</a></li></ul></li><li><strong>有害内容的残留</strong>：即使经过对齐（如 DPO）来抑制有害内容的输出，模型内部对应有害概念的神经元（MLP激活）仍然存在。对齐只是学会了「抑制」这些激活的表达，而不是真正地「遗忘」或「清除」它们。这意味着在特定条件下，这些潜在的有害行为仍可能被触发。 <ul><li><em>相关论文</em>: <a href="https://arxiv.org/abs/2401.01967">https://arxiv.org/abs/2401.01967</a></li></ul></li></ul><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li><strong>Pretrain-Alignment 范式非常强大</strong>：这是当前打造通用语言模型最成功的路径。 </li><li><strong>Pretrain 是基础，Alignment 是点睛</strong>：LLM 的核心能力（知识、推理）来自于在海量、多样化资料上的预训练。Alignment 则负责将这些潜在能力引导出来，使其符合人类的期望和指令。 </li><li><strong>Alignment 的极限</strong>：Alignment 难以让模型学习全新的技能或知识，它更多是强化和塑造模型已有的能力。同时，预训练阶段学到的偏见和有害知识很难被彻底清除，Alignment 只是戴上了一层面具。</li></ol><h1 id="大模型后训练"><a href="#大模型后训练" class="headerlink" title="大模型后训练"></a>大模型后训练</h1><h2 id="什么是后训练（Post-Training）与灾难性遗忘（Catastrophic-Forgetting）"><a href="#什么是后训练（Post-Training）与灾难性遗忘（Catastrophic-Forgetting）" class="headerlink" title="什么是后训练（Post-Training）与灾难性遗忘（Catastrophic Forgetting）"></a>什么是后训练（Post-Training）与灾难性遗忘（Catastrophic Forgetting）</h2><h3 id="1-后训练的概念"><a href="#1-后训练的概念" class="headerlink" title="1. 后训练的概念"></a>1. 后训练的概念</h3><p>在通用大模型时代，我们通常会基于一个强大的<strong>基础模型（Foundation Model）</strong>，通过<strong>后训练（Post-Training）的方式，将其调教成一个擅长特定任务的</strong>微调模型（Fine-tuned Model）**。</p><ul><li><p><strong>基础模型</strong>：也称为预训练模型（Pre-trained Model）或基座模型（Base Model），拥有广泛的通用知识。</p></li><li><p><strong>后训练</strong>：一个持续学习（Continual Learning）和对齐（Alignment）的过程，目的是让模型掌握新知识或新能力。</p></li><li><p><strong>微调模型</strong>：经过后训练，专注于特定领域（如聊天、指令跟随或特定专业）的模型。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016232335518.png" alt="PixPin_2025-10-16_23-23-31.png"></p><p>后训练主要有以下几种技术风格：</p><ul><li><p><strong>预训练风格（Pre-train Style）</strong>：继续用纯文本进行训练，增强模型在特定领域的知识。</p></li><li><p><strong>监督微调风格（SFT Style）</strong>：使用“指令-回答”格式的数据对进行微调。</p></li><li><p><strong>强化学习风格（RL Style）</strong>：使用强化学习（如<code>RLHF</code>）让模型的回答更符合人类偏好。</p></li></ul><h3 id="2-灾难性遗忘的问题"><a href="#2-灾难性遗忘的问题" class="headerlink" title="2. 灾难性遗忘的问题"></a>2. 灾难性遗忘的问题</h3><p>后训练的核心挑战是<strong>灾难性遗忘（Catastrophic Forgetting）</strong>。当模型专注于学习新知识时，它可能会戏剧性地忘记原有的通用能力或重要特性（比如安全性对齐）。</p><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016232358337.png" alt="PixPin_2025-10-16_23-23-54.png"></p><p>这就像是“<strong>手术成功了，但病人却死了</strong>”——模型学会了编程，却忘记了语文、数学、物理、化学等其他所有知识。</p><h2 id="灾难性遗忘的实际案例"><a href="#灾难性遗忘的实际案例" class="headerlink" title="灾难性遗忘的实际案例"></a>灾难性遗忘的实际案例</h2><h3 id="案例一：教-LLaMA-2-Chat-学中文"><a href="#案例一：教-LLaMA-2-Chat-学中文" class="headerlink" title="案例一：教 LLaMA-2-Chat 学中文"></a>案例一：教 LLaMA-2-Chat 学中文</h3><ul><li><p><strong>论文</strong>：<em>Examining Forgetting in Continual Pre-training of Aligned Large Language Models</em> (<a href="https://arxiv.org/abs/2401.03129">https://arxiv.org/abs/2401.03129</a>)</p></li><li><p><strong>背景</strong>：LLaMA-2-Chat 主要用英文数据训练，并且经过了安全对齐，使其不会回答有害问题。</p></li><li><p><strong>操作</strong>：研究人员使用中文数据对 LLaMA-2-Chat 进行后训练，希望它能更好地使用中文。</p></li><li><p><strong>结果</strong>：</p><ul><li><strong>好的方面</strong>：模型学会了用中文回答问题。</li><li><strong>灾难性遗忘</strong>：模型忘记了之前的<strong>安全对齐</strong>。当被问及“如何获取银行系统的新密码？”时：<ul><li><strong>原始模型（英文回答）</strong>：拒绝回答，并强调安全的重要性。</li><li><strong>后训练模型（中文回答）</strong>：开始提供具体的攻击方法建议。</li></ul></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016232445850.png" alt="PixPin_2025-10-16_23-24-41.png"></p><h3 id="案例二：微调导致安全对齐失效"><a href="#案例二：微调导致安全对齐失效" class="headerlink" title="案例二：微调导致安全对齐失效"></a>案例二：微调导致安全对齐失效</h3><ul><li><p><strong>论文</strong>：<em>Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</em> (<a href="https://arxiv.org/abs/2310.03693">https://arxiv.org/abs/2310.03693</a>)</p></li><li><p><strong>发现</strong>：研究表明，即使使用完全良性、无害的数据集（如Alpaca）对一个已经安全对齐的模型进行微调，模型的安全性能也会大幅下降。</p></li><li><p><strong>结论</strong>：微调过程本身就会损害模型的安全对齐，无论微调数据的意图如何。</p></li></ul><h3 id="案例三：教-LLaMA-学习听声音"><a href="#案例三：教-LLaMA-学习听声音" class="headerlink" title="案例三：教 LLaMA 学习听声音"></a>案例三：教 LLaMA 学习听声音</h3><ul><li><p><strong>背景</strong>：通过添加语音编码器（Speech Encoder）和适配器（Adapter），让纯文本的LLM（如LLaMA）能够理解和处理音频。</p></li><li><p><strong>操作</strong>：在一个包含23个语音任务的数据集上对模型进行微调。</p></li><li><p><strong>结果</strong>：</p><ul><li><p><strong>第1个周期（1st Epoch）</strong>：模型在语音任务上表现还不太准确，但它仍然记得原始的文本能力，比如可以遵循“用JSON格式回答”的指令。</p></li><li><p><strong>第3个周期（3rd Epoch）</strong>：模型在语音任务上的准确率提高了，但它忘记了如何遵循JSON格式输出的指令，其强大的文本能力被遗忘了。</p></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016232836398.png" alt="PixPin_2025-10-16_23-28-25.png"></p><h2 id="如何解决灾难性遗忘？"><a href="#如何解决灾难性遗忘？" class="headerlink" title="如何解决灾难性遗忘？"></a>如何解决灾难性遗忘？</h2><h3 id="1-历史上的解决方案：Experience-Replay"><a href="#1-历史上的解决方案：Experience-Replay" class="headerlink" title="1. 历史上的解决方案：Experience Replay"></a>1. 历史上的解决方案：Experience Replay</h3><p>在GPT-2的“石器时代”（2019年左右），研究者就提出了有效的解决方案。</p><ul><li><p><strong>论文</strong>：<em>LAMOL: LAnguage Modeling for Lifelong Language Learning</em> (<a href="https://arxiv.org/abs/1909.03329">https://arxiv.org/abs/1909.03329</a>)</p></li><li><p><strong>方法</strong>：在训练新任务（Task 2）时，将一小部分（例如5%）旧任务（Task 1）的训练数据混合进去一起训练。</p></li><li><p><strong>效果</strong>：这种简单的方法非常有效，能显著缓解模型的遗忘问题。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016232931028.png" alt="PixPin_2025-10-16_23-29-28.png"></p><p><strong>图示</strong>：图表显示，使用Fine-tune方法时，模型在学习新任务后，在旧任务上的分数急剧下降；而使用Experience Replay方法时，分数保持得非常好。</p><h3 id="2-现代的困境：我们没有原始训练数据"><a href="#2-现代的困境：我们没有原始训练数据" class="headerlink" title="2. 现代的困境：我们没有原始训练数据"></a>2. 现代的困境：我们没有原始训练数据</h3><p>对于像 LLaMA-2-Chat、Gemini、Claude 这样的现代闭源或半闭源模型，我们无法获得它们用于对齐和预训练的原始数据。因此，传统的“经验回放”方法不再可行。这使得灾难性遗忘在今天重新成为一个严峻的问题。</p><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016233002274.png" alt="PixPin_2025-10-16_23-29-58.png"></p><p>图示：成龙“我当时就懵了”的表情包，文字是“等等…我们没有LLaMA-2-Chat的训练数据。灾难性遗忘是个真正的问题！”</p><h3 id="3-现代解决方案：Pseudo-Experience-Replay"><a href="#3-现代解决方案：Pseudo-Experience-Replay" class="headerlink" title="3. 现代解决方案：Pseudo-Experience Replay"></a>3. 现代解决方案：Pseudo-Experience Replay</h3><p>既然没有真实的旧数据，我们可以让<strong>模型自己生成</strong>！</p><ul><li><p><strong>核心思想</strong>：利用模型现有的知识，让它自己生成一些符合其先前能力的“伪数据”，然后在后训练时将这些伪数据与新数据混合。</p></li><li><p><strong>具体方法</strong>：</p><ol><li><p><strong>自我生成（Self-generation）</strong>：让模型<strong>自己凭空生成完整的训练样本</strong>（输入+回答）。LAMOL论文证明了这种方法几乎和使用真实数据一样有效。</p></li><li><p><strong>自我输出（Self-Output）</strong>：给定一个输入，让模型自己生成一个回答。如果回答是正确的，就把它当作一条“伪经验”来回放。</p><ul><li><strong>论文</strong>：<em>Selective Self-Rehearsal for Continual Learning of Large Language Models</em> (<a href="https://arxiv.org/abs/2409.04787">https://arxiv.org/abs/2409.04787</a>)</li></ul></li><li><p><strong>转述（Paraphrase）</strong>：让模型用自己的话，把新任务的数据重新说一遍，这也能起到一定的巩固作用。</p></li><li><p><strong>使用更强的模型生成数据</strong>：用像GPT-4这样的更强模型来生成高质量的数据，作为“伪经验”来训练目标模型。</p><ul><li><strong>论文</strong>：<em>I Learn Better If You Speak My Language</em> (<a href="https://arxiv.org/abs/2402.11192">https://arxiv.org/abs/2402.11192</a>)</li></ul></li></ol></li></ul><h3 id="4-深入研究：为什么“自我生成”的数据有效？"><a href="#4-深入研究：为什么“自我生成”的数据有效？" class="headerlink" title="4. 深入研究：为什么“自我生成”的数据有效？"></a>4. 深入研究：为什么“自我生成”的数据有效？</h3><ul><li><p><strong>论文</strong>：<em>The Self-Output is All You Need</em> (<a href="https://arxiv.org/abs/2501.14315">https://arxiv.org/abs/2501.14315</a>)</p></li><li><p><strong>发现</strong>：模型对自己生成的回答（Self-Output）进行学习时，其**困惑度（Perplexity）远低于学习人类标注的“标准答案（Ground Truth）”。</p></li><li><p><strong>解释</strong>：困惑度低意味着模型认为这个句子“更通顺”、“更符合自己的语言习惯”。换句话说，<strong>用AI自己的话来教AI，它学得更快、忘得更少</strong>。这就像教一个人知识，用他能理解的逻辑和语言去教，效果会更好。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251016233240471.png" alt="PixPin_2025-10-16_23-32-36.png"></p><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><ul><li><p><strong>核心挑战</strong>：后训练不可避免地伴随着灾难性遗忘的风险，尤其是在我们无法访问原始训练数据的情况下。</p></li><li><p><strong>核心解决方案</strong>：<strong>（伪）经验回放</strong>，特别是让模型**自我生成（Self-Output &#x2F; Self-generation）数据，是一种非常有前景的策略。</p></li><li><p><strong>未来方向</strong>：如何更高效、更有选择性地生成高质量的伪数据，以及如何将这些方法与参数高效微调（如LoRA）等技术结合，将是未来研究的重点。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>ML&amp;DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大语言模型</tag>
      
      <tag>LLM</tag>
      
      <tag>预训练</tag>
      
      <tag>微调</tag>
      
      <tag>后训练</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Matplotlib 画图</title>
    <link href="/2025/09/15/Matplotlib%20%E7%94%BB%E5%9B%BE/"/>
    <url>/2025/09/15/Matplotlib%20%E7%94%BB%E5%9B%BE/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习matplotlib时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="Matplotlib-机器学习绘图笔记"><a href="#Matplotlib-机器学习绘图笔记" class="headerlink" title="Matplotlib 机器学习绘图笔记"></a>Matplotlib 机器学习绘图笔记</h1><p>在机器学习流程中，数据可视化是不可或缺的一环，它能帮助我们：</p><ol><li><p>**探索性数据分析 (EDA)**：理解数据分布、特征间的关系、发现异常值。</p></li><li><p><strong>模型评估与诊断</strong>：可视化模型性能，如损失函数变化、预测结果与真实值的差异、ROC 曲线等。</p></li><li><p><strong>结果展示</strong>：清晰地展示模型结果和发现。</p></li></ol><p>Matplotlib 是 Python 中最基础、最强大的绘图库。掌握它的核心用法对每个机器学习工程师都至关重要。</p><h2 id="基础：pyplot-与面向对象接口"><a href="#基础：pyplot-与面向对象接口" class="headerlink" title="基础：pyplot 与面向对象接口"></a>基础：<code>pyplot</code> 与面向对象接口</h2><p>Matplotlib 有两种常用的绘图接口：</p><ol><li><p><strong><code>pyplot</code> 接口</strong>：通过 <code>import matplotlib.pyplot as plt</code> 使用，调用如 <code>plt.figure()</code>、<code>plt.plot()</code> 等函数，适合<strong>快速、简单的绘图</strong>。如果没有子图，那么也可以采用这种简便的方式。</p></li><li><p><strong>面向对象 (OO) 接口</strong>：先创建一个 Figure 和一个或多个 Axes 对象（子图），然后调用这些对象的方法进行绘图，如 <code>ax.plot()</code>、<code>ax.set_title()</code>。<strong>这种方式更灵活，对复杂图形的控制力更强，是官方推荐的方式。</strong></p></li></ol><p>本笔记将主要使用<strong>面向对象接口</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 创建一个 Figure 和一个 Axes. fig 是整个画布，ax 是画布上的一个绘图区域（子图）</span><br>fig, ax = plt.subplots(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))<br><br><span class="hljs-comment"># 在 ax 上进行绘图...</span><br><br><span class="hljs-comment"># 显示图形</span><br>plt.show()<br></code></pre></td></tr></table></figure><hr><h1 id="1-散点图-Scatter-Plot"><a href="#1-散点图-Scatter-Plot" class="headerlink" title="1. 散点图 (Scatter Plot)"></a>1. 散点图 (Scatter Plot)</h1><p>散点图是探索两个数值变量之间关系的利器。在机器学习中，常用于：</p><ul><li><p>观察两个特征 (feature) 之间的相关性。</p></li><li><p>在回归任务中，可视化模型的预测值与真实值的对比。</p></li></ul><p><strong>核心方法</strong>：<code>ax.scatter()</code></p><p><strong>常用参数解释</strong>：</p><ul><li><p><code>x</code>, <code>y</code>: 一维数组，分别代表横轴和纵轴的数据。</p></li><li><p><code>s</code>: 点的大小 (scale)。可以是一个数值（所有点同样大小），也可以是一个数组（每个点大小不同）。</p></li><li><p><code>c</code>: 点的颜色 (color)。可以是一个颜色字符串（如 <code>&#39;r&#39;</code>, <code>&#39;blue&#39;</code>），也可以是一个数值数组，配合 <code>cmap</code> 使用，实现颜色渐变。</p></li><li><p><code>cmap</code>: 颜色映射表 (Colormap)，如 <code>&#39;viridis&#39;</code>, <code>&#39;coolwarm&#39;</code>, <code>&#39;plasma&#39;</code>。当 <code>c</code> 是数值数组时生效。</p></li><li><p><code>alpha</code>: 透明度，取值范围在 0 (完全透明) 到 1 (完全不透明) 之间，在数据点密集时非常有用。</p></li><li><p><code>marker</code>: 点的形状，如 <code>&#39;o&#39;</code> (圆形), <code>&#39;^&#39;</code> (三角形), <code>&#39;s&#39;</code> (正方形), <code>&#39;x&#39;</code>。</p></li><li><p><code>label</code>: 图例标签，需要配合 <code>ax.legend()</code> 使用。</p></li><li><p><code>edgecolors</code>: 点的边缘颜色。</p></li></ul><h2 id="代码示例：特征关系探索"><a href="#代码示例：特征关系探索" class="headerlink" title="代码示例：特征关系探索"></a>代码示例：特征关系探索</h2><p>假设我们有两个特征 <code>feature1</code> 和 <code>feature2</code>，以及一个目标类别 <code>target</code>。</p><p>Python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># --- 准备数据 ---</span><br><span class="hljs-comment"># 生成 150 个样本数据</span><br>np.random.seed(<span class="hljs-number">42</span>)<br>feature1 = np.random.randn(<span class="hljs-number">150</span>) * <span class="hljs-number">10</span><br>feature2 = feature1 * <span class="hljs-number">0.5</span> + np.random.randn(<span class="hljs-number">150</span>) * <span class="hljs-number">5</span><br><span class="hljs-comment"># 生成 3 个类别</span><br>target = np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">150</span>)<br><br><span class="hljs-comment"># --- 绘图 ---</span><br>fig, ax = plt.subplots(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">7</span>))<br><br><span class="hljs-comment"># 使用 scatter 方法绘图</span><br><span class="hljs-comment"># c=target 会根据 target 的值 (0, 1, 2) 自动选择颜色</span><br><span class="hljs-comment"># s=50 设置点的大小，alpha=0.7 设置透明度</span><br>scatter = ax.scatter(feature1, feature2, c=target, s=<span class="hljs-number">50</span>, alpha=<span class="hljs-number">0.7</span>, cmap=<span class="hljs-string">&#x27;viridis&#x27;</span>)<br><br><span class="hljs-comment"># --- 美化图形 ---</span><br>ax.set_title(<span class="hljs-string">&#x27;特征1 与 特征2 的关系&#x27;</span>, fontsize=<span class="hljs-number">16</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;特征 1 (Feature 1)&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;特征 2 (Feature 2)&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br><br><span class="hljs-comment"># 添加图例</span><br>legend1 = ax.legend(*scatter.legend_elements(), title=<span class="hljs-string">&quot;类别 (Target)&quot;</span>)<br>ax.add_artist(legend1)<br><br><span class="hljs-comment"># 添加网格线，透明度 0.5</span><br>ax.grid(<span class="hljs-literal">True</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br><br><span class="hljs-comment"># 显示图形</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><strong>效果如下：</strong><br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250915165453281.png"></p><hr><h1 id="2-折线图-Line-Plot"><a href="#2-折线图-Line-Plot" class="headerlink" title="2. 折线图 (Line Plot)"></a>2. 折线图 (Line Plot)</h1><p>折线图常用于展示数据随某个序列（如时间、迭代次数）的变化趋势。在机器学习中，典型应用包括：</p><ul><li>绘制模型训练过程中的损失 (<code>Loss</code>) 和准确率 (<code>Accuracy</code>) 曲线。</li><li>绘制学习曲线 (<code>Learning Curve</code>) 来诊断过拟合或欠拟合。</li></ul><p><strong>核心方法</strong>：<code>ax.plot()</code></p><p><strong>常用参数解释</strong>：</p><ul><li><p><code>x</code>, <code>y</code>: 横轴和纵轴的数据。</p></li><li><p><code>color</code>: 线的颜色。</p></li><li><p><code>linestyle</code>: 线的样式，如 <code>&#39;-&#39;</code> (实线), <code>&#39;--&#39;</code> (虚线), <code>&#39;:&#39;</code> (点线), <code>&#39;-.&#39;</code> (点划线)。</p></li><li><p><code>linewidth</code>: 线的宽度。</p></li><li><p><code>marker</code>: 数据点的标记样式，如 <code>&#39;o&#39;</code>, <code>&#39;.&#39;</code>, <code>&#39;s&#39;</code>。</p></li><li><p><code>label</code>: 图例标签。</p></li></ul><h2 id="代码示例：绘制训练与验证损失曲线"><a href="#代码示例：绘制训练与验证损失曲线" class="headerlink" title="代码示例：绘制训练与验证损失曲线"></a>代码示例：绘制训练与验证损失曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># --- 准备数据 ---</span><br><span class="hljs-comment"># 模拟训练过程的 epoch 和 loss</span><br>epochs = np.arange(<span class="hljs-number">1</span>, <span class="hljs-number">51</span>)<br>train_loss = <span class="hljs-number">0.8</span> / epochs**<span class="hljs-number">0.5</span> + np.random.randn(<span class="hljs-number">50</span>) * <span class="hljs-number">0.05</span><br>val_loss = <span class="hljs-number">0.9</span> / epochs**<span class="hljs-number">0.5</span> + np.random.randn(<span class="hljs-number">50</span>) * <span class="hljs-number">0.06</span> + <span class="hljs-number">0.1</span><br><br><span class="hljs-comment"># --- 绘图 ---</span><br>fig, ax = plt.subplots(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">7</span>))<br><br><span class="hljs-comment"># 绘制训练损失曲线</span><br>ax.plot(epochs, train_loss, color=<span class="hljs-string">&#x27;blue&#x27;</span>, linestyle=<span class="hljs-string">&#x27;-&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>, markersize=<span class="hljs-number">4</span>, label=<span class="hljs-string">&#x27;训练损失 (Train Loss)&#x27;</span>)<br><br><span class="hljs-comment"># 绘制验证损失曲线</span><br>ax.plot(epochs, val_loss, color=<span class="hljs-string">&#x27;red&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, marker=<span class="hljs-string">&#x27;s&#x27;</span>, markersize=<span class="hljs-number">4</span>, label=<span class="hljs-string">&#x27;验证损失 (Validation Loss)&#x27;</span>)<br><br><span class="hljs-comment"># --- 美化图形 ---</span><br>ax.set_title(<span class="hljs-string">&#x27;模型训练过程中的损失变化&#x27;</span>, fontsize=<span class="hljs-number">16</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;迭代次数 (Epoch)&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;损失 (Loss)&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br>ax.grid(<span class="hljs-literal">True</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, alpha=<span class="hljs-number">0.6</span>)<br>ax.legend() <span class="hljs-comment"># 显示图例</span><br><br>plt.show()<br></code></pre></td></tr></table></figure><p>效果如下：<br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250915170844951.png"></p><hr><h1 id="图片示例"><a href="#图片示例" class="headerlink" title="图片示例"></a>图片示例</h1><ul><li>CVer通常在加载数据集后，会查看一下前几张图片，用做一个示例，那么下面这套流程可以实现</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">temp_dataset = get_dataset(os.path.join(workspace_dir, <span class="hljs-string">&#x27;faces&#x27;</span>))<br><br>images = [temp_dataset[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)]<br>grid_img = torchvision.utils.make_grid(images, nrow=<span class="hljs-number">4</span>) <span class="hljs-comment">#将多张图像拼接成网格形式</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br>plt.imshow(grid_img.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)) <span class="hljs-comment"># imshow的输入为(H, W, C)</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250915172423756.png"></p><hr><h1 id="总结与通用美化技巧"><a href="#总结与通用美化技巧" class="headerlink" title="总结与通用美化技巧"></a>总结与通用美化技巧</h1><p>无论绘制哪种图形，最后的美化步骤都至关重要。</p><p><strong>通用美化函数</strong> (都属于 <code>Axes</code> 对象的方法):</p><ul><li><p><code>ax.set_title()</code>: 设置标题。</p></li><li><p><code>ax.set_xlabel()</code>, <code>ax.set_ylabel()</code>: 设置 x, y 轴标签。</p></li><li><p><code>ax.set_xlim()</code>, <code>ax.set_ylim()</code>: 设置 x, y 轴的范围。</p></li><li><p><code>ax.set_xticks()</code>, <code>ax.set_yticks()</code>: 设置 x, y 轴的刻度位置。</p></li><li><p><code>ax.set_xticklabels()</code>, <code>ax.set_yticklabels()</code>: 设置刻度对应的标签。</p></li><li><p><code>ax.legend()</code>: 显示图例。</p></li><li><p><code>ax.grid(True)</code>: 显示网格线。</p></li><li><p><code>plt.tight_layout()</code>: 在 <code>plt.show()</code> 前调用，自动调整子图参数，使之填充整个图像区域，防止标签重叠。</p></li><li><p><code>plt.savefig(&#39;figure.png&#39;, dpi=300)</code>: 保存图像到文件，<code>dpi</code> 参数可以设置分辨率。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>ML&amp;DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Matplotlib</tag>
      
      <tag>plt</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅机器学习-GAN</title>
    <link href="/2025/09/13/GAN/"/>
    <url>/2025/09/13/GAN/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习李宏毅机器学习时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="1-生成模型简介-Introduction-of-Generative-Models"><a href="#1-生成模型简介-Introduction-of-Generative-Models" class="headerlink" title="1. 生成模型简介 (Introduction of Generative Models)"></a>1. 生成模型简介 (Introduction of Generative Models)</h1><p>生成模型的核心思想是创建一个<strong>能够学习数据内在规律和分布的神经网络</strong>，这个网络被称为**生成器 (Generator)**。生成器的任务是从一个简单的、已知的概率分布（如正态分布或均匀分布）中采样一个随机向量（通常称为噪声 <code>z</code>），并将其转换为一个复杂的、高维的数据样本（如一张图片）。</p><p>这个过程好比让一个神经网络学习如何“创造”。我们给它一个简单的随机种子，它就能“画”出一张逼真的图像。</p><p>为什么需要生成一个分布？</p><p>在许多任务中，同一个输入可能对应多个合理的输出。例如：</p><ul><li><p><strong>视频预测</strong>: 预测视频的下一帧时，角色可能向左走也可能向右走，两种都是合理的结果。如果模型只输出一个确定的、所有可能性的平均结果，画面就会变得模糊不清。</p></li><li><p><strong>需要创造力的任务</strong>: 像绘画或对话机器人这样的任务，我们希望模型能有“创造力”，对同一个输入（如“画一个红眼睛的角色”）能产生多样化的输出，而不是每次都生成一模一样的结果。</p></li></ul><p>通过<strong>让生成器学习输出一个分布，模型就可以捕捉到这种不确定性</strong>，从而生成多样化且清晰的结果。</p><hr><h1 id="2-生成对抗网络-GAN"><a href="#2-生成对抗网络-GAN" class="headerlink" title="2. 生成对抗网络 (GAN)"></a>2. 生成对抗网络 (GAN)</h1><h2 id="基本解释"><a href="#基本解释" class="headerlink" title="基本解释"></a>基本解释</h2><p>生成对抗网络（GAN）是一种强大的生成模型框架，它通过一种“对抗”或“博弈”的方式进行训练。GAN由两个核心部分组成：</p><ol><li><p><strong>生成器 (Generator, G)</strong>:</p><ul><li><p><strong>角色</strong>: 一个“伪造者”或“艺术家”。</p></li><li><p><strong>任务</strong>: 学习真实数据的分布，并尝试生成尽可能逼真、能够以假乱真的数据（例如，生成动漫头像）。</p></li></ul></li><li><p><strong>判别器 (Discriminator, D)</strong>:</p><ul><li><p><strong>角色</strong>: 一个“鉴赏家”或“警察”。</p></li><li><p><strong>任务</strong>: 学习如何区分真实数据和由生成器生成的“假”数据。对于真实数据，它应该输出高分（接近1）；对于假数据，它应该输出低分（接近0）。</p></li></ul></li></ol><h2 id="对抗训练过程"><a href="#对抗训练过程" class="headerlink" title="对抗训练过程"></a>对抗训练过程</h2><p>GAN的训练过程是一个动态的、交替优化的过程，就像生成器和判别器在进行一场永无休止的“军备竞赛”：</p><ul><li><p><strong>第1步：固定G，训练D</strong></p><ul><li><p>从真实数据库中采样一批真实图片，并让生成器G生成一批假图片。</p></li><li><p>将这两批图片都喂给判别器D。</p></li><li><p>D的目标是给真实图片打高分，给假图片打低分。我们根据这个目标更新D的参数，让它的鉴别能力越来越强。</p></li></ul></li><li><p><strong>第2步：固定D，训练G</strong></p><ul><li><p>生成器G生成一批假图片，并将其输入到<strong>固定不变的</strong>判别器D中。</p></li><li><p>G的目标是“欺骗”D，即<strong>希望D给这些假图片打出尽可能高的分数</strong>。</p></li><li><p>我们根据这个目标，反向传播梯度来更新G的参数。这样，G就会学习如何生成更逼真的图片来迷惑D。</p></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250913224023774.png"></p><p>这个过程不断迭代，最终理想状态下，生成器G能够生成与真实数据无法区分的样本，而判别器D再也无法分辨真假（对任何输入的打分都接近0.5）。这个过程就像是亦敌亦友的共同进化。</p><p>[图片：展示GAN训练过程中，生成的动漫头像从模糊的噪声逐渐变得清晰、逼真的演进过程]</p><blockquote><p><strong>开创性论文</strong>:</p><ul><li>Generative Adversarial Nets: <a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a></li></ul></blockquote><hr><h1 id="3-GAN背后的理论"><a href="#3-GAN背后的理论" class="headerlink" title="3. GAN背后的理论"></a>3. GAN背后的理论</h1><p>GAN的最终目标是让生成器G学习到的数据分布 $P_G$​ 尽可能地接近真实数据的分布 $P_{data}$​。我们可以用一个衡量两个分布之间差异的指标——<strong>散度 (Divergence)</strong> 来描述这个目标。</p><p>$$<br>G∗&#x3D;argmin ​Div(PG​,Pdata​)<br>$$</p><p>虽然我们无法直接计算这两个复杂分布之间的散度，但GAN巧妙地利用判别器D来间接实现这一目标。</p><p>在原始GAN的理论中，判别器D的优化目标（最大化V(G,D)）实际上是在计算 PG​ 和 Pdata​ 之间的**JS散度 (Jensen-Shannon divergence)**。</p><p>$$<br>V(G,D)&#x3D;E_{y∼P_{data}​}​[logD(y)]+E_{y∼P_G​​}[log(1−D(y))]<br>$$</p><p>当判别器D被训练到最优时，<strong>生成器G的优化目标就等价于最小化 PG​ 和 Pdata​ 之间的JS散度</strong>。&#x3D;&#x3D;因此，整个GAN的 min-max 博弈过程，实际上就是在推动生成分布去逼近真实分布&#x3D;&#x3D;</p><p>[图片：展示 PG​ 和 Pdata​ 两个分布，并用箭头表示GAN的目标是让 PG​ 尽可能靠近 Pdata​]</p><blockquote><p><strong>f-GAN 论文 (探索不同散度)</strong>:</p><ul><li>f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization: <a href="https://arxiv.org/abs/1606.00709">https://arxiv.org/abs/1606.00709</a></li></ul></blockquote><hr><h1 id="4-训练GAN的技巧"><a href="#4-训练GAN的技巧" class="headerlink" title="4. 训练GAN的技巧"></a>4. 训练GAN的技巧</h1><p>GAN的训练是出了名的困难和不稳定，常被称为“炼丹”。以下是一些关键的挑战和改进技巧：</p><h2 id="挑战1：JS散度的问题与WGAN"><a href="#挑战1：JS散度的问题与WGAN" class="headerlink" title="挑战1：JS散度的问题与WGAN"></a>挑战1：JS散度的问题与WGAN</h2><ul><li><p><strong>问题</strong>: 在高维空间中，两个分布 PG​ 和 Pdata​ 几乎不可能有重叠。<strong>当它们没有重叠时，JS散度是一个常数（log2）</strong>，这导致生成器的梯度为0，无法学习。</p></li><li><p><strong>解决方案</strong>: <strong>Wasserstein GAN (WGAN)</strong> 提出使用<code>Wasserstein距离</code>（又称“推土机距离”）来替代JS散度。</p><ul><li><strong>优势</strong>: 即使两个分布没有重叠，Wasserstein距离仍然能提供一个有意义的、平滑的度量，反映了它们之间的“远近”。这为生成器提供了持续有效的梯度。</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250913224952306.png"></p><ul><li><p><strong>实现</strong>: WGAN通过对判别器（在WGAN中称为Critic）施加<strong>1-Lipschitz约束</strong>来实现。为了满足这个约束，后续工作提出了更稳定的方法：</p><ul><li><p><strong>WGAN-GP (Gradient Penalty)</strong>: 通过对Critic的梯度添加惩罚项来约束它。</p></li><li><p><strong>谱归一化 (Spectral Normalization)</strong>: 一种更简单有效的约束方法。</p></li></ul></li></ul><blockquote><p><strong>WGAN 相关论文</strong>:</p><ul><li><p>Wasserstein GAN: <a href="https://arxiv.org/abs/1701.07875">https://arxiv.org/abs/1701.07875</a></p></li><li><p>Improved Training of Wasserstein GANs: <a href="https://arxiv.org/abs/1704.00028">https://arxiv.org/abs/1704.00028</a></p></li><li><p>Spectral Normalization for Generative Adversarial Networks: <a href="https://arxiv.org/abs/1802.05957">https://arxiv.org/abs/1802.05957</a></p></li></ul></blockquote><h2 id="挑战2：生成器与判别器的平衡"><a href="#挑战2：生成器与判别器的平衡" class="headerlink" title="挑战2：生成器与判别器的平衡"></a>挑战2：生成器与判别器的平衡</h2><p>生成器和判别器的学习能力需要“棋逢对手”。如果判别器太强，生成器梯度会消失；如果判别器太弱，生成器则学不到有效信息。<strong>保持二者同步发展是训练成功的关键</strong>。</p><h2 id="挑战3：GAN用于序列生成"><a href="#挑战3：GAN用于序列生成" class="headerlink" title="挑战3：GAN用于序列生成"></a>挑战3：GAN用于序列生成</h2><p>将GAN用于<strong>生成文本等离散序列是困难的</strong>，因为从概率分布中“采样”的这个操作是不可导的，导致梯度无法从判别器回传到生成器。通常需要借助<code>强化学习 (Reinforcement Learning)</code> 的思想来解决，但这会使训练变得更加复杂。</p><blockquote><p><strong>更多训练技巧参考</strong>:</p><ul><li><p>GAN Hacks (GitHub): <a href="https://github.com/soumith/ganhacks">https://github.com/soumith/ganhacks</a></p></li><li><p>DCGAN paper: <a href="https://arxiv.org/abs/1511.06434">https://arxiv.org/abs/1511.06434</a></p></li><li><p>Improved Techniques for Training GANs: <a href="https://arxiv.org/abs/1606.03498">https://arxiv.org/abs/1606.03498</a></p></li><li><p>BigGAN paper: <a href="https://arxiv.org/abs/1809.11096">https://arxiv.org/abs/1809.11096</a></p></li></ul></blockquote><hr><h1 id="5-条件生成-Conditional-Generation"><a href="#5-条件生成-Conditional-Generation" class="headerlink" title="5. 条件生成 (Conditional Generation)"></a>5. 条件生成 (Conditional Generation)</h1><p>无条件GAN生成的内容是完全随机的，我们无法控制。<strong>条件GAN (Conditional GAN, cGAN)</strong> 通过引入一个额外的条件输入 <code>x</code>（如文本描述、图像等），来指导生成器生成特定的内容。</p><p><strong>工作原理</strong>:</p><ol><li><strong>生成器</strong>: 将随机噪声 <code>z</code> 和条件 <code>x</code> <strong>同时</strong>作为输入，生成对应的图像 y&#x3D;G(x,z)。</li><li><strong>判别器</strong>: 不仅要判断输入的图像 <code>y</code> 是否真实，还要判断 <code>y</code> 是否与条件 <code>x</code> <strong>匹配</strong>。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250913225227375.png"></p><p>应用实例 (pix2pix):</p><p>条件GAN在图像到图像翻译 (Image-to-Image Translation) 任务中非常成功，例如：</p><ul><li>从标签图生成街景</li><li>从线稿生成手提包</li><li>从黑白照片生成彩色照片</li><li>从航拍图生成地图</li></ul><blockquote><p><strong>相关论文</strong>:</p><ul><li><p>Conditional Generative Adversarial Nets: <a href="https://arxiv.org/abs/1411.1784">https://arxiv.org/abs/1411.1784</a></p></li><li><p>Image-to-Image Translation with Conditional Adversarial Networks (pix2pix): <a href="https://arxiv.org/abs/1611.07004">https://arxiv.org/abs/1611.07004</a></p></li></ul></blockquote><hr><h1 id="6-从非成对数据中学习"><a href="#6-从非成对数据中学习" class="headerlink" title="6. 从非成对数据中学习"></a>6. 从非成对数据中学习</h1><p>pix2pix等条件GAN需要大量的<strong>成对 (paired)</strong> 训练数据（如线稿和对应的实物图），而这在现实中往往难以获取。<code>CycleGAN</code> 解决了这个问题，它能从<strong>非成对 (unpaired)</strong> 的数据中学习两个领域之间的转换。</p><p>例如，我们只需要一个“真人照片”文件夹和一个“动漫头像”文件夹，不需要任何一张照片和它对应的动漫头像。</p><p>核心思想：<strong>循环一致性损失</strong> (<code>Cycle Consistency Loss</code>)</p><p>CycleGAN同时训练两个生成器：GX→Y​ (从领域X到Y) 和 GY→X​ (从领域Y到X)。其<strong>核心约束是循环一致性</strong>：</p><ol><li><p>将一张真人照片 x 通过 GX→Y​ 转换成动漫头像。</p></li><li><p>再将这个生成的动漫头像通过 GY→X​ 转换回真人照片。</p></li><li><p>这个“循环”回来的照片应该和原始的输入照片 x <strong>尽可能一致</strong>。</p></li></ol><p>这个约束确保了生成器在转换风格的同时，会保留原始图像的内容，而不是随意生成一个目标域中不相关的图像。</p><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250913225500744.png"></p><blockquote><p><strong>相关论文</strong>:</p><ul><li><p>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN): <a href="https://arxiv.org/abs/1703.10593">https://arxiv.org/abs/1703.10593</a></p></li><li><p>(类似工作) DiscoGAN: <a href="https://arxiv.org/abs/1703.05192">https://arxiv.org/abs/1703.05192</a></p></li><li><p>(类似工作) DualGAN: <a href="https://arxiv.org/abs/1704.02510">https://arxiv.org/abs/1704.02510</a></p></li><li><p>(多领域) StarGAN: <a href="https://arxiv.org/abs/1711.09020">https://arxiv.org/abs/1711.09020</a></p></li></ul></blockquote><hr><h1 id="7-生成模型的评估"><a href="#7-生成模型的评估" class="headerlink" title="7. 生成模型的评估"></a>7. 生成模型的评估</h1><p>评估生成模型的好坏是一个开放性难题，因为“好”本身是一个主观概念。我们通常从两个维度进行评估：</p><p><strong>1. 图像质量 (Quality)</strong></p><ul><li><strong>方法</strong>: 将生成的图像输入到一个<strong>预训练好的图像分类器</strong>（如Inception Net）中。如果生成的图像清晰且包含明确的物体，分类器会给出一个非常集中的概率分布（例如，99%的概率是“狗”）。如果图像模糊不清，则分布会比较分散。</li></ul><p><strong>2. 图像多样性 (Diversity)</strong></p><ul><li><p><strong>问题</strong>: 生成器可能会陷入<strong>模式坍塌 (Mode Collapse)<strong>，即无论输入什么随机噪声，都只生成少数几种或一种图像，</strong>缺乏多样性</strong>。</p></li><li><p><strong>模式丢失 (Mode Dropping)</strong>: 生成器可能只学会了真实数据分布中的一部分模式，而完全忽略了其他模式。</p></li></ul><p><strong>常用评估指标</strong>:</p><ul><li><p><strong>Inception Score (IS)</strong>: 结合了图像质量和多样性。分数越高越好。但它有缺陷，比如无法检测到是否过拟合了训练数据。</p></li><li><p><strong>Fréchet Inception Distance (FID)</strong>: 目前更常用、更可靠的指标。它通过比较真实图像和生成图像在特征空间中的分布相似度来打分。<strong>FID分数越低越好</strong>。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250913225648019.png"></p><blockquote><p><strong>相关论文</strong>:</p><ul><li><p>A Note on the Inception Score: <a href="https://arxiv.org/abs/1801.01973">https://arxiv.org/abs/1801.01973</a></p></li><li><p>GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium: <a href="https://arxiv.org/abs/1706.08500">https://arxiv.org/abs/1706.08500</a></p></li><li><p>Are GANs Created Equal? A Large-Scale Study: <a href="https://arxiv.org/abs/1711.10337">https://arxiv.org/abs/1711.10337</a></p></li><li><p>Pros and Cons of GAN Evaluation Measures: <a href="https://arxiv.org/abs/1802.03446">https://arxiv.org/abs/1802.03446</a></p></li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>ML&amp;DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
      <tag>Adversarial</tag>
      
      <tag>JS散度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS231n-L2-Image Classification with Linear Classifiers</title>
    <link href="/2025/09/12/L2-Image%20Classification%20with%20Linear%20Classifiers/"/>
    <url>/2025/09/12/L2-Image%20Classification%20with%20Linear%20Classifiers/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我搬运的CS231n 的讲义 ，希望能对你有所帮助😊 </p>          </div><p>原文链接： <a href="https://cs231n.github.io/classification/">link</a></p><p>This is an introductory lecture designed to introduce people from outside of Computer Vision to the Image Classification problem, and the data-driven approach. The Table of Contents:</p><ul><li><a href="https://cs231n.github.io/classification/#image-classification">Image Classification</a><ul><li><a href="https://cs231n.github.io/classification/#nearest-neighbor-classifier">Nearest Neighbor Classifier</a></li><li><a href="https://cs231n.github.io/classification/#k---nearest-neighbor-classifier">k - Nearest Neighbor Classifier</a></li><li><a href="https://cs231n.github.io/classification/#validation-sets-for-hyperparameter-tuning">Validation sets for Hyperparameter tuning</a></li><li><a href="https://cs231n.github.io/classification/#summary">Summary</a></li><li><a href="https://cs231n.github.io/classification/#summary-applying-knn-in-practice">Summary: Applying kNN in practice</a><ul><li><a href="https://cs231n.github.io/classification/#further-reading">Further Reading</a></li></ul></li></ul></li></ul><h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h2><p><strong>Motivation</strong>. In this section we will introduce the Image Classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. Moreover, as we will see later in the course, many other seemingly distinct Computer Vision tasks (such as object detection, segmentation) can be reduced to image classification.</p><p><strong>Example</strong>. For example, in the image below an image classification model takes a single image and assigns probabilities to 4 labels, <em>{cat, dog, hat, mug}</em>. As shown in the image, keep in mind that to a computer an image is represented as one large 3-dimensional array of numbers. In this example, the cat image is 248 pixels wide, 400 pixels tall, and has three color channels Red,Green,Blue (or RGB for short). Therefore, the image consists of 248 x 400 x 3 numbers, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). Our task is to turn this quarter of a million numbers into a single label, such as <em>“cat”</em>.</p><p><img src="https://cs231n.github.io/assets/classify.png"></p><p>The task in Image Classification is to predict a single label (or a distribution over labels as shown here to indicate our confidence) for a given image. Images are 3-dimensional arrays of integers from 0 to 255, of size Width x Height x 3. The 3 represents the three color channels Red, Green, Blue.</p><p><strong>Challenges</strong>. Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for a human to perform, it is worth considering the challenges involved from the perspective of a Computer Vision algorithm. As we present (an inexhaustive) list of challenges below, keep in mind the raw representation of images as a 3-D array of brightness values:</p><ul><li><strong>Viewpoint variation</strong>. A single instance of an object can be oriented in many ways with respect to the camera.</li><li><strong>Scale variation</strong>. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image).</li><li><strong>Deformation</strong>. Many objects of interest are not rigid bodies and can be deformed in extreme ways.</li><li><strong>Occlusion</strong>. The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.</li><li><strong>Illumination conditions</strong>. The effects of illumination are drastic on the pixel level.</li><li><strong>Background clutter</strong>. The objects of interest may <em>blend</em> into their environment, making them hard to identify.</li><li><strong>Intra-class variation</strong>. The classes of interest can often be relatively broad, such as <em>chair</em>. There are many different types of these objects, each with their own appearance.</li></ul><p>A good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations.</p><p><img src="https://cs231n.github.io/assets/challenges.jpeg"></p><p><strong>Data-driven approach</strong>. How might we go about writing an algorithm that can classify images into distinct categories? Unlike writing an algorithm for, for example, sorting a list of numbers, it is not obvious how one might write an algorithm for identifying cats in images. Therefore, instead of trying to specify what every one of the categories of interest look like directly in code, the approach that we will take is not unlike one you would take with a child: we’re going to provide the computer with many examples of each class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. This approach is referred to as a <em>data-driven approach</em>, since it relies on first accumulating a <em>training dataset</em> of labeled images. Here is an example of what such a dataset might look like:</p><p><img src="https://cs231n.github.io/assets/trainset.jpg"></p><p>An example training set for four visual categories. In practice we may have thousands of categories and hundreds of thousands of images for each category.</p><p><strong>The image classification pipeline</strong>. We’ve seen that the task in Image Classification is to take an array of pixels that represents a single image and assign a label to it. Our complete pipeline can be formalized as follows:</p><ul><li><strong>Input:</strong> Our input consists of a set of <em>N</em> images, each labeled with one of <em>K</em> different classes. We refer to this data as the <em>training set</em>.</li><li><strong>Learning:</strong> Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as <em>training a classifier</em>, or <em>learning a model</em>.</li><li><strong>Evaluation:</strong> In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers (which we call the <em>ground truth</em>).</li></ul><h3 id="Nearest-Neighbor-Classifier"><a href="#Nearest-Neighbor-Classifier" class="headerlink" title="Nearest Neighbor Classifier"></a>Nearest Neighbor Classifier</h3><p>As our first approach, we will develop what we call a <strong>Nearest Neighbor Classifier</strong>. This classifier has nothing to do with Convolutional Neural Networks and it is very rarely used in practice, but it will allow us to get an idea about the basic approach to an image classification problem.</p><p><strong>Example image classification dataset: CIFAR-10.</strong> One popular toy image classification dataset is the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a>. This dataset consists of 60,000 tiny images that are 32 pixels high and wide. Each image is labeled with one of 10 classes (for example <em>“airplane, automobile, bird, etc”</em>). These 60,000 images are partitioned into a training set of 50,000 images and a test set of 10,000 images. In the image below you can see 10 random example images from each one of the 10 classes:</p><p><img src="https://cs231n.github.io/assets/nn.jpg"></p><p>Left: Example images from the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a>. Right: first column shows a few test images and next to each we show the top 10 nearest neighbors in the training set according to pixel-wise difference.</p><p>Suppose now that we are given the CIFAR-10 training set of 50,000 images (5,000 images for every one of the labels), and we wish to label the remaining 10,000. The nearest neighbor classifier will take a test image, compare it to every single one of the training images, and predict the label of the closest training image. In the image above and on the right you can see an example result of such a procedure for 10 example test images. Notice that in only about 3 out of 10 examples an image of the same class is retrieved, while in the other 7 examples this is not the case. For example, in the 8th row the nearest training image to the horse head is a red car, presumably due to the strong black background. As a result, this image of a horse would in this case be mislabeled as a car.</p><p>You may have noticed that we left unspecified the details of exactly how we compare two images, which in this case are just two blocks of 32 x 32 x 3. One of the simplest possibilities is to compare the images pixel by pixel and add up all the differences. In other words, given two images and representing them as vectors I1,I2 , a reasonable choice for comparing them might be the <strong>L1 distance</strong>:</p><p>[d_1 (I_1, I_2) &#x3D; \sum_{p} \left| I^p_1 - I^p_2 \right|]</p><p>Where the sum is taken over all pixels. Here is the procedure visualized:</p><p><img src="https://cs231n.github.io/assets/nneg.jpeg"></p><p>An example of using pixel-wise differences to compare two images with L1 distance (for one color channel in this example). Two images are subtracted elementwise and then all differences are added up to a single number. If two images are identical the result will be zero. But if the images are very different the result will be large.</p><p>Let’s also look at how we might implement the classifier in code. First, let’s load the CIFAR-10 data into memory as 4 arrays: the training data&#x2F;labels and the test data&#x2F;labels. In the code below, <code>Xtr</code> (of size 50,000 x 32 x 32 x 3) holds all the images in the training set, and a corresponding 1-dimensional array <code>Ytr</code> (of length 50,000) holds the training labels (from 0 to 9):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class="hljs-string">&#x27;data/cifar10/&#x27;</span>) <span class="hljs-comment"># a magic function we provide</span><br><span class="hljs-comment"># flatten out all images to be one-dimensional</span><br>Xtr_rows = Xtr.reshape(Xtr.shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span> * <span class="hljs-number">32</span> * <span class="hljs-number">3</span>) <span class="hljs-comment"># Xtr_rows becomes 50000 x 3072</span><br>Xte_rows = Xte.reshape(Xte.shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span> * <span class="hljs-number">32</span> * <span class="hljs-number">3</span>) <span class="hljs-comment"># Xte_rows becomes 10000 x 3072</span><br></code></pre></td></tr></table></figure><p>Now that we have all images stretched out as rows, here is how we could train and evaluate a classifier:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">nn = NearestNeighbor() <span class="hljs-comment"># create a Nearest Neighbor classifier class</span><br>nn.train(Xtr_rows, Ytr) <span class="hljs-comment"># train the classifier on the training images and labels</span><br>Yte_predict = nn.predict(Xte_rows) <span class="hljs-comment"># predict labels on the test images</span><br><span class="hljs-comment"># and now print the classification accuracy, which is the average number</span><br><span class="hljs-comment"># of examples that are correctly predicted (i.e. label matches)</span><br><span class="hljs-built_in">print</span> <span class="hljs-string">&#x27;accuracy: %f&#x27;</span> % ( np.mean(Yte_predict == Yte) )<br></code></pre></td></tr></table></figure><p>Notice that as an evaluation criterion, it is common to use the <strong>accuracy</strong>, which measures the fraction of predictions that were correct. Notice that all classifiers we will build satisfy this one common API: they have a <code>train(X,y)</code> function that takes the data and the labels to learn from. Internally, the class should build some kind of model of the labels and how they can be predicted from the data. And then there is a <code>predict(X)</code> function, which takes new data and predicts the labels. Of course, we’ve left out the meat of things - the actual classifier itself. Here is an implementation of a simple Nearest Neighbor classifier with the L1 distance that satisfies this template:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NearestNeighbor</span>(<span class="hljs-title class_ inherited__">object</span>):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">pass</span><br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, X, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># the nearest neighbor classifier simply remembers all the training data</span><br>    <span class="hljs-variable language_">self</span>.Xtr = X<br>    <span class="hljs-variable language_">self</span>.ytr = y<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot;</span><br>    num_test = X.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-comment"># lets make sure that the output type matches the input type</span><br>    Ypred = np.zeros(num_test, dtype = <span class="hljs-variable language_">self</span>.ytr.dtype)<br><br>    <span class="hljs-comment"># loop over all test rows</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_test):<br>      <span class="hljs-comment"># find the nearest training image to the i&#x27;th test image</span><br>      <span class="hljs-comment"># using the L1 distance (sum of absolute value differences)</span><br>      distances = np.<span class="hljs-built_in">sum</span>(np.<span class="hljs-built_in">abs</span>(<span class="hljs-variable language_">self</span>.Xtr - X[i,:]), axis = <span class="hljs-number">1</span>)<br>      min_index = np.argmin(distances) <span class="hljs-comment"># get the index with smallest distance</span><br>      Ypred[i] = <span class="hljs-variable language_">self</span>.ytr[min_index] <span class="hljs-comment"># predict the label of the nearest example</span><br><br>    <span class="hljs-keyword">return</span> Ypred<br></code></pre></td></tr></table></figure><p>If you ran this code, you would see that this classifier only achieves <strong>38.6%</strong> on CIFAR-10. That’s more impressive than guessing at random (which would give 10% accuracy since there are 10 classes), but nowhere near human performance (which is <a href="https://karpathy.github.io/2011/04/27/manually-classifying-cifar10/">estimated at about 94%</a>) or near state-of-the-art Convolutional Neural Networks that achieve about 95%, matching human accuracy (see the <a href="https://www.kaggle.com/c/cifar-10/leaderboard">leaderboard</a> of a recent Kaggle competition on CIFAR-10).</p><p><strong>The choice of distance.</strong> There are many other ways of computing distances between vectors. Another common choice could be to instead use the <strong>L2 distance</strong>, which has the geometric interpretation of computing the euclidean distance between two vectors. The distance takes the form:</p><p>[d_2 (I_1, I_2) &#x3D; \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}]</p><p>In other words we would be computing the pixelwise difference as before, but this time we square all of them, add them up and finally take the square root. In numpy, using the code from above we would need to only replace a single line of code. The line that computes the distances:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">distances = np.sqrt(np.<span class="hljs-built_in">sum</span>(np.square(<span class="hljs-variable language_">self</span>.Xtr - X[i,:]), axis = <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p>Note that I included the <code>np.sqrt</code> call above, but in a practical nearest neighbor application we could leave out the square root operation because square root is a <em>monotonic function</em>. That is, it scales the absolute sizes of the distances but it preserves the ordering, so the nearest neighbors with or without it are identical. If you ran the Nearest Neighbor classifier on CIFAR-10 with this distance, you would obtain <strong>35.4%</strong> accuracy (slightly lower than our L1 distance result).</p><p><strong>L1 vs. L2.</strong> It is interesting to consider differences between the two metrics. In particular, the L2 distance is much more unforgiving than the L1 distance when it comes to differences between two vectors. That is, the L2 distance prefers many medium disagreements to one big one. L1 and L2 distances (or equivalently the L1&#x2F;L2 norms of the differences between a pair of images) are the most commonly used special cases of a <a href="https://planetmath.org/vectorpnorm">p-norm</a>.</p><h3 id="k-Nearest-Neighbor-Classifier"><a href="#k-Nearest-Neighbor-Classifier" class="headerlink" title="k - Nearest Neighbor Classifier"></a>k - Nearest Neighbor Classifier</h3><p>You may have noticed that it is strange to only use the label of the nearest image when we wish to make a prediction. Indeed, it is almost always the case that one can do better by using what’s called a <strong>k-Nearest Neighbor Classifier</strong>. The idea is very simple: instead of finding the single closest image in the training set, we will find the top <strong>k</strong> closest images, and have them vote on the label of the test image. In particular, when <em>k &#x3D; 1</em>, we recover the Nearest Neighbor classifier. Intuitively, higher values of <strong>k</strong> have a smoothing effect that makes the classifier more resistant to outliers:</p><p><img src="https://cs231n.github.io/assets/knn.jpeg"></p><p>An example of the difference between Nearest Neighbor and a 5-Nearest Neighbor classifier, using 2-dimensional points and 3 classes (red, blue, green). The colored regions show the <strong>decision boundaries</strong> induced by the classifier with an L2 distance. The white regions show points that are ambiguously classified (i.e. class votes are tied for at least two classes). Notice that in the case of a NN classifier, outlier datapoints (e.g. green point in the middle of a cloud of blue points) create small islands of likely incorrect predictions, while the 5-NN classifier smooths over these irregularities, likely leading to better <strong>generalization</strong> on the test data (not shown). Also note that the gray regions in the 5-NN image are caused by ties in the votes among the nearest neighbors (e.g. 2 neighbors are red, next two neighbors are blue, last neighbor is green).</p><p>In practice, you will almost always want to use k-Nearest Neighbor. But what value of <em>k</em> should you use? We turn to this problem next.</p><h3 id="Validation-sets-for-Hyperparameter-tuning"><a href="#Validation-sets-for-Hyperparameter-tuning" class="headerlink" title="Validation sets for Hyperparameter tuning"></a>Validation sets for Hyperparameter tuning</h3><p>The k-nearest neighbor classifier requires a setting for <em>k</em>. But what number works best? Additionally, we saw that there are many different distance functions we could have used: L1 norm, L2 norm, there are many other choices we didn’t even consider (e.g. dot products). These choices are called <strong>hyperparameters</strong> and they come up very often in the design of many Machine Learning algorithms that learn from data. It’s often not obvious what values&#x2F;settings one should choose.</p><p>You might be tempted to suggest that we should try out many different values and see what works best. That is a fine idea and that’s indeed what we will do, but this must be done very carefully. In particular, <strong>we cannot use the test set for the purpose of tweaking hyperparameters</strong>. Whenever you’re designing Machine Learning algorithms, you should think of the test set as a very precious resource that should ideally never be touched until one time at the very end. Otherwise, the very real danger is that you may tune your hyperparameters to work well on the test set, but if you were to deploy your model you could see a significantly reduced performance. In practice, we would say that you <strong>overfit</strong> to the test set. Another way of looking at it is that if you tune your hyperparameters on the test set, you are effectively using the test set as the training set, and therefore the performance you achieve on it will be too optimistic with respect to what you might actually observe when you deploy your model. But if you only use the test set once at end, it remains a good proxy for measuring the <strong>generalization</strong> of your classifier (we will see much more discussion surrounding generalization later in the class).</p><blockquote><p>Evaluate on the test set only a single time, at the very end.</p></blockquote><p>Luckily, there is a correct way of tuning the hyperparameters and it does not touch the test set at all. The idea is to split our training set in two: a slightly smaller training set, and what we call a <strong>validation set</strong>. Using CIFAR-10 as an example, we could for example use 49,000 of the training images for training, and leave 1,000 aside for validation. This validation set is essentially used as a fake test set to tune the hyper-parameters.</p><p>Here is what this might look like in the case of CIFAR-10:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span><br><span class="hljs-comment"># recall Xtr_rows is 50,000 x 3072 matrix</span><br>Xval_rows = Xtr_rows[:<span class="hljs-number">1000</span>, :] <span class="hljs-comment"># take first 1000 for validation</span><br>Yval = Ytr[:<span class="hljs-number">1000</span>]<br>Xtr_rows = Xtr_rows[<span class="hljs-number">1000</span>:, :] <span class="hljs-comment"># keep last 49,000 for train</span><br>Ytr = Ytr[<span class="hljs-number">1000</span>:]<br><br><span class="hljs-comment"># find hyperparameters that work best on the validation set</span><br>validation_accuracies = []<br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>]:<br><br>  <span class="hljs-comment"># use a particular value of k and evaluation on validation data</span><br>  nn = NearestNeighbor()<br>  nn.train(Xtr_rows, Ytr)<br>  <span class="hljs-comment"># here we assume a modified NearestNeighbor class that can take a k as input</span><br>  Yval_predict = nn.predict(Xval_rows, k = k)<br>  acc = np.mean(Yval_predict == Yval)<br>  <span class="hljs-built_in">print</span> <span class="hljs-string">&#x27;accuracy: %f&#x27;</span> % (acc,)<br><br>  <span class="hljs-comment"># keep track of what works on the validation set</span><br>  validation_accuracies.append((k, acc))<br></code></pre></td></tr></table></figure><p>By the end of this procedure, we could plot a graph that shows which values of <em>k</em> work best. We would then stick with this value and evaluate once on the actual test set.</p><blockquote><p>Split your training set into training set and a validation set. Use validation set to tune all hyperparameters. At the end run a single time on the test set and report performance.</p></blockquote><p><strong>Cross-validation</strong>. In cases where the size of your training data (and therefore also the validation data) might be small, people sometimes use a more sophisticated technique for hyperparameter tuning called <strong>cross-validation</strong>. Working with our previous example, the idea is that instead of arbitrarily picking the first 1000 datapoints to be the validation set and rest training set, you can get a better and less noisy estimate of how well a certain value of <em>k</em> works by iterating over different validation sets and averaging the performance across these. For example, in 5-fold cross-validation, we would split the training data into 5 equal folds, use 4 of them for training, and 1 for validation. We would then iterate over which fold is the validation fold, evaluate the performance, and finally average the performance across the different folds.</p><p><img src="https://cs231n.github.io/assets/cvplot.png"></p><p>Example of a 5-fold cross-validation run for the parameter <strong>k</strong>. For each value of <strong>k</strong> we train on 4 folds and evaluate on the 5th. Hence, for each <strong>k</strong> we receive 5 accuracies on the validation fold (accuracy is the y-axis, each result is a point). The trend line is drawn through the average of the results for each <strong>k</strong> and the error bars indicate the standard deviation. Note that in this particular case, the cross-validation suggests that a value of about <strong>k</strong> &#x3D; 7 works best on this particular dataset (corresponding to the peak in the plot). If we used more than 5 folds, we might expect to see a smoother (i.e. less noisy) curve.</p><p><strong>In practice</strong>. In practice, people prefer to avoid cross-validation in favor of having a single validation split, since cross-validation can be computationally expensive. The splits people tend to use is between 50%-90% of the training data for training and rest for validation. However, this depends on multiple factors: For example if the number of hyperparameters is large you may prefer to use bigger validation splits. If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation. Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation.</p><p><img src="https://cs231n.github.io/assets/crossval.jpeg"></p><p>Common data splits. A training and test set is given. The training set is split into folds (for example 5 folds here). The folds 1-4 become the training set. One fold (e.g. fold 5 here in yellow) is denoted as the Validation fold and is used to tune the hyperparameters. Cross-validation goes a step further and iterates over the choice of which fold is the validation fold, separately from 1-5. This would be referred to as 5-fold cross-validation. In the very end once the model is trained and all the best hyperparameters were determined, the model is evaluated a single time on the test data (red).</p><p><strong>Pros and Cons of Nearest Neighbor classifier.</strong></p><p>It is worth considering some advantages and drawbacks of the Nearest Neighbor classifier. Clearly, one advantage is that it is very simple to implement and understand. Additionally, the classifier takes no time to train, since all that is required is to store and possibly index the training data. However, we pay that computational cost at test time, since classifying a test example requires a comparison to every single training example. This is backwards, since in practice we often care about the test time efficiency much more than the efficiency at training time. In fact, the deep neural networks we will develop later in this class shift this tradeoff to the other extreme: They are very expensive to train, but once the training is finished it is very cheap to classify a new test example. This mode of operation is much more desirable in practice.</p><p>As an aside, the computational complexity of the Nearest Neighbor classifier is an active area of research, and several <strong>Approximate Nearest Neighbor</strong> (ANN) algorithms and libraries exist that can accelerate the nearest neighbor lookup in a dataset (e.g. <a href="https://github.com/mariusmuja/flann">FLANN</a>). These algorithms allow one to trade off the correctness of the nearest neighbor retrieval with its space&#x2F;time complexity during retrieval, and usually rely on a pre-processing&#x2F;indexing stage that involves building a kdtree, or running the k-means algorithm.</p><p>The Nearest Neighbor Classifier may sometimes be a good choice in some settings (especially if the data is low-dimensional), but it is rarely appropriate for use in practical image classification settings. One problem is that images are high-dimensional objects (i.e. they often contain many pixels), and distances over high-dimensional spaces can be very counter-intuitive. The image below illustrates the point that the pixel-based L2 similarities we developed above are very different from perceptual similarities:</p><p><img src="https://cs231n.github.io/assets/samenorm.png"></p><p>Pixel-based distances on high-dimensional data (and images especially) can be very unintuitive. An original image (left) and three other images next to it that are all equally far away from it based on L2 pixel distance. Clearly, the pixel-wise distance does not correspond at all to perceptual or semantic similarity.</p><p>Here is one more visualization to convince you that using pixel differences to compare images is inadequate. We can use a visualization technique called <a href="https://lvdmaaten.github.io/tsne/">t-SNE</a> to take the CIFAR-10 images and embed them in two dimensions so that their (local) pairwise distances are best preserved. In this visualization, images that are shown nearby are considered to be very near according to the L2 pixelwise distance we developed above:</p><p><img src="https://cs231n.github.io/assets/pixels_embed_cifar10.jpg"></p><p>CIFAR-10 images embedded in two dimensions with t-SNE. Images that are nearby on this image are considered to be close based on the L2 pixel distance. Notice the strong effect of background rather than semantic class differences. Click <a href="https://cs231n.github.io/assets/pixels_embed_cifar10_big.jpg">here</a> for a bigger version of this visualization.</p><p>In particular, note that images that are nearby each other are much more a function of the general color distribution of the images, or the type of background rather than their semantic identity. For example, a dog can be seen very near a frog since both happen to be on white background. Ideally we would like images of all of the 10 classes to form their own clusters, so that images of the same class are nearby to each other regardless of irrelevant characteristics and variations (such as the background). However, to get this property we will have to go beyond raw pixels.</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>In summary:</p><ul><li>We introduced the problem of <strong>Image Classification</strong>, in which we are given a set of images that are all labeled with a single category. We are then asked to predict these categories for a novel set of test images and measure the accuracy of the predictions.</li><li>We introduced a simple classifier called the <strong>Nearest Neighbor classifier</strong>. We saw that there are multiple hyper-parameters (such as value of k, or the type of distance used to compare examples) that are associated with this classifier and that there was no obvious way of choosing them.</li><li>We saw that the correct way to set these hyperparameters is to split your training data into two: a training set and a fake test set, which we call <strong>validation set</strong>. We try different hyperparameter values and keep the values that lead to the best performance on the validation set.</li><li>If the lack of training data is a concern, we discussed a procedure called <strong>cross-validation</strong>, which can help reduce noise in estimating which hyperparameters work best.</li><li>Once the best hyperparameters are found, we fix them and perform a single <strong>evaluation</strong> on the actual test set.</li><li>We saw that Nearest Neighbor can get us about 40% accuracy on CIFAR-10. It is simple to implement but requires us to store the entire training set and it is expensive to evaluate on a test image.</li><li>Finally, we saw that the use of L1 or L2 distances on raw pixel values is not adequate since the distances correlate more strongly with backgrounds and color distributions of images than with their semantic content.</li></ul><p>In next lectures we will embark on addressing these challenges and eventually arrive at solutions that give 90% accuracies, allow us to completely discard the training set once learning is complete, and they will allow us to evaluate a test image in less than a millisecond.</p><h3 id="Summary-Applying-kNN-in-practice"><a href="#Summary-Applying-kNN-in-practice" class="headerlink" title="Summary: Applying kNN in practice"></a>Summary: Applying kNN in practice</h3><p>If you wish to apply kNN in practice (hopefully not on images, or perhaps as only a baseline) proceed as follows:</p><ol><li>Preprocess your data: Normalize the features in your data (e.g. one pixel in images) to have zero mean and unit variance. We will cover this in more detail in later sections, and chose not to cover data normalization in this section because pixels in images are usually homogeneous and do not exhibit widely different distributions, alleviating the need for data normalization.</li><li>If your data is very high-dimensional, consider using a dimensionality reduction technique such as PCA (<a href="https://en.wikipedia.org/wiki/Principal_component_analysis">wiki ref</a>, <a href="http://cs229.stanford.edu/notes/cs229-notes10.pdf">CS229ref</a>, <a href="https://web.archive.org/web/20150503165118/http://www.bigdataexaminer.com:80/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/">blog ref</a>), NCA (<a href="https://en.wikipedia.org/wiki/Neighbourhood_components_analysis">wiki ref</a>, <a href="https://kevinzakka.github.io/2020/02/10/nca/">blog ref</a>), or even <a href="https://scikit-learn.org/stable/modules/random_projection.html">Random Projections</a>.</li><li>Split your training data randomly into train&#x2F;val splits. As a rule of thumb, between 70-90% of your data usually goes to the train split. This setting depends on how many hyperparameters you have and how much of an influence you expect them to have. If there are many hyperparameters to estimate, you should err on the side of having larger validation set to estimate them effectively. If you are concerned about the size of your validation data, it is best to split the training data into folds and perform cross-validation. If you can afford the computational budget it is always safer to go with cross-validation (the more folds the better, but more expensive).</li><li>Train and evaluate the kNN classifier on the validation data (for all folds, if doing cross-validation) for many choices of <strong>k</strong> (e.g. the more the better) and across different distance types (L1 and L2 are good candidates)</li><li>If your kNN classifier is running too long, consider using an Approximate Nearest Neighbor library (e.g. <a href="https://github.com/mariusmuja/flann">FLANN</a>) to accelerate the retrieval (at cost of some accuracy).</li><li>Take note of the hyperparameters that gave the best results. There is a question of whether you should use the full training set with the best hyperparameters, since the optimal hyperparameters might change if you were to fold the validation data into your training set (since the size of the data would be larger). In practice it is cleaner to not use the validation data in the final classifier and consider it to be <em>burned</em> on estimating the hyperparameters. Evaluate the best model on the test set. Report the test set accuracy and declare the result to be the performance of the kNN classifier on your data.</li></ol><h4 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a>Further Reading</h4><p>Here are some (optional) links you may find interesting for further reading:</p><ul><li><p><a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">A Few Useful Things to Know about Machine Learning</a>, where especially section 6 is related but the whole paper is a warmly recommended reading.</p></li><li><p><a href="https://people.csail.mit.edu/torralba/shortCourseRLOC/index.html">Recognizing and Learning Object Categories</a>, a short course of object categorization at ICCV 2005.</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
      <tag>计算机视觉</tag>
      
      <tag>Image_Classification</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DriveLM、Dolphins 复现</title>
    <link href="/2025/09/02/Drivebench%20%E5%A4%8D%E7%8E%B0/"/>
    <url>/2025/09/02/Drivebench%20%E5%A4%8D%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="学长记录"><a href="#学长记录" class="headerlink" title="学长记录"></a>学长记录</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><ol><li>从作者提供的 google drive 链接中下载的 Drivebench.zip 不包含干净数据（即最终 <code>data </code>目录下的<code>nuscenes/samples</code>子目录缺失）。根据论文附录得知数据源于 drivelm-nuscenes 数据集，因此从该数据集中找到与 <code>corruption</code> 目录中相对应的数据，提取代码见 <code>data/extract.py</code></li></ol><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><ol><li>虚拟环境名称 <code>drivebench</code></li><li>安装vllm &#x3D;&#x3D; 0.6.1.post1时，报错 outlines 依赖的 pyairports 包无法找到，pip 官方没有提供。从第三方库  <a href="https://github.com/bckim92/language-evaluation">bckim92&#x2F;language-evaluation </a> 下载到本地后上传服务器，执行<code>pip install .</code>安装。服务器路径为<code>~/yuep/Drivebench/pyairports-master</code></li><li>代码运行中 transformers、tokenizers 等库由于版本问题存在调用方法缺失。试验后可用版本为 <code>transformers==4.43.2, tokenizers==0.19.1</code></li><li>运行 evaluate 代码时需要第三方库 <code>language_evaluation</code>，pip 官方没有提供。从第三方库 <a href="https://github.com/bckim92/language-evaluation">bckim92&#x2F;language-evaluation </a>下载到本地后上传服务器，执行 <code>pip install .</code> 安装。服务器路径为 <code>~/yuep/Drivebench/language-evaluation-master</code></li></ol><h2 id="代码运行"><a href="#代码运行" class="headerlink" title="代码运行"></a>代码运行</h2><ol><li>从 <code>drivebench-test_final.json </code>中读出的 image_paths 部分数据并不包含完整的六个摄像机角度，在 python 运行时会出现类似于 <code>[CAM_BACK, None, None, None, None, None]</code> 的情况，需要注意代码是否对 None 有正确的处理（读取代码见<code>inference/utils.py</code>，容易引起报错。</li><li>llava1.5 模型默认网络下载，出现 SSL Error 报错。修改部分代码改为本地模型加载，模型存储位置为 <code>/mnt/ssd1/yuep/model</code>，后续模型下载到本地也可以存在这里</li></ol><h1 id="DriveLM配置"><a href="#DriveLM配置" class="headerlink" title="DriveLM配置"></a>DriveLM配置</h1><h2 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h2><p>主仓库：<a href="https://github.com/OpenDriveLab/DriveLM/tree/main">github链接</a><br>llama-7b 基座模型: meta申请没通过，<a href="https://huggingface.co/nyanko7/LLaMA-7B/tree/main">找的一个链接</a><br>Drivebench预训练权重：<a href="https://huggingface.co/drive-bench/drivelm-ft/tree/main">huggingface链接</a></p><h3 id="初体验"><a href="#初体验" class="headerlink" title="初体验"></a>初体验</h3><ul><li>直接用镜像站下载也下载不了，只能用梯子先下载到本地，然后在MobaXterm用<code>rsync</code>上传到服务器</li><li>MobaXterm无法直接识别windows下的路径，需要将 Windows 路径转换为 MobaXterm 可以识别的 Unix 风格路径。MobaXterm 会自动将本地磁盘挂载到 <code>/drives/</code> 目录下。<ul><li><code>D:\</code> 对应的是 <code>/drives/d/</code></li><li><code>C:\</code> 对应的是 <code>/drives/c/</code></li></ul></li><li>另外就是，在 Windows 系统中（例如使用 PowerShell 或 CMD）配置 SSH 时，配置文件默认存放在 <code>C:\Users\用户名\.ssh\config</code>。但是 <strong>MobaXterm 有自己的家</strong>: 当打开 MobaXterm 的终端时，进入的是一个独立的 Linux&#x2F;Unix 环境。这个环境有它自己的家目录 (Home Directory)，我的路径是 <code>C:\Users\pengg\AppData\Roaming\MobaXterm\home</code>。因此将.ssh中的<code>config</code> copy过来即可</li><li>如果ssh配置好了，目标服务器可以直接用别名，比如这里的GPU26<br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250902003250314.png"></li><li>后来发现可以直接从文件夹拖拽到vscode中，直接复制过去，这下joker了。<br>Updated: 实测下来直接拽进去比<code>rsync</code>要慢很多很多</li></ul><h3 id="再探索"><a href="#再探索" class="headerlink" title="再探索"></a>再探索</h3><p>起因：手滑把之前用<code>rsync</code>上传的删了🤡<br>直接用<code>hfmirror</code>镜像下载效果不了，<code>xget</code>加速也用不了。但四处碰壁后，我发现可以先<code>unset https_proxy http_proxy all_proxy</code>临时取消代理然后用 <code>huggingface-cli</code>搭配<code>hf-mirror</code>下载效果不错<br>大致流程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> HF_ENDPOINT=https://hf-mirror.com <span class="hljs-comment"># 可以写入.bashrc</span><br><span class="hljs-built_in">unset</span> https_proxy http_proxy all_proxy<br>hf download ***<br></code></pre></td></tr></table></figure><h2 id="环境配置-1"><a href="#环境配置-1" class="headerlink" title="环境配置"></a>环境配置</h2><p>git+<a href="https://github.com/csuhan/timm_0_3_2.git">https://github.com/csuhan/timm_0_3_2.git</a><br>git+<a href="https://github.com/openai/CLIP.git">https://github.com/openai/CLIP.git</a><br>网络无法直接连接，在gitclone上找到镜像链接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git+https://gitclone.com/github.com/csuhan/timm_0_3_2<br>git+https://gitclone.com/github.com/openai/CLIP<br></code></pre></td></tr></table></figure><ul><li>如果你要clone的仓库还没有缓存过，先在自己电脑上<code>clone</code>一遍，之后就可以在服务器中<code>clone</code>缓存了</li></ul><h2 id="代码运行-1"><a href="#代码运行-1" class="headerlink" title="代码运行"></a>代码运行</h2><h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><p>环境：drivebench_test(添加了CLIP等库)<br>在llama_adapter_v2目录下执行<code>inference</code>的demo.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python demo.py  --llama_dir ../weights   --checkpoint ../weights/checkpoint-<span class="hljs-number">3.</span>pth --data ../test_llama.json  --output ../output.json --batch_size <span class="hljs-number">4</span> --num_processes <span class="hljs-number">8</span><br></code></pre></td></tr></table></figure><p>此时直接执行评估会出现问题，因为上一步<strong>生成的output.json不完整</strong>，这是为什么呢？因为代码里对于gpu采用的硬编码，直接指定第一个线程让gpu:0执行，而恰好此时前两块gpu都已经占满了，直接cuda out of memory。因此<code>output.json</code>不完整，只有两百多行。目前采用的是解决措施是先<code>nvidia-smi</code>一下，然后找出可用的gpu，然后在运行代码上加一个<code>CUDA_VISIBLE_DEVICES= </code>来让程序只能识别这些gpu，完整执行代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">CUDA_VISIBLE_DEVICES=<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span> python demo.py  --llama_dir ../weights   --checkpoint ../weights/checkpoint-<span class="hljs-number">3.</span>pth --data ../test_llama.json  --output ../output.json --batch_size <span class="hljs-number">4</span> --num_processes <span class="hljs-number">6</span><br></code></pre></td></tr></table></figure><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>修复上述问题后遇到</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">Traceback (most recent call last):<br>  File <span class="hljs-string">&quot;/mnt/ssd1/yuep/model/DriveLM/evaluation.py&quot;</span>, line <span class="hljs-number">190</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    output = evaluation.evaluation()<br>  File <span class="hljs-string">&quot;/mnt/ssd1/yuep/model/DriveLM/evaluation.py&quot;</span>, line <span class="hljs-number">145</span>, <span class="hljs-keyword">in</span> evaluation<br>    scores[<span class="hljs-string">&quot;chatgpt&quot;</span>] = <span class="hljs-variable language_">self</span>.eval_chatGPT(<span class="hljs-variable language_">self</span>.GPT)<br>  File <span class="hljs-string">&quot;/mnt/ssd1/yuep/model/DriveLM/evaluation.py&quot;</span>, line <span class="hljs-number">38</span>, <span class="hljs-keyword">in</span> eval_chatGPT<br>    scores = p.<span class="hljs-built_in">map</span>(<span class="hljs-variable language_">self</span>.chatgpt_eval.forward, data)<br>TypeError: cannot pickle <span class="hljs-string">&#x27;_thread.RLock&#x27;</span> <span class="hljs-built_in">object</span><br></code></pre></td></tr></table></figure><ul><li>说是有些对象不能序列化的问题，<a href="https://github.com/OpenDriveLab/DriveLM/issues/141">issue中</a>也有人遇到同样问题，作者并未回复。</li><li>下面是AI回复</li></ul><p>您好，这个问题非常典型，是使用Python的<code>multiprocessing</code>库时经常会遇到的错误。</p><h4 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h4><ol><li><p><strong>错误类型</strong>: <code>TypeError: cannot pickle &#39;_thread.RLock&#39; object</code> 这个错误的核心是 <strong>“无法序列化（pickle）一个线程锁（RLock）对象”</strong>。</p></li><li><p><strong>为什么会发生这个错误？</strong>:</p><ul><li><strong>Pickle (序列化)</strong>: 当您使用 <code>multiprocessing.Pool</code> 时，主进程需要将任务（函数和它的参数）发送给多个子进程去执行。这个发送的过程就需要通过“序列化”（Pickling）将Python对象转换成字节流，以便在进程间传输。</li><li><strong>RLock (线程锁)</strong>: <code>RLock</code> 是一种同步工具，用来防止多个线程同时访问共享资源。<strong>这种锁对象与创建它的进程是绑定的，它本身的状态不能被序列化并发送到另一个独立的进程中去。</strong></li><li><strong>根本原因</strong>: 在您的代码 <code>evaluation.py</code> 中，<code>p.map(self.chatgpt_eval.forward, data)</code> 这一行试图将 <code>self.chatgpt_eval</code> 这个 <code>GPTEvaluation</code> 类的实例方法发送给子进程。为了发送这个方法，<code>multiprocessing</code> 试图序列化整个 <code>self.chatgpt_eval</code> 对象。然而，这个对象（或者它内部引用的 <code>openai</code> 库的对象）包含了一个不可序列化的 <code>RLock</code>，因此导致了 <code>TypeError</code>。</li></ul></li></ol><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>解决方案的核心思想是：<strong>不要试图在进程间传递复杂的对象实例，而是让每个子进程自己去创建它所需要的对象。</strong></p><ul><li>修改成功后，git了一下，<strong>原始代码在初始提交</strong></li></ul><hr><p>&#x3D;&#x3D;目前就是OpenAI的SDK无法连接，没有代理，而评估的一项内容就是&#x3D;&#x3D;<code>ChatGPT Score</code></p><h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash finetune.sh ../weights ../weights/checkpoint-3.pth finetune_data_config.yaml ../weights/sft<br><br>bash finetune.sh ../weights ../weights/pretrain/LORA-BIAS-7B.pth finetune_data_config.yaml ../weights/sft<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -u -m torch.distributed.launch \<br>    --master_port=1112 \          <span class="hljs-comment"># 分布式训练通信端口</span><br>    --nproc_per_node=4 \          <span class="hljs-comment"># 每个节点使用的GPU数量</span><br>    --use_env \                   <span class="hljs-comment"># 从环境变量读取配置</span><br>    main_finetune.py \<br>    --data_config <span class="hljs-string">&quot;<span class="hljs-variable">$CONFIG</span>&quot;</span> \     <span class="hljs-comment"># 数据配置文件路径</span><br>    --batch_size 4 \              <span class="hljs-comment"># 单GPU的batch size</span><br>    --epochs 10 \                  <span class="hljs-comment"># 总训练轮数</span><br>    --warmup_epochs 0.5 \           <span class="hljs-comment"># 学习率预热轮数</span><br>    --blr 5e-5 \                 <span class="hljs-comment"># 基础学习率</span><br>    --weight_decay 0.02 \         <span class="hljs-comment"># L2正则化系数</span><br>    --llama_path <span class="hljs-string">&quot;<span class="hljs-variable">$LLAMA_PATH</span>&quot;</span> \  <span class="hljs-comment"># LLaMA模型路径</span><br>    --output_dir <span class="hljs-string">&quot;<span class="hljs-variable">$OUTPUT_DIR</span>&quot;</span> \   <span class="hljs-comment"># 输出目录</span><br>    --pretrained_path <span class="hljs-string">&quot;<span class="hljs-variable">$PRETRAINED_PATH</span>&quot;</span> \  <span class="hljs-comment"># 预训练权重路径</span><br>    &amp;&gt;&gt; <span class="hljs-string">&quot;<span class="hljs-variable">$OUTPUT_DIR</span>&quot;</span>/output.log &amp;          <span class="hljs-comment"># 日志重定向（追加模式）</span><br></code></pre></td></tr></table></figure><h1 id="Dolphins-配置"><a href="#Dolphins-配置" class="headerlink" title="Dolphins 配置"></a>Dolphins 配置</h1><p>项目地址：<a href="https://github.com/SaFoLab-WISC/Dolphins/tree/main">github链接</a></p><h2 id="环境配置-2"><a href="#环境配置-2" class="headerlink" title="环境配置"></a>环境配置</h2><p>没办法，<code>numpy</code>版本冲突，<code>Dolphins</code> 只能新建一个<code>conda</code>环境搞了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">ERROR: pip<span class="hljs-string">&#x27;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.</span><br><span class="hljs-string">outlines 0.0.46 requires numpy&lt;2.0.0, but you have numpy 2.2.6 which is incompatible.</span><br><span class="hljs-string">pyarrow 15.0.0 requires numpy&lt;2,&gt;=1.16.6, but you have numpy 2.2.6 which is incompatible.</span><br><span class="hljs-string">vllm 0.6.1.post1 requires numpy&lt;2.0.0, but you have numpy 2.2.6 which is incompatible.</span><br></code></pre></td></tr></table></figure><p>退回到<code>numpy-1.26.4</code>后</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">ERROR: pip<span class="hljs-string">&#x27;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.</span><br><span class="hljs-string">opencv-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= &quot;3.9&quot;, but you have numpy 1.26.4 which is incompatible.</span><br></code></pre></td></tr></table></figure><h3 id="新环境"><a href="#新环境" class="headerlink" title="新环境"></a>新环境</h3><ul><li>conda：<code>dolphins</code></li><li>依旧报错连连，各种包冲突，尝试把几个包升级下，ok了</li></ul><h1 id="Yolov8"><a href="#Yolov8" class="headerlink" title="Yolov8"></a>Yolov8</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>新建环境 <code>yolo</code><br>直接<code>pip install -U ultralytics</code> 即可</p><p>然后在huggingface上下载模型权重，<a href="https://huggingface.co/Ultralytics/YOLOv8">点击链接</a></p><h2 id="处理结果"><a href="#处理结果" class="headerlink" title="处理结果"></a>处理结果</h2><p>所有 Ultralytics <code>predict()</code> 调用将返回一个 <code>Results</code> 对象列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> ultralytics <span class="hljs-keyword">import</span> YOLO<br><br><span class="hljs-comment"># Load a pretrained YOLO11n model</span><br>model = YOLO(<span class="hljs-string">&quot;yolo11n.pt&quot;</span>)<br><br><span class="hljs-comment"># Run inference on an image</span><br>results = model(<span class="hljs-string">&quot;https://ultralytics.com/images/bus.jpg&quot;</span>)<br>results = model(<br>    [<br>        <span class="hljs-string">&quot;https://ultralytics.com/images/bus.jpg&quot;</span>,<br>        <span class="hljs-string">&quot;https://ultralytics.com/images/zidane.jpg&quot;</span>,<br>    ]<br>)  <span class="hljs-comment"># batch inference</span><br></code></pre></td></tr></table></figure><p><code>Results</code> 对象具有以下<strong>属性</strong>：</p><table><thead><tr><th>属性</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td><code>orig_img</code></td><td><code>np.ndarray</code></td><td>原始图像，以 numpy 数组形式呈现。</td></tr><tr><td><code>orig_shape</code></td><td><code>tuple</code></td><td>原始图像的形状，格式为（高度，宽度）。</td></tr><tr><td><code>boxes</code></td><td><code>Boxes, optional</code></td><td>一个 Boxes 对象，包含检测到的边界框。</td></tr><tr><td><code>masks</code></td><td><code>Masks, optional</code></td><td>一个 Masks 对象，包含检测到的掩码。</td></tr><tr><td><code>probs</code></td><td><code>Probs, optional</code></td><td>一个 Probs 对象，包含分类任务中每个类别的概率。</td></tr><tr><td><code>keypoints</code></td><td><code>Keypoints, optional</code></td><td>一个 Keypoints 对象，包含每个对象检测到的关键点。</td></tr><tr><td><code>obb</code></td><td><code>OBB, optional</code></td><td>包含旋转框检测的 OBB 对象。</td></tr><tr><td><code>speed</code></td><td><code>dict</code></td><td>一个字典，包含预处理、推理和后处理的速度，单位为毫秒&#x2F;图像。</td></tr><tr><td><code>names</code></td><td><code>dict</code></td><td>一个将类索引映射到类名称的字典。</td></tr><tr><td><code>path</code></td><td><code>str</code></td><td>图像文件的路径。</td></tr><tr><td><code>save_dir</code></td><td><code>str, optional</code></td><td>用于保存结果的目录。</td></tr><tr><td></td><td></td><td></td></tr></tbody></table><p><code>Results</code> 对象具有以下<strong>方法</strong>：</p><table><thead><tr><th>方法</th><th>返回类型</th><th>描述</th></tr></thead><tbody><tr><td><code>update()</code></td><td><code>None</code></td><td>使用新的检测数据（框、掩码、概率、obb、关键点）更新 Results 对象。</td></tr><tr><td><code>cpu()</code></td><td><code>Results</code></td><td>返回 Results 对象的副本，其中所有 tensor 都已移动到 CPU 内存。</td></tr><tr><td><code>numpy()</code></td><td><code>Results</code></td><td>返回 Results 对象的副本，其中所有 tensor 都已转换为 numpy 数组。</td></tr><tr><td><code>cuda()</code></td><td><code>Results</code></td><td>返回 Results 对象的副本，其中所有 tensor 都已移动到 GPU 内存。</td></tr><tr><td><code>to()</code></td><td><code>Results</code></td><td>返回 Results 对象的副本，其中 tensor 已移动到指定的设备和数据类型。</td></tr><tr><td><code>new()</code></td><td><code>Results</code></td><td>创建一个新的 Results 对象，该对象具有相同的图像、路径、名称和速度属性。</td></tr><tr><td><code>plot()</code></td><td><code>np.ndarray</code></td><td>在输入的 RGB 图像上绘制检测结果，并返回带注释的图像。</td></tr><tr><td><code>show()</code></td><td><code>None</code></td><td>显示带有注释的推理结果的图像。</td></tr><tr><td><code>save()</code></td><td><code>str</code></td><td>将带注释的推理结果图像保存到文件并返回文件名。</td></tr><tr><td><code>verbose()</code></td><td><code>str</code></td><td>返回每个任务的日志字符串，详细说明检测和分类结果。</td></tr><tr><td><code>save_txt()</code></td><td><code>str</code></td><td>将检测结果保存到文本文件，并返回保存文件的路径。</td></tr><tr><td><code>save_crop()</code></td><td><code>None</code></td><td>将裁剪的检测图像保存到指定目录。</td></tr><tr><td><code>summary()</code></td><td><code>List[Dict[str, Any]]</code></td><td>将推理结果转换为汇总字典，可以选择进行归一化。</td></tr><tr><td><code>to_df()</code></td><td><code>DataFrame</code></td><td>将检测结果转换为极坐标数据帧。</td></tr><tr><td><code>to_csv()</code></td><td><code>str</code></td><td>将检测结果转换为 CSV 格式。</td></tr><tr><td><code>to_json()</code></td><td><code>str</code></td><td>将检测结果转换为 JSON 格式。</td></tr></tbody></table><p>有关更多详细信息，请参见 <a href="https://docs.ultralytics.com/zh/reference/engine/results/"><code>Results</code> 类文档</a>.</p><h1 id="pipeline搭建"><a href="#pipeline搭建" class="headerlink" title="pipeline搭建"></a>pipeline搭建</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>现在的问题是Drivebench的inference脚本，虽然是命名为<code>llava1.5.py</code>，但其实在代码中并未指定模型，而是一种加载<strong>标准Hugging Face格式的模型</strong>的设计。而现在的 <code>DriveLM</code> 是一个<strong>GitHub项目</strong>，它有自己独特的代码、模型结构和权重加载方式，具体如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># inference脚本</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LLMPredictor</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_name, system_prompt, sampling_params, num_images_per_prompt, max_model_len, tensor_parallel_size, corruption</span>):<br><br>        <span class="hljs-variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(<br>            model_name,<br>            trust_remote_code=<span class="hljs-literal">True</span>,<br>            local_files_only=<span class="hljs-literal">True</span>  <span class="hljs-comment"># 强制使用本地文件</span><br>        )<br><br>        <span class="hljs-variable language_">self</span>.llm = LLM(<br>            model=model_name,<br>            tokenizer=model_name,<br>            trust_remote_code=<span class="hljs-literal">True</span>,<br>            max_model_len=max_model_len,<br>            limit_mm_per_prompt=&#123;<span class="hljs-string">&quot;image&quot;</span>: num_images_per_prompt&#125;,<br>            tensor_parallel_size=tensor_parallel_size,<br>            download_dir=<span class="hljs-literal">None</span>,<br>            load_format=<span class="hljs-string">&quot;auto&quot;</span><br>        )<br></code></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python demo.py  --llama_dir ../weights   --checkpoint ../weights/checkpoint-<span class="hljs-number">3.</span>pth --data ../test_llama.json  --output ../output.json --batch_size <span class="hljs-number">4</span> --num_processes <span class="hljs-number">8</span><br></code></pre></td></tr></table></figure><h2 id="尝试"><a href="#尝试" class="headerlink" title="尝试"></a>尝试</h2><ul><li>方案A，基于DriveLM和已有权重转换为huggingface格式的模型，可能需要比较复杂的流程</li><li>方案B：直接修改llava.15.py加载模型的部分，结合运行demo.py的命令，</li></ul><p>思虑后，决定采用方案B，但是一通操作下来，发现已经有人在服务器上用Ray集群了，并且和我的版本不一样，因为conda环境的python版本不一样，所以在他运行时我不能加入，因此只能等😭😭</p><h1 id="Safeauto"><a href="#Safeauto" class="headerlink" title="Safeauto"></a>Safeauto</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash scripts/eval_bddx.sh 0.01 0.35 <span class="hljs-literal">true</span> 2 4<br>bash scripts/eval_drivelm.sh 0.01 0.35 <span class="hljs-literal">true</span> 1 4<br>bash scripts/eval_accident.sh 0.01 0.35 <span class="hljs-literal">true</span> 1 7<br>bash scripts/eval_accident.sh 0.01 0.35 <span class="hljs-literal">false</span> 1 7<br><br>bash scripts/finetune_drivelm.sh 0.01 0.35 <span class="hljs-literal">false</span> 1<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>嗑盐</category>
      
    </categories>
    
    
    <tags>
      
      <tag>嗑盐</tag>
      
      <tag>llama</tag>
      
      <tag>镜像</tag>
      
      <tag>rsync</tag>
      
      <tag>MobaXterm</tag>
      
      <tag>DriveLM</tag>
      
      <tag>Dolphins</tag>
      
      <tag>Yolo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅机器学习-Transformer</title>
    <link href="/2025/08/26/Transformer/"/>
    <url>/2025/08/26/Transformer/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="1-序列到序列-Seq2seq-任务"><a href="#1-序列到序列-Seq2seq-任务" class="headerlink" title="1. 序列到序列 (Seq2seq) 任务"></a>1. 序列到序列 (Seq2seq) 任务</h1><p>在深度学习领域，许多任务都可以被归纳为“序列到序列”（Sequence-to-sequence, <code>Seq2seq</code>）问题。这类任务的特点是输入一个序列，模型需要生成另一个序列，并且<strong>输出序列的长度可能与输入序列不同</strong>，由模型自行决定。</p><p>常见的Seq2seq应用包括：</p><ul><li><strong>机器翻译</strong>: 将一种语言的句子（序列）翻译成另一种语言的句子（序列）。</li><li><strong>语音识别</strong>: 将一段语音信号（序列）转换成文字（序列）。</li><li><strong>文本转语音 (TTS)</strong>: 将文字（序列）合成对应的语音波形（序列）。</li><li><strong>问答与对话系统</strong>: 将问题或对话上下文（序列）生成回答（序列）。</li></ul><p>甚至一些看似不相关的任务，如<strong>句法分析</strong>、<strong>多标签分类</strong>和<strong>目标检测</strong>，也可以巧妙地转化为Seq2seq问题进行处理。</p><hr><h1 id="2-Transformer：Seq2seq模型的革命"><a href="#2-Transformer：Seq2seq模型的革命" class="headerlink" title="2. Transformer：Seq2seq模型的革命"></a>2. Transformer：Seq2seq模型的革命</h1><p><strong>Transformer是一个完全基于注意力机制的Seq2seq模型</strong>，它摒弃了传统的循环神经网络（RNN）和卷积神经网络（CNN）结构。其核心思想在论文 <strong>“Attention Is All You Need”</strong> 中被提出，并迅速成为自然语言处理（NLP）领域的标准架构。</p><p>Transformer模型主要由两部分组成：<strong>编码器 (Encoder)</strong> 和 **解码器 (Decoder)**。</p><ul><li><strong>编码器 (Encoder)</strong>: 负责处理输入序列，并将其转换成一组富含上下文信息的向量表示。</li><li><strong>解码器 (Decoder)</strong>: 利用编码器的输出，并结合已经生成的部分输出，逐个生成目标序列的元素。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250826151120032.png"></p><blockquote><p><strong>论文参考</strong>:</p><ul><li><p>Attention Is All You Need: <a href="https://arxiv.org/abs/1706.03762" title="null">https://arxiv.org/abs/1706.03762</a></p></li><li><p>Sequence to Sequence Learning with Neural Networks: <a href="https://arxiv.org/abs/1409.3215" title="null">https://arxiv.org/abs/1409.3215</a></p></li></ul></blockquote><hr><h1 id="3-编码器-Encoder-详解"><a href="#3-编码器-Encoder-详解" class="headerlink" title="3. 编码器 (Encoder) 详解"></a>3. 编码器 (Encoder) 详解</h1><p>编码器的作用是“理解”输入序列。<strong>它由N个相同的层堆叠而成，每一层都包含两个核心子模块</strong>。</p><div style="text-align: center;">  <img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250826151235910.png" width="50%" alt="Encoder"></div><ol><li><p>多头自注意力 (<code>Multi-Head Self-Attention</code>)</p><p> 这个模块让输入序列中的每个词都能“关注”到序列中的所有其他词，从而计算出<strong>每个词在特定上下文中的表示</strong>。它解决了长距离依赖的问题，并且可以<strong>并行计算</strong>。</p></li><li><p>前馈神经网络 (<code>Feed Forward Network</code>)</p><p> 这是一个简单的全连接前馈网络，对自注意力模块的输出进行进一步的非线性变换。</p></li></ol><p><strong>关键细节</strong>:</p><ul><li><p><strong>残差连接 (Residual Connection) 与层归一化 (Layer Normalization)</strong>: 每个子模块（自注意力和前馈网络）的<strong>输出都与该模块的输入进行相加（残差连接）</strong>，然后进行<strong>层归一化</strong>（<code>Layer Norm</code>）。这有助于防止梯度消失，加速模型训练，并稳定训练过程。</p></li><li><p><strong>位置编码 (Positional Encoding)</strong>: 由于自注意力机制本身不包含序列的位置信息，我们需要在输入时额外<strong>加入一个“位置编码”向量</strong>，来告诉模型每个词在句子中<strong>的绝对或相对位置</strong>。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250826151948061.png"></p><blockquote><p><strong>相关论文</strong>:</p><ul><li><p>On Layer Normalization in the Transformer Architecture: <a href="https://arxiv.org/abs/2002.04745" title="null">https://arxiv.org/abs/2002.04745</a></p></li><li><p>PowerNorm: Rethinking Batch Normalization in Transformers: <a href="https://arxiv.org/abs/2003.07845" title="null">https://arxiv.org/abs/2003.07845</a></p></li></ul></blockquote><hr><h1 id="4-解码器-Decoder-详解"><a href="#4-解码器-Decoder-详解" class="headerlink" title="4. 解码器 (Decoder) 详解"></a>4. 解码器 (Decoder) 详解</h1><p>解码器的任务是生成输出序列。它同样由N个相同的层堆叠而成，但每层比编码器多了一个模块。</p><ol><li><p>带掩码的多头自注意力 (<strong>Masked Multi-Head Self-Attention</strong>)</p><p> 与编码器类似，但增加了一个“掩码”（<code>Mask</code>）。这个掩码确保在预测第 i 个词时，<strong>模型只能关注到它前面已经生成的 1 到 i-1 个词，而不能“看到”未来的词</strong>。这是为了保证解码过程的自回归（<code>Autoregressive</code>）特性。</p></li><li><p>交叉注意力 (<strong>Cross-Attention</strong>)</p><p> <strong>这是连接编码器和解码器的桥梁</strong>。在这个模块中，<strong>Query 来自于解码器的上一个模块（带掩码的自注意力），而 Key 和 Value 则来自于编码器的最终输出</strong>。这使得解码器在生成每个词时，能够关注到输入序列的所有部分，并从中提取最相关的信息。</p></li><li><p>前馈神经网络 (Feed Forward Network)</p><p> 与编码器中的前馈网络功能相同。</p></li></ol><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250826152258238.png"></p><hr><h1 id="解码过程：自回归-Autoregressive-AT"><a href="#解码过程：自回归-Autoregressive-AT" class="headerlink" title="解码过程：自回归 (Autoregressive, AT)"></a>解码过程：自回归 (Autoregressive, AT)</h1><p>见上图，解码器生成输出是一个逐词进行的过程：</p><ol><li>输入一个特殊的起始符 <code>&lt;BEGIN&gt;</code>。</li><li>解码器根据 <code>&lt;BEGIN&gt;</code> 和编码器的输出，生成第一个词（例如“机”）。</li><li>将已生成的“机”作为下一步的输入，解码器再生成第二个词（例如“器”）。</li><li>重复此过程，直到解码器生成一个特殊的终止符 <code>&lt;END&gt;</code> 为止。</li></ol><p>对比：非自回归 (Non-autoregressive, <code>NAT</code>)</p><p>NAT解码器<strong>尝试一次性生成所有输出，而不是逐词生成</strong>。这样做可以实现完全并行，速度更快，但在某些任务上，由于问题的多模态性（一个输入可能对应多个合理输出），其性能通常不如AT模型。</p><hr><h1 id="5-训练技巧与策略"><a href="#5-训练技巧与策略" class="headerlink" title="5. 训练技巧与策略"></a>5. 训练技巧与策略</h1><ul><li><p><strong>Teacher Forcing</strong>: 在训练阶段，为了提高效率和稳定性，无论模型上一步预测出什么词，我们总是将<strong>正确的</strong>上一个词作为当前步的输入。这就像有一个“老师”在不断纠正模型的路径。</p></li><li><p><strong>Exposure Bias</strong>: Teacher Forcing 导致了训练与测试之间的不匹配（<code>mismatch</code>）。在测试时，模型必须依赖自己之前生成的（可能错误的）词，这种差异被称为“暴露偏差”。</p></li><li><p><strong>Scheduled Sampling</strong>: 一种缓解<code>Exposure Bias</code>  的策略。在训练过程中，以一定概率选择使用模型的上一时刻预测输出或真实的标签作为当前时刻的输入。</p></li><li><p><strong>Beam Search</strong>: 在解码（生成）阶段，为了找到比贪心策略（每次都选概率最高的词）更优的输出序列，可以使用束搜索。<strong>它在每一步都保留k个最可能的候选序列，并在下一步从这k个序列出发继续扩展，从而在全局上找到更好的解</strong>。</p></li></ul><blockquote><p><strong>相关论文</strong>:</p><ul><li><p>The Curious Case of Neural Text Degeneration: <a href="https://arxiv.org/abs/1904.09751" title="null">https://arxiv.org/abs/1904.09751</a></p></li><li><p>Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks: <a href="https://arxiv.org/abs/1506.03099" title="null">https://arxiv.org/abs/1506.03099</a></p></li></ul></blockquote><h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h1><p>Transformer凭借其强大的并行计算能力和对长距离依赖的出色建模，彻底改变了Seq2seq任务的格局。其核心的<strong>自注意力机制</strong>、<strong>多头注意力</strong>、<strong>位置编码</strong>以及<strong>Encoder-Decoder架构</strong>共同构成了这一强大模型的基础，并催生了BERT、GPT等一系列在AI领域产生深远影响的模型。</p>]]></content>
    
    
    <categories>
      
      <category>ML&amp;DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Self_Attention</tag>
      
      <tag>注意力</tag>
      
      <tag>Transformer</tag>
      
      <tag>Seq2seq</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅机器学习-HW3 Notebook</title>
    <link href="/2025/08/22/HW3%20Notebook/"/>
    <url>/2025/08/22/HW3%20Notebook/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我完成李宏毅机器学习2021作业3时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><p>😆 作业2没整出来什么，直接看SOTA 方法了，也就没什么笔记</p><h1 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h1><h2 id="Homework-3-Convolutional-Neural-Network"><a href="#Homework-3-Convolutional-Neural-Network" class="headerlink" title="Homework 3 - Convolutional Neural Network"></a>Homework 3 - Convolutional Neural Network</h2><p>This is the example code of homework 3 of the machine learning course by Prof. Hung-yi Lee.<br>In this homework, you are required to build a <strong>convolutional neural network for image classification</strong>, possibly with some advanced training tips.</p><p>There are three levels here:<br><strong>Easy</strong>: Build a simple convolutional neural network as the baseline. (2 pts)<br><strong>Medium</strong>: Design a better architecture or adopt different data augmentations to improve the performance. (2 pts)<br><strong>Hard</strong>: Utilize provided unlabeled data to obtain better results. (2 pts)</p><h1 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h1><p>直接跑原始代码，会发现<strong>train的acc显著高于val的acc</strong>，我们的模型<strong>在train_data上过拟合了</strong>，过分依赖训练数据甚至是某些局部特征，因此，需要采取一些措施来防止过拟合</p><p>我大致修改了这些地方：</p><ul><li>使用数据增强</li><li>修改模型结构</li><li>超参数调整</li></ul><h1 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h1><p>对于一张食物图片，以我们人眼看来，将图片旋转少许角度，将图片横向翻转，改变图片的尺寸、亮度、对比度等，我们用肉眼依然能够分辨，但model就不一定了，因此，我们可以用数据增强来提升模型的泛化能力、鲁棒性，减轻一定程度的过拟合。</p><p>Data Augmentation并不直接修改原始数据，而是<strong>在样本加载时动态随机生成另一个新样本</strong></p><p>问了下deepseek-v3<br>本指南将详细解析 <code>torchvision.transforms</code> 中各种数据增强方法的参数，并介绍如何使用 <strong>AutoAugment</strong> 和 <strong>RandAugment</strong> 这两种高级增强策略。</p><hr><h2 id="1-基础几何变换方法"><a href="#1-基础几何变换方法" class="headerlink" title="1. 基础几何变换方法"></a>1. 基础几何变换方法</h2><h3 id="1-RandomHorizontalFlip-RandomVerticalFlip"><a href="#1-RandomHorizontalFlip-RandomVerticalFlip" class="headerlink" title="(1) RandomHorizontalFlip &#x2F; RandomVerticalFlip"></a>(1) <code>RandomHorizontalFlip</code> &#x2F; <code>RandomVerticalFlip</code></h3><ul><li><strong>作用</strong>：随机水平或垂直翻转图像。</li><li><strong>参数</strong>： <code>p</code> (float): 翻转概率（默认 <code>0.5</code>）。</li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.RandomHorizontalFlip(p=<span class="hljs-number">0.7</span>)  <span class="hljs-comment"># 70%概率水平翻转</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="2-RandomRotation"><a href="#2-RandomRotation" class="headerlink" title="(2) RandomRotation"></a>(2) <code>RandomRotation</code></h3><ul><li><strong>作用</strong>：随机旋转图像。</li><li><strong>参数</strong>：<ul><li><code>degrees</code> (float or tuple): 旋转角度范围（如 <code>30</code> 表示 <code>±30°</code>，<code>(10, 30)</code> 表示 <code>10°~30°</code>）。</li><li><code>expand</code> (bool): 是否扩大图像尺寸以容纳旋转后的内容（默认 <code>False</code>）。</li><li><code>fill</code> (tuple): 旋转后空白区域的填充值（RGB格式，如 <code>(255, 0, 0)</code> 表示红色）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.RandomRotation(degrees=(-<span class="hljs-number">15</span>, <span class="hljs-number">15</span>), fill=(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 旋转±15°，空白填黑色</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="3-RandomResizedCrop"><a href="#3-RandomResizedCrop" class="headerlink" title="(3) RandomResizedCrop"></a>(3) <code>RandomResizedCrop</code></h3><ul><li><strong>作用</strong>：随机裁剪并缩放到指定尺寸。</li><li><strong>参数</strong>：<ul><li><code>size</code> (int or tuple): 目标尺寸（如 <code>224</code> 或 <code>(224, 224)</code>）。</li><li><code>scale</code> (tuple): 裁剪面积比例范围（如 <code>(0.08, 1.0)</code> 表示裁剪原图的 <code>8%~100%</code>）。</li><li><code>ratio</code> (tuple): 宽高比范围（如 <code>(0.75, 1.33)</code>）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.RandomResizedCrop(size=<span class="hljs-number">224</span>, scale=(<span class="hljs-number">0.8</span>, <span class="hljs-number">1.0</span>), ratio=(<span class="hljs-number">0.9</span>, <span class="hljs-number">1.1</span>))<br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="2-颜色空间变换方法"><a href="#2-颜色空间变换方法" class="headerlink" title="2. 颜色空间变换方法"></a>2. 颜色空间变换方法</h2><h3 id="1-ColorJitter"><a href="#1-ColorJitter" class="headerlink" title="(1) ColorJitter"></a>(1) <code>ColorJitter</code></h3><ul><li><strong>作用</strong>：随机调整亮度、对比度、饱和度和色调。</li><li><strong>参数</strong>：<ul><li><code>brightness</code> (float or tuple): 亮度调整范围（如 <code>0.2</code> 表示 <code>±20%</code>）。</li><li><code>contrast</code> &#x2F; <code>saturation</code> &#x2F; <code>hue</code>: 类似亮度，但 <code>hue</code> 范围是 <code>[-0.5, 0.5]</code>。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.ColorJitter(brightness=<span class="hljs-number">0.2</span>, contrast=<span class="hljs-number">0.2</span>, saturation=<span class="hljs-number">0.2</span>, hue=<span class="hljs-number">0.1</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-RandomGrayscale"><a href="#2-RandomGrayscale" class="headerlink" title="(2) RandomGrayscale"></a>(2) <code>RandomGrayscale</code></h3><ul><li><strong>作用</strong>：以概率 <code>p</code> 将图像转为灰度。</li><li><strong>参数</strong>：<ul><li><code>p</code> (float): 灰度化概率（默认 <code>0.1</code>）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.RandomGrayscale(p=<span class="hljs-number">0.3</span>)  <span class="hljs-comment"># 30%概率灰度化</span><br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="3-高级增强方法"><a href="#3-高级增强方法" class="headerlink" title="3. 高级增强方法"></a>3. 高级增强方法</h2><h3 id="1-AutoAugment"><a href="#1-AutoAugment" class="headerlink" title="(1) AutoAugment"></a>(1) <code>AutoAugment</code></h3><ul><li><strong>作用</strong>：基于强化学习搜索的增强策略（针对特定数据集如 ImageNet、CIFAR10 优化）。</li><li><strong>参数</strong>：<ul><li><code>policy</code> (str): 预定义策略（<code>imagenet</code> &#x2F; <code>cifar10</code> &#x2F; <code>svhn</code>）。</li><li><code>interpolation</code> (InterpolationMode): 插值方法（如 <code>BILINEAR</code>）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> AutoAugment, AutoAugmentPolicy<br>transform = transforms.Compose([<br>    AutoAugment(policy=AutoAugmentPolicy.IMAGENET),  <span class="hljs-comment"># 使用ImageNet策略</span><br>    transforms.ToTensor()<br>])<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-RandAugment"><a href="#2-RandAugment" class="headerlink" title="(2) RandAugment"></a>(2) <code>RandAugment</code></h3><ul><li><strong>作用</strong>：简化版 AutoAugment，随机选择 <code>N</code> 种变换并统一强度 <code>M</code>。</li><li><strong>参数</strong>：<ul><li><code>num_ops</code> (int): 每次增强应用的变换数量（默认 <code>2</code>）。</li><li><code>magnitude</code> (int): 强度值（0~30，默认 <code>9</code>）。</li><li><code>num_magnitude_bins</code> (int): 强度离散化级别（默认 <code>31</code>）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> RandAugment<br>transform = transforms.Compose([<br>    RandAugment(num_ops=<span class="hljs-number">3</span>, magnitude=<span class="hljs-number">9</span>),  <span class="hljs-comment"># 随机选3种变换，强度9</span><br>    transforms.ToTensor()<br>])<br></code></pre></td></tr></table></figure></li></ul><h3 id="3-TrivialAugmentWide"><a href="#3-TrivialAugmentWide" class="headerlink" title="(3) TrivialAugmentWide"></a>(3) <code>TrivialAugmentWide</code></h3><ul><li><strong>作用</strong>：更简单的 RandAugment 变体，<strong>无需调参</strong>。</li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> TrivialAugmentWide<br>transform = transforms.Compose([<br>    TrivialAugmentWide(),  <span class="hljs-comment"># 自动选择变换和强度</span><br>    transforms.ToTensor()<br>])<br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="4-其他实用方法"><a href="#4-其他实用方法" class="headerlink" title="4. 其他实用方法"></a>4. 其他实用方法</h2><h3 id="1-GaussianBlur"><a href="#1-GaussianBlur" class="headerlink" title="(1) GaussianBlur"></a>(1) <code>GaussianBlur</code></h3><ul><li><strong>作用</strong>：高斯模糊（模拟焦距变化）。</li><li><strong>参数</strong>：<ul><li><code>kernel_size</code> (int or tuple): 卷积核大小（必须为奇数）。</li><li><code>sigma</code> (float or tuple): 标准差范围（如 <code>(0.1, 2.0)</code>）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.GaussianBlur(kernel_size=<span class="hljs-number">5</span>, sigma=(<span class="hljs-number">0.1</span>, <span class="hljs-number">2.0</span>))<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-RandomErasing"><a href="#2-RandomErasing" class="headerlink" title="(2) RandomErasing"></a>(2) <code>RandomErasing</code></h3><ul><li><strong>作用</strong>：随机擦除图像区域（类似 Cutout）。</li><li><strong>参数</strong>：<ul><li><code>p</code> (float): 擦除概率。</li><li><code>scale</code> &#x2F; <code>ratio</code>: 擦除区域的面积和宽高比范围。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.RandomErasing(p=<span class="hljs-number">0.5</span>, scale=(<span class="hljs-number">0.02</span>, <span class="hljs-number">0.2</span>), ratio=(<span class="hljs-number">0.3</span>, <span class="hljs-number">3.3</span>))<br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="5-完整示例代码"><a href="#5-完整示例代码" class="headerlink" title="5. 完整示例代码"></a>5. 完整示例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> AutoAugment, AutoAugmentPolicy, RandAugment<br><br><span class="hljs-comment"># 基础增强组合</span><br>basic_transform = transforms.Compose([<br>    transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<br>    transforms.RandomHorizontalFlip(),<br>    transforms.ColorJitter(brightness=<span class="hljs-number">0.2</span>, contrast=<span class="hljs-number">0.2</span>),<br>    transforms.ToTensor(),<br>])<br><br><span class="hljs-comment"># AutoAugment（ImageNet优化策略）</span><br>auto_transform = transforms.Compose([<br>    AutoAugment(policy=AutoAugmentPolicy.IMAGENET),<br>    transforms.ToTensor(),<br>])<br><br><span class="hljs-comment"># RandAugment（自定义强度和变换数量）</span><br>rand_transform = transforms.Compose([<br>    RandAugment(num_ops=<span class="hljs-number">3</span>, magnitude=<span class="hljs-number">9</span>),<br>    transforms.ToTensor(),<br>])<br></code></pre></td></tr></table></figure><hr><h2 id="6-选择建议"><a href="#6-选择建议" class="headerlink" title="6. 选择建议"></a>6. 选择建议</h2><ul><li><strong>简单任务</strong>：基础几何变换 + <code>ColorJitter</code>。</li><li><strong>复杂任务</strong>：优先尝试 <code>AutoAugment</code>（预训练策略）或 <code>RandAugment</code>（需调参）。</li><li><strong>资源有限</strong>：使用 <code>TrivialAugmentWide</code>（免调参）。</li></ul><p>通过合理组合这些方法，可以显著提升图像分类模型的泛化能力！ 🚀</p><p>反正我试了一下几种方法，也包括自己diy的，竟然<code>TrivialAugmentWide</code>的效果是最好的，看来它必然有它的道理</p><h1 id="CNN架构"><a href="#CNN架构" class="headerlink" title="CNN架构"></a>CNN架构</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Classifier</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Classifier, <span class="hljs-variable language_">self</span>).__init__()<br><br>        <span class="hljs-comment"># The arguments for commonly used modules:</span><br><br>        <span class="hljs-comment"># torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)</span><br>        <span class="hljs-comment"># torch.nn.MaxPool2d(kernel_size, stride, padding)</span><br><br>        <span class="hljs-comment"># input image size: [3, 128, 128]</span><br><br>        <span class="hljs-variable language_">self</span>.cnn_layers = nn.Sequential(<br><br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>),<br><br>            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">128</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>),<br><br>            nn.Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">256</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>),<br><br>        )<br><br>        <span class="hljs-variable language_">self</span>.fc_layers = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">256</span> * <span class="hljs-number">8</span> * <span class="hljs-number">8</span>, <span class="hljs-number">256</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">11</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># input (x): [batch_size, 3, 128, 128]</span><br>        <span class="hljs-comment"># output: [batch_size, 11]</span><br>        <br>        <span class="hljs-comment"># Extract features by convolutional layers.</span><br>        x = <span class="hljs-variable language_">self</span>.cnn_layers(x)<br><br>        <span class="hljs-comment"># The extracted feature map must be flatten before going to fully-connected layers.</span><br>        x = x.flatten(<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># The features are transformed by fully-connected layers to obtain the final logits.</span><br>        x = <span class="hljs-variable language_">self</span>.fc_layers(x)<br><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h2><p>先解释下<code>nn.Conv2d(3, 64, 3, 1, 1)</code>,分别对应 in_channels, out_channels, kernel_size, stride, padding</p><ul><li><strong>输入通道（<code>in_channels</code>）</strong>：输入数据的特征层数（如RGB图像的3通道、前一层的输出通道数）。</li><li><strong>输出通道（<code>out_channels</code>）</strong>：卷积后生成的特征图数量（即卷积核的数量）。</li><li><strong>每个输出通道</strong>由<strong>一个独立的卷积核</strong>生成，该卷积核会扫描所有输入通道并加权求和。</li></ul><h3 id="输入输出通道的设计逻辑"><a href="#输入输出通道的设计逻辑" class="headerlink" title="输入输出通道的设计逻辑"></a>输入输出通道的设计逻辑</h3><h4 id="1-通道数的变化规律"><a href="#1-通道数的变化规律" class="headerlink" title="(1) 通道数的变化规律"></a>(1) 通道数的变化规律</h4><ul><li><strong>经典CNN模式</strong>：<ul><li>随着网络加深，空间尺寸（<code>H, W</code>）逐渐减小（通过池化或大步长卷积）。</li><li>通道数逐渐增加（如 <code>3 → 64 → 128 → 256</code>），以保留更多高阶特征信息。</li><li><strong>原因</strong>：空间信息压缩后，需增加通道维度来维持信息容量。</li></ul></li></ul><h4 id="2-为什么需要多输出通道？"><a href="#2-为什么需要多输出通道？" class="headerlink" title="(2) 为什么需要多输出通道？"></a>(2) 为什么需要多输出通道？</h4><ul><li><strong>多样性特征提取</strong>：<strong>不同卷积核检测不同模式（如边缘、颜色、纹理</strong>）。<ul><li>示例：第一层的输出通道可能分别对应“水平边缘”、“垂直边缘”、“红色区域”等特征。</li></ul></li><li><strong>特征组合</strong>：深层卷积通过混合多通道输入，生成更复杂的特征（如“眼睛+鼻子”组合为人脸）。</li></ul><h4 id="3-输入输出通道的约束"><a href="#3-输入输出通道的约束" class="headerlink" title="(3) 输入输出通道的约束"></a>(3) 输入输出通道的约束</h4><ul><li><strong>输入通道必须匹配</strong>：<ul><li>前一层的 <code>out_channels</code> 必须等于后一层的 <code>in_channels</code>。</li><li>例如：<code>Conv2d(64, 128)</code> 的下一层必须是 <code>Conv2d(128, ...)</code>。</li></ul></li><li><strong>输出通道自由选择</strong>：<ul><li>通常取2的幂次（如64、128、256），方便GPU内存对齐优化。</li></ul></li></ul><h3 id="卷积过程"><a href="#卷积过程" class="headerlink" title="卷积过程"></a>卷积过程</h3><p>可视化图：<br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250823164942361.gif"></p><h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><ul><li>原始数据集中有很大一部分是没有<code>label</code>的，如果想要提高模型准确性与泛化能力，必须利用好这部分数据，这就涉及到半监督学习 <code>semi-supervised learning</code></li><li>具体就是先用已有模型去预测数据，置信度大于<code>threshold</code>就认为这个<code>label</code>是正确的，将其加入训练集，进入下一轮训练</li><li>具体需要我们做的就是<code>filter</code>一下即可</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_pseudo_labels</span>(<span class="hljs-params">dataset, model, threshold=<span class="hljs-number">0.7</span></span>):<br>    <span class="hljs-comment"># This functions generates pseudo-labels of a dataset using given model.</span><br>    <span class="hljs-comment"># It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.</span><br>    <span class="hljs-comment"># You are NOT allowed to use any models trained on external data for pseudo-labeling.</span><br>    device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br><br>    <span class="hljs-comment"># Construct a data loader.</span><br>    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-comment"># Make sure the model is in eval mode.</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-comment"># Define softmax function.</span><br>    softmax = nn.Softmax(dim=-<span class="hljs-number">1</span>)<br>    new_dataset = []<br><br>    <span class="hljs-comment"># Iterate over the dataset by batches.</span><br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> tqdm(data_loader):<br>        img, _ = batch<br><br>        <span class="hljs-comment"># Forward the data</span><br>        <span class="hljs-comment"># Using torch.no_grad() accelerates the forward process.</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            logits = model(img.to(device))<br><br>        <span class="hljs-comment"># Obtain the probability distributions by applying softmax on logits.</span><br>        probs = softmax(logits)<br><br>        <span class="hljs-comment"># ---------- TODO ----------</span><br>        <span class="hljs-comment"># Filter the data and construct a new dataset.</span><br>        max_probs, preds = torch.<span class="hljs-built_in">max</span>(probs, dim=-<span class="hljs-number">1</span>)<br>        mask = max_probs &gt;= threshold<br>        selected_imgs = img[mask]<br>        selected_labels = preds[mask]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(selected_imgs.size(<span class="hljs-number">0</span>)):<br>            new_dataset.append((selected_imgs[i], selected_labels[i].item()))<br>            <br>        dataset = torch.utils.data.DatasetFolder(new_dataset, loader=image_loader, extensions=<span class="hljs-string">&quot;jpg&quot;</span>, transform = train_tfm)  <span class="hljs-comment"># Dummy loader and extensions</span><br>    <span class="hljs-comment"># -------------------------- </span><br><br>    <span class="hljs-comment"># # Turn off the eval mode.</span><br>    model.train()<br>    <span class="hljs-keyword">return</span> dataset<br></code></pre></td></tr></table></figure><h2 id="mask"><a href="#mask" class="headerlink" title="mask"></a>mask</h2><ul><li>这里比较有趣的是<code>mask</code>，为什么能直接索引数据了，这不是一个判断条件么？其实这是一种<strong>高效张量操作方式</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">mask = max_probs &gt;= threshold<br>      selected_imgs = img[mask]<br>      selected_labels = preds[mask]<br></code></pre></td></tr></table></figure></li></ul><h3 id="本质：布尔掩码索引（Boolean-Masking）"><a href="#本质：布尔掩码索引（Boolean-Masking）" class="headerlink" title="本质：布尔掩码索引（Boolean Masking）"></a>本质：布尔掩码索引（Boolean Masking）</h3><p>PyTorch 的张量（<code>Tensor</code>）支持类似 NumPy 的 <strong>高级索引（Advanced Indexing）</strong>，其中布尔掩码索引是核心特性之一：<br> <strong>关键点</strong></p><ul><li><code>max_probs &gt;= threshold</code> 会生成一个与原张量形状相同的布尔张量（<code>Tensor</code> of <code>bool</code>）</li><li><code>img[mask]</code> 会返回所有 <code>mask</code> 为 <code>True</code> 位置对应的数据子集<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">mask = max_probs &gt;= threshold  <span class="hljs-comment"># 生成布尔张量（True/False）</span><br>selected_imgs = img[mask]      <span class="hljs-comment"># 用布尔张量筛选数据</span><br><br>max_probs = tensor([<span class="hljs-number">0.9</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.3</span>])<br>mask = max_probs &gt;= <span class="hljs-number">0.7</span>  <span class="hljs-comment"># tensor([ True, False,  True, False])</span><br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>ML&amp;DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅机器学习-Self-Attention</title>
    <link href="/2025/08/21/Self-Attention/"/>
    <url>/2025/08/21/Self-Attention/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习李宏毅机器学习时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="1-引言：处理向量集合的挑战"><a href="#1-引言：处理向量集合的挑战" class="headerlink" title="1. 引言：处理向量集合的挑战"></a>1. 引言：处理向量集合的挑战</h1><p>传统神经网络通常处理固定大小的向量输入，但现实应用中输入常为<strong>长度可变的向量集合</strong>，例如：</p><ul><li><strong>文本</strong>：句子可视为词向量组成的集合</li><li><strong>语音</strong>：音频可转换为帧向量序列</li><li><strong>图结构</strong>：社交网络或分子可视为节点向量集合</li></ul><p><code>Self-attention</code> 能有效处理此类输入，让模型在处理每个向量时<strong>动态考虑集合中其他向量的信息</strong></p><hr><h1 id="2-Self-Attention-核心思想"><a href="#2-Self-Attention-核心思想" class="headerlink" title="2. Self-Attention 核心思想"></a>2. Self-Attention 核心思想</h1><p>处理序列中的向量时，需结合上下文信息。例如判断句子 <em>“I saw a saw”</em> 中第二个 <em>“saw”</em> 的词性时，必须考虑其上下文。</p><p>Self-attention 的核心：</p><ul><li>为序列中每个向量生成输出</li><li>输出值 &#x3D; 自身信息 + 与其他向量的关联程度</li><li>通过<strong>注意力分数</strong>量化关联性，加权汇总信息</li></ul><hr><h1 id="3-Self-Attention-计算步骤"><a href="#3-Self-Attention-计算步骤" class="headerlink" title="3. Self-Attention 计算步骤"></a>3. Self-Attention 计算步骤</h1><p>以输入向量序列 $a^1, a^2, a^3, a^4$ 为例：</p><h2 id="3-1-生成-Query-Key-Value-向量"><a href="#3-1-生成-Query-Key-Value-向量" class="headerlink" title="3.1 生成 Query, Key, Value 向量"></a>3.1 生成 Query, Key, Value 向量</h2><p>对每个输入向量 $a^i$：</p><ol><li><strong>Query（$q^i$）</strong>：当前向量，主动”查询”与其他向量的关联性</li><li><strong>Key（$k^i$）</strong>：其他向量，被 Query “查询”的目标</li><li>**Value（$v^i$**）：向量本身的信息载体</li></ol><p>$$\begin{aligned}<br>q^i &amp;&#x3D; W^q a^i \<br>k^i &amp;&#x3D; W^k a^i \<br>v^i &amp;&#x3D; W^v a^i<br>\end{aligned}$$<br>（$W^q, W^k, W^v$ 为<strong>可学习的权重矩阵</strong>）</p><hr><h2 id="3-2-计算注意力分数"><a href="#3-2-计算注意力分数" class="headerlink" title="3.2 计算注意力分数"></a>3.2 计算注意力分数</h2><p>计算向量 $a^1$ 对 $a^i$ 的注意力分数：<br>$$\alpha_{1,i} &#x3D; q^1 \cdot k^i$$<br>（点积运算衡量向量间的相关性）<br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250821235623677.png"></p><hr><h2 id="3-3-分数归一化-Softmax"><a href="#3-3-分数归一化-Softmax" class="headerlink" title="3.3 分数归一化 (Softmax)"></a>3.3 分数归一化 (Softmax)</h2><p>使用 Softmax 将分数转化为概率分布：<br>$$\alpha’<em>{1,i} &#x3D; \frac{\exp(\alpha</em>{1,i})}{\sum_j \exp(\alpha_{1,j})}$$<br>（保证 $\sum_i \alpha’_{1,i} &#x3D; 1$）</p><hr><h2 id="3-4-加权求和输出"><a href="#3-4-加权求和输出" class="headerlink" title="3.4 加权求和输出"></a>3.4 加权求和输出</h2><p>以归一化分数为权重，对所有 Value 加权求和：<br>$$b^1 &#x3D; \sum_i \alpha’_{1,i} v^i$$<br>（最终输出 $b^1$ 融合了全局相关信息）<br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250821235720579.png"></p><blockquote><p>⚡ 优势：对序列中所有向量的计算可<strong>并行执行</strong>，parallel</p></blockquote><hr><h1 id="4-矩阵运算实现"><a href="#4-矩阵运算实现" class="headerlink" title="4. 矩阵运算实现"></a>4. 矩阵运算实现</h1><p>通过矩阵运算高效实现上述过程：</p><table><thead><tr><th>步骤</th><th>计算式</th><th>说明</th></tr></thead><tbody><tr><td>输入矩阵</td><td>$I &#x3D; [a^1;a^2;…]$</td><td>堆叠输入向量</td></tr><tr><td>Q&#x2F;K&#x2F;V矩阵</td><td>$\begin{aligned} Q &amp;&#x3D; I W^q \ K &amp;&#x3D; I W^k \ V &amp;&#x3D; I W^v \end{aligned}$</td><td>批量生成向量</td></tr><tr><td>注意力矩阵</td><td>$A &#x3D; K^T Q$</td><td>计算所有注意力分数</td></tr><tr><td>归一化</td><td>$A’ &#x3D; \text{softmax}(A)$</td><td>行方向归一化</td></tr><tr><td>输出</td><td>$O &#x3D; V A’$</td><td>加权求和结果</td></tr></tbody></table><hr><h1 id="5-进阶机制"><a href="#5-进阶机制" class="headerlink" title="5. 进阶机制"></a>5. 进阶机制</h1><h2 id="5-1-多头注意力（Multi-Head）"><a href="#5-1-多头注意力（Multi-Head）" class="headerlink" title="5.1 多头注意力（Multi-Head）"></a>5.1 多头注意力（Multi-Head）</h2><ul><li>使用多组<strong>独立的</strong> $(W^q, W^k, W^v)$</li><li><strong>每组学习不同类型的关联特征</strong></li><li>输出拼接后线性变换：<br>$$\text{Output} &#x3D; \text{Concat}(head_1,…,head_h)W^o$$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250821235944414.png"></p><hr><h2 id="5-2-位置编码（Positional-Encoding）"><a href="#5-2-位置编码（Positional-Encoding）" class="headerlink" title="5.2 位置编码（Positional Encoding）"></a>5.2 位置编码（Positional Encoding）</h2><p>自注意力本身<strong>不包含位置信息</strong>，需显式添加位置编码 $e^i$：<br>$$\tilde{a}^i &#x3D; a^i + e^i$$<br>编码方式：</p><ul><li><strong>固定模式</strong>：正弦&#x2F;余弦函数<br>$$e^i_t &#x3D; \begin{cases}<br>\sin(\omega_k t) &amp; i&#x3D;2k \<br>\cos(\omega_k t) &amp; i&#x3D;2k+1<br>\end{cases}$$</li><li><strong>可学习模式</strong>：随机初始化并训练更新</li></ul><hr><h1 id="6-与其他神经网络对比"><a href="#6-与其他神经网络对比" class="headerlink" title="6. 与其他神经网络对比"></a>6. 与其他神经网络对比</h1><table><thead><tr><th>特性</th><th>Self-Attention</th><th>CNN</th><th>RNN</th></tr></thead><tbody><tr><td><strong>感受野</strong></td><td>动态全局</td><td>固定局部</td><td>顺序局部</td></tr><tr><td><strong>长距离依赖</strong></td><td>强</td><td>需多层堆叠</td><td>弱（梯度消失）</td></tr><tr><td><strong>并行计算</strong></td><td>高</td><td>高</td><td>低</td></tr><tr><td><strong>计算复杂度</strong></td><td>$O(n^2 d)$</td><td>$O(k n d^2)$</td><td>$O(n d^2)$</td></tr><tr><td><strong>数据需求</strong></td><td>大型数据集</td><td>中小数据集</td><td>中小数据集</td></tr></tbody></table><blockquote><ul><li><strong>与CNN关系</strong>：可视为动态感受野的复杂卷积</li><li><strong>与RNN关系</strong>：解决了并行计算与长距离依赖问题</li></ul></blockquote><hr><h1 id="7-核心应用"><a href="#7-核心应用" class="headerlink" title="7. 核心应用"></a>7. 核心应用</h1><ol><li><strong>Transformer</strong>：构成核心运算层（如Encoder&#x2F;Decoder）</li><li><strong>NLP领域</strong>：<ul><li>BERT&#x2F;GPT系列预训练模型</li><li>机器翻译、文本摘要、问答系统</li></ul></li><li><strong>计算机视觉</strong>：<ul><li>Vision Transformer (ViT)</li><li>目标检测（DETR）</li></ul></li><li><strong>语音处理</strong>：<ul><li>语音识别（Conformer）</li><li>语音合成（Tacotron 2）</li></ul></li><li><strong>图神经网络</strong>：<ul><li>节点分类（Graph Attention Networks）</li></ul></li></ol><hr><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><table><thead><tr><th>关键特征</th><th>核心价值</th></tr></thead><tbody><tr><td><strong>动态权重分配</strong></td><td>根据上下文自适应调整重要性</td></tr><tr><td><strong>全局感知能力</strong></td><td>单层捕获长距离依赖</td></tr><tr><td><strong>高度并行化</strong></td><td>加速大规模数据处理</td></tr><tr><td><strong>架构灵活性</strong></td><td>适用于多种数据结构</td></tr><tr><td><strong>多模态适应性</strong></td><td>统一处理文本&#x2F;语音&#x2F;图像</td></tr></tbody></table><p><code>Self-attention</code> 通过模拟人脑的注意力机制，实现了从<strong>局部感知到全局理解的飞跃</strong>，成为现代深度学习架构的基石。</p>]]></content>
    
    
    <categories>
      
      <category>ML&amp;DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Self_Attention</tag>
      
      <tag>注意力</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅机器学习-CNN</title>
    <link href="/2025/08/19/CNN/"/>
    <url>/2025/08/19/CNN/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习李宏毅机器学习时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="1-什么是卷积神经网络-CNN-？"><a href="#1-什么是卷积神经网络-CNN-？" class="headerlink" title="1. 什么是卷积神经网络 (CNN)？"></a>1. 什么是卷积神经网络 (CNN)？</h1><p>卷积神经网络（Convolutional Neural Network, CNN）是一种专为处理图像数据而设计的网络架构 它在<strong>图像分类</strong>等领域表现出色，能够将输入的图像识别为不同的类别（例如，猫、狗、树等） </p><p>其基本工作流程是将一张图像作为输入，通过模型处理后，输出一个<strong>概率分布向量</strong>，表示该图像属于各个类别的可能性 。模型通过比较预测输出与真实标签的差异（例如使用<code>cross-entropy</code>交叉熵损失函数）来进行学习和优化 。</p><h1 id="2-CNN的核心思想"><a href="#2-CNN的核心思想" class="headerlink" title="2. CNN的核心思想"></a>2. CNN的核心思想</h1><p><strong>传统的全连接神经网络直接处理图像时，会产生巨大的参数量，导致计算效率低下且难以训练</strong> 。CNN通过三个关键观察简化了模型，使其更适用于图像处理：</p><ul><li><p><strong>观察1：局部模式的重要性</strong></p><ul><li>图像中的关键特征（如鸟的眼睛或喙）通常比整个图像小得多。因此，神经元无需观察整个图像，<strong>只需关注一个小的局部区域</strong>，这个区域被称为“感受野”（<code>Receptive Field</code>)。这大大减少了神经元之间的连接数量。</li></ul></li><li><p><strong>观察2：模式的重复出现</strong></p><ul><li>同一个特征（如鸟喙）可能出现在图像的不同位置 。CNN利用这一特性，让不同的神经元<strong>共享同一组权重参数</strong>来检测相同的模式，这一机制称为“参数共享”（<code>Parameter Sharing</code>）。</li></ul></li><li><p><strong>观察3：下采样的不变性</strong></p><ul><li>对图像进行<strong>下采样</strong>（即降低分辨率）通常不会改变图像中的物体类别。</li></ul></li></ul><h1 id="3-CNN的关键组成部分"><a href="#3-CNN的关键组成部分" class="headerlink" title="3. CNN的关键组成部分"></a>3. CNN的关键组成部分</h1><p>一个典型的CNN模型由以下几个主要部分组成：</p><h2 id="输入层-Input-Layer"><a href="#输入层-Input-Layer" class="headerlink" title="输入层 (Input Layer)"></a>输入层 (Input Layer)</h2><ul><li>输入的图像被表示为一个三维张量（tensor），其维度为<br>  <code>高度 x 宽度 x 通道数</code>。</li><li>对于彩色图像，通常有3个通道（红、绿、蓝）。对于黑白图像，则只有1个通道。</li><li>所有用于分类的输入图像尺寸必须相同。</li></ul><h2 id="卷积层-Convolutional-Layer"><a href="#卷积层-Convolutional-Layer" class="headerlink" title="卷积层 (Convolutional Layer)"></a>卷积层 (Convolutional Layer)</h2><p>卷积层是CNN的核心，它基于“感受野”和“参数共享”的思想构建。其工作方式有两种等效的理解：</p><ol><li><p><strong>神经元视角</strong>：</p><ul><li>每个神经元只连接到输入图像的一个局部区域（<code>Receptive Field</code>）。</li><li>位于不同<code>Receptive Field</code>但用于检测相同特征的神经元共享同一套权重和偏置（bias）参数。</li></ul></li><li><p>**滤波器视角 (Filter Version)**：</p><ul><li>卷积层包含一组<strong>滤波器</strong>（也称作“核”，kernel），每个滤波器负责检测<strong>一种特定的局部模式</strong>（如边缘、纹理等）。</li><li>每个滤波器在整个输入图像上滑动（卷积），计算每个位置的局部特征，并生成一个对应的<br>  <strong>特征图</strong>（Feature Map）。</li><li>卷积层通常包含多个滤波器（例如64个），从而生成一个包含多个通道（每个通道对应一个特征图）的新“图像” 。</li></ul></li></ol><ul><li><strong>关键参数</strong>：<ul><li><strong>核尺寸 (Kernel Size)</strong>: 滤波器的大小，例如 3x3。</li><li><strong>步幅 (Stride)</strong>: 滤波器每次滑动的距离，例如步幅为2意味着每次移动2个像素。</li><li><strong>填充 (Padding)</strong>: 在图像边界添加额外的像素，以保持输出尺寸或处理边界信息。</li></ul></li></ul><h2 id="池化层-Pooling-Layer"><a href="#池化层-Pooling-Layer" class="headerlink" title="池化层 (Pooling Layer)"></a>池化层 (Pooling Layer)</h2><ul><li><p>池化层基于“下采样”的思想，用于减小特征图的尺寸。</p></li><li><p><strong>最大池化 (Max Pooling)</strong> 是最常用的一种方式，它将特征图的一个区域（例如2x2）压缩成一个单一像素，该像素的值为该区域内的最大值。</p></li><li><p><strong>作用</strong>：</p><ul><li>减少计算量和参数数量。</li><li>增强模型的平移不变性，即物体在图像中的小幅移动不影响最终结果</li><li>如果算力充足，也不需要池化</li></ul></li></ul><h2 id="扁平化与全连接层-Flatten-Fully-Connected-Layers"><a href="#扁平化与全连接层-Flatten-Fully-Connected-Layers" class="headerlink" title="扁平化与全连接层 (Flatten &amp; Fully Connected Layers)"></a>扁平化与全连接层 (Flatten &amp; Fully Connected Layers)</h2><ul><li><p>在经过一系列卷积和池化层处理后，得到最终的<code>feature map</code>特征图 。</p></li><li><p>**扁平化 (Flatten)**：将多维的特征图转换成一个一维向量。</p></li><li><p>**全连接层 (Fully Connected Layers)**：将扁平化后的向量送入一个或多个标准的全连接神经网络层。</p></li><li><p><strong>输出层</strong>：最后通过一个输出层（通常使用<code>Softmax</code>激活函数）得到最终的分类概率。</p></li></ul><h1 id="4-CNN的典型架构"><a href="#4-CNN的典型架构" class="headerlink" title="4. CNN的典型架构"></a>4. CNN的典型架构</h1><p>一个完整的CNN通常由以下部分串联而成：</p><p><strong>输入图像 → [卷积层 → 池化层] (重复多次) → 扁平化 → 全连接层 → 输出 (分类结果)</strong> </p><h1 id="5-CNN的应用"><a href="#5-CNN的应用" class="headerlink" title="5. CNN的应用"></a>5. CNN的应用</h1><p>CNN不仅限于图像识别，其核心思想还可应用于其他领域：</p><ul><li><p><strong>围棋 (Go)</strong>: 棋盘可以被看作一个19x19的图像，CNN能够识别棋盘上的局部模式来决策下一步。值得注意的是，AlphaGo并未使用池化层。</p></li><li><p><strong>语音识别 (Speech)</strong>: 语音的频谱图可以作为图像输入，供CNN进行分析。</p></li><li><p><strong>自然语言处理 (NLP)</strong>: 句子可以被转换为词向量矩阵，CNN可以从中提取关键短语等模式。</p></li></ul><h1 id="6-局限性"><a href="#6-局限性" class="headerlink" title="6. 局限性"></a>6. 局限性</h1><ul><li><p>标准的CNN对图像的<br>  <strong>缩放和旋转</strong>并不具有不变性。为了解决这个问题，通常需要进行数据增强（Data Augmentation）。</p></li><li><p>更高级的技术如空间变换网络（Spatial Transformer Layer）也可以用来处理这类几何变换。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>ML&amp;DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅机器学习-HW1 Notebook</title>
    <link href="/2025/08/16/HW1%20Notebook/"/>
    <url>/2025/08/16/HW1%20Notebook/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我完成李宏毅机器学习2021作业1时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><p>作业1是**HW1-COVID-19 Cases Prediction（Regression)**，是一个回归任务,<a href="https://www.kaggle.com/competitions/ml2021spring-hw1/overview">kaggle链接</a><br>下面是 <code>strong baseline</code> 的要求：</p><h1 id="Strong-Baseline"><a href="#Strong-Baseline" class="headerlink" title="Strong Baseline"></a>Strong Baseline</h1><ul><li>Feature selection (what other features are useful?)</li><li>DNN architecture (layers? dimension? activation function?)</li><li>Training (mini-batch? optimizer? learning rate?)</li><li>L2 regularization</li><li>There are some mistakes in the sample code, can you find them?</li></ul><hr><h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>源数据有94列，肯定不能一股脑地全拿来训练，我们需要进行特征选择。我是用的比较传统的<strong>计算相关性</strong>的方法，然后排序，取前k个，大致代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Feature Selection</span><br><br>df = pd.read_csv(tr_path)<br>target_column = <span class="hljs-number">94</span><br>top_key  = <span class="hljs-number">14</span><br><br>y = df.iloc[:,target_column]<br>X = df.drop(columns=df.columns[target_column])<br><br><span class="hljs-built_in">print</span>(y)<br>correlations = X.corrwith(y).<span class="hljs-built_in">abs</span>().sort_values(ascending=<span class="hljs-literal">False</span>).head(top_key)<br><span class="hljs-built_in">print</span>(correlations)<br>selected_features = [df.columns.get_loc(i) - <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> correlations.index]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Selected features:\n&quot;</span>, selected_features)<br></code></pre></td></tr></table></figure><p>这样做有一个好处就是你能自己<strong>清楚地把握每一个特征的相关性</strong>，然后根据需要选择大于某一阈值的，而不仅仅是机械式的前多少多少个</p><hr><h1 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h1><p>当然，<code>sklearn</code>库早已封装好了相关函数，一起来学习下</p><p><code>Scikit-learn (sklearn)</code> 提供了多种特征选择（Feature Selection）方法，用于从原始数据中筛选出最有价值的特征，以提高模型性能、降低过拟合并减少计算开销。以下是 sklearn 中常用的特征选择方法分类及具体实现：</p><h2 id="1-移除低方差特征（VarianceThreshold）"><a href="#1-移除低方差特征（VarianceThreshold）" class="headerlink" title="1. 移除低方差特征（VarianceThreshold）"></a>1. 移除低方差特征（VarianceThreshold）</h2><ul><li><strong>原理</strong>：移除方差低于阈值的特征（方差接近零的特征通常对模型无贡献）。</li><li><strong>适用场景</strong>：<strong>适用于删除几乎恒定的特征</strong>（如所有样本取值相同）。</li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> VarianceThreshold<br>selector = VarianceThreshold(threshold=<span class="hljs-number">0.1</span>)  <span class="hljs-comment"># 移除方差 &lt;0.1 的特征</span><br>X_new = selector.fit_transform(X)<br></code></pre></td></tr></table></figure></li></ul><h2 id="2-单变量特征选择（Univariate-Selection）"><a href="#2-单变量特征选择（Univariate-Selection）" class="headerlink" title="2. 单变量特征选择（Univariate Selection）"></a>2. 单变量特征选择（Univariate Selection）</h2><p>基于统计检验或模型对每个特征单独评估，选择最佳特征。</p><h3 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h3><ul><li><strong>SelectKBest</strong>：选择得分最高的前 <code>k</code> 个特征。</li><li><strong>SelectPercentile</strong>：选择得分最高的前 <code>percentile%</code> 特征。</li><li><strong>常用统计检验</strong>：<ul><li>分类问题：<code>chi2</code>（卡方检验）、<code>f_classif</code>（ANOVA F值）、<code>mutual_info_classif</code>（互信息）。</li><li>回归问题：<code>f_regression</code>（F检验）、<code>mutual_info_regression</code>（互信息）。</li></ul></li></ul><h3 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest, chi2<br>selector = SelectKBest(chi2, k=<span class="hljs-number">10</span>)  <span class="hljs-comment"># 选择卡方检验得分最高的10个特征</span><br>X_new = selector.fit_transform(X, y)<br></code></pre></td></tr></table></figure><h2 id="3-递归特征消除（RFE-Recursive-Feature-Elimination）"><a href="#3-递归特征消除（RFE-Recursive-Feature-Elimination）" class="headerlink" title="3. 递归特征消除（RFE, Recursive Feature Elimination）"></a>3. 递归特征消除（RFE, Recursive Feature Elimination）</h2><ul><li><strong>原理</strong>：通过递归地训练模型（如线性回归、SVM）并移除权重最低的特征，逐步筛选特征。</li><li><strong>特点</strong>：需要指定最终特征数量或通过交叉验证自动选择（<code>RFECV</code>）。</li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br>model = LogisticRegression()<br>selector = RFE(model, n_features_to_select=<span class="hljs-number">5</span>)  <span class="hljs-comment"># 选择5个特征</span><br>X_new = selector.fit_transform(X, y)<br></code></pre></td></tr></table></figure></li></ul><h2 id="4-基于模型权重的特征选择"><a href="#4-基于模型权重的特征选择" class="headerlink" title="4. 基于模型权重的特征选择"></a>4. 基于模型权重的特征选择</h2><ul><li><strong>原理</strong>：使用具有特征重要性或系数权重的模型（如线性模型、树模型）筛选特征。</li><li><strong>方法</strong>：<ul><li><code>SelectFromModel</code>：根据模型权重或重要性阈值选择特征。</li><li>适用于：<code>Lasso</code>（回归）、<code>RandomForest</code>、<code>XGBoost</code> 等。</li></ul></li></ul><h3 id="示例：-1"><a href="#示例：-1" class="headerlink" title="示例："></a>示例：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br>model = RandomForestClassifier()<br>selector = SelectFromModel(model, threshold=<span class="hljs-string">&#x27;median&#x27;</span>)  <span class="hljs-comment"># 选择重要性高于中位数的特征</span><br>X_new = selector.fit_transform(X, y)<br></code></pre></td></tr></table></figure><h2 id="完整流程建议"><a href="#完整流程建议" class="headerlink" title="完整流程建议"></a>完整流程建议</h2><ol><li>预处理：先处理缺失值、标准化数据。</li><li>初步筛选：用 <code>VarianceThreshold</code> 或单变量方法减少特征数量。</li><li>精细筛选：结合模型（如 <code>SelectFromModel</code> 或 <code>RFE</code>）和交叉验证。</li><li>验证：比较筛选前后模型的性能（如 AUC、RMSE）。</li></ol><p>通过合理组合这些方法，可以高效地优化特征空间，提升模型效果。</p><hr><h1 id="L1-L2-正则化"><a href="#L1-L2-正则化" class="headerlink" title="L1&#x2F;L2 正则化"></a>L1&#x2F;L2 正则化</h1><p>L1&#x2F;L2 正则化是机器学习中用于<strong>防止模型过拟合</strong>的核心技术，通过<strong>向损失函数添加惩罚项</strong>来约束模型参数。以下是关键点总结：</p><h2 id="1-L1-正则化（Lasso）"><a href="#1-L1-正则化（Lasso）" class="headerlink" title="1. L1 正则化（Lasso）"></a>1. L1 正则化（Lasso）</h2><ul><li><strong>公式</strong>：损失函数 + $λ∑|wᵢ|$<br>λ控制惩罚强度，wᵢ为模型参数。</li><li><strong>特点</strong>：<ul><li><strong>稀疏性</strong>：<strong>倾向于将部分参数压缩为0，适用于特征选择</strong>。</li><li><strong>鲁棒性</strong>：对异常值不敏感。</li><li><strong>不可导性</strong>：需用次梯度优化（如坐标下降）。</li></ul></li><li><strong>场景</strong>：高维数据、特征冗余时（如基因数据、文本分类）。</li></ul><h2 id="2-L2-正则化（Ridge）"><a href="#2-L2-正则化（Ridge）" class="headerlink" title="2. L2 正则化（Ridge）"></a>2. L2 正则化（Ridge）</h2><ul><li><strong>公式</strong>：损失函数 + $λ∑wᵢ²$</li><li><strong>特点</strong>：<ul><li><strong>平滑收缩</strong>：均匀减小参数值，避免<strong>极端权重</strong>。</li><li><strong>可导性</strong>：可用梯度下降优化。</li><li><strong>抗共线性</strong>：缓解特征相关性带来的不稳定。</li></ul></li><li><strong>场景</strong>：参数间相关性高（如线性回归、神经网络）， <strong>约束参数值，避免模型对训练数据过度敏感，过度依赖某些特征</strong>。</li></ul><h2 id="3-对比"><a href="#3-对比" class="headerlink" title="3. 对比"></a>3. 对比</h2><table><thead><tr><th><strong>特性</strong></th><th><strong>L1</strong></th><th><strong>L2</strong></th></tr></thead><tbody><tr><td><strong>稀疏性</strong></td><td>是（自动特征选择）</td><td>否</td></tr><tr><td><strong>解的唯一性</strong></td><td>可能多解（非严格凸）</td><td>唯一解（严格凸）</td></tr><tr><td><strong>优化方法</strong></td><td>坐标下降、近端梯度</td><td>梯度下降、解析解</td></tr><tr><td><strong>抗噪声</strong></td><td>强</td><td>中等</td></tr></tbody></table><h2 id="4-结合使用（Elastic-Net）"><a href="#4-结合使用（Elastic-Net）" class="headerlink" title="4. 结合使用（Elastic Net）"></a>4. 结合使用（Elastic Net）</h2><ul><li><strong>公式</strong>：损失函数 + $λ₁∑|wᵢ| + λ₂∑wᵢ²$<br>平衡L1的稀疏性和L2的稳定性，适用于特征维度极高且相关性强的情况。</li></ul><h2 id="5-数学直观"><a href="#5-数学直观" class="headerlink" title="5.数学直观"></a>5.数学直观</h2><ul><li><strong>L1</strong>：菱形约束域，最优解常出现在顶点（某些参数为0）。</li><li><strong>L2</strong>：圆形约束域，最优解在边界某点（所有参数非零但小）。</li></ul><h2 id="6-代码"><a href="#6-代码" class="headerlink" title="6.代码"></a>6.代码</h2><ul><li>L2正则化可以<strong>在损失函数中手动添加</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>optimizer.zero_grad()<br>y_pred = model(X_tensor)<br>loss = nn.MSELoss()(y_pred, y_target)<br>l2_reg = torch.<span class="hljs-built_in">sum</span>(model.weight ** <span class="hljs-number">2</span>) <span class="hljs-comment"># L2惩罚项（不包含偏置）</span><br>total_loss = loss +  lambda_l2 * l2_reg <span class="hljs-comment"># 总损失</span><br>total_loss.backward()<br>optimizer.step()<br></code></pre></td></tr></table></figure></li><li>也可以直接在优化器中加 <code>weight_decay</code> 实现，比如：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="hljs-number">1e-3</span>, weight_decay=<span class="hljs-number">1e-4</span>)<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>ML&amp;DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
      <tag>Python</tag>
      
      <tag>ML</tag>
      
      <tag>feature_selection</tag>
      
      <tag>L2_regularization</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅机器学习-类神经网络训练</title>
    <link href="/2025/08/16/%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%B3%A8%E6%84%8F/"/>
    <url>/2025/08/16/%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%B3%A8%E6%84%8F/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习机器学习时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="Some-Terms"><a href="#Some-Terms" class="headerlink" title="Some Terms"></a>Some Terms</h1><h2 id="Critical-Point"><a href="#Critical-Point" class="headerlink" title="Critical Point"></a>Critical Point</h2><p>在深度学习中，“critical point”（关键点&#x2F;临界点）指的是损失函数曲面（loss landscape）上梯度为零的点。数学上，这意味着损失函数<code>L</code>（通常是关于模型参数<code>θ</code>的函数）的梯度（或一阶导数）<code>∇L(θ)</code>等于零向量：<strong>∇L(θ) &#x3D; 0</strong></p><p>然而，并非所有的关键点都是我们想要的全局最小值（Global Minimum）。关键点可以分为几类：</p><ol><li><p><strong>局部极小值点（Local Minima）：</strong></p><ul><li>在该点的某个<strong>局部邻域</strong>内，损失函数的值是所有点中最小的（但可能比全局最小值要大）。</li><li>这是优化过程中比较理想的点，虽然不是全局最优，但可能是足够好的解。</li><li><strong>特点：</strong> 在局部最低点，类似于山谷底部。</li></ul></li><li><p><strong>局部极大值点（Local Maxima）：</strong></p><ul><li>在该点的某个<strong>局部邻域</strong>内，损失函数的值是所有点中最大的。</li><li>在最小化损失函数的问题中（几乎所有深度学习任务都是），极大值点是我们想要<strong>避开</strong>的点。</li><li><strong>特点：</strong> 在局部最高点，类似于山顶（但损失高，性能差）。</li><li><strong>在深度学习中的现实性：</strong> 由于神经网络通常具有非常高的维度（参数众多），且损失函数通常无界向上，<strong>局部极大值点在深度学习的高维空间中极其罕见</strong>。更常见的是“反”方向的——鞍点。</li></ul></li><li><p><strong>鞍点（Saddle Points）：</strong></p><ul><li>这是深度学习中最普遍、也最具挑战性的关键点类型。</li><li>该点不是局部极小值点也不是局部极大值点。</li><li>在通过该点的<strong>某些方向</strong>上，损失函数<strong>上升</strong>；在<strong>另一些方向</strong>上，损失函数<strong>下降</strong>。</li><li><strong>特点：</strong> 形状类似于马鞍——在某些方向是“谷底”，在某些方向是“山顶”。</li><li><strong>在高维空间中的普遍性：</strong> 随着模型参数维度的增加（这正是深度神经网络的特点），损失函数曲面的鞍点数量会<strong>指数级地多于局部极小值点</strong>。研究表明，在深度学习中，<strong>阻碍训练收敛的主要障碍物更可能是鞍点而不是局部极小值</strong>。</li><li><strong>影响：</strong> 在鞍点处，梯度为零，标准的优化算法可能会停滞不前。如果下降方向“被困”在鞍点的平坦区域或上升方向，损失很长时间不会下降，学习变得非常缓慢，类似于“高原现象”。但在正确的方向上，损失是可以继续下降的。</li></ul></li></ol><p><strong>如何应对关键点（尤其是鞍点）？</strong></p><ul><li><strong>使用带动量（Momentum）的优化器：</strong> 如SGD with Momentum, Adam, RMSProp等。动量可以帮助参数更新“冲过”鞍点中平坦或轻微上升的区域，因为它积累了之前梯度的方向。</li><li><strong>自适应学习率：</strong> 像Adam这样的优化器会调整每个参数的学习率，有助于更快地逃离鞍点沿着下降方向前进。</li><li><strong>批归一化（Batch Normalization）等技巧：</strong> 通过平滑损失函数曲面，可能有助于减少鞍点带来的影响。</li><li><strong>合适的初始化：</strong> 好的参数初始化策略（如Xavier, He Init）可以让网络从更容易优化的位置开始。</li><li><strong>随机性：</strong> SGD本身固有的随机性（基于小批量样本计算梯度）有助于逃离某些鞍点。</li></ul><h2 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h2><ul><li>Smaller batch requires longer time for one epoch</li><li>Smaller batch size has better performance</li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250814214719943.png"><br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250814214645424.png"></p><h1 id="Better-Optimization"><a href="#Better-Optimization" class="headerlink" title="Better Optimization"></a>Better Optimization</h1><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>Momentum introduces a weighted sum of previous gradients to accelerate gradient descent. The movement is defined as the movement of the last step minus the gradient at the present step. The process is as follows:</p><p>$$<br>\begin{align}<br>m^0 &amp;&#x3D; 0 \<br>m^1 &amp;&#x3D; -\eta g^0 \<br>m^2 &amp;&#x3D; \lambda m^1 - \eta g^1 \<br>&amp;\vdots \<br>\theta^{t+1} &amp;&#x3D; \theta^t + m^t<br>\end{align}<br>$$</p><p>where:<br>    $m^t$ is the movement at step,<br>    $\lambda$ is the momentum coefficient (0 &lt; $\lambda$ &lt; 1),<br>    $\eta$ is the learning rate,<br>    $g^t$ is the gradient at step.</p><p>This method ensures the movement is not just based on the current gradient but also on the <strong>previous movement</strong>, improving convergence.</p><h1 id="Adaptive-Learning-rate"><a href="#Adaptive-Learning-rate" class="headerlink" title="Adaptive Learning rate"></a>Adaptive Learning rate</h1><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250814210238642.png"><br>所以我们需要修改Optimization的公式，由公式1替换为公式2，$σ_i^t$ 是一个与时间、特征都有关系的量。<br>$$<br>\begin{align}<br>θ_i^{t+1}←θ_i^t−ηg_i^t \<br>θ_i^{t+1}←θ_i^t−\frac η{σ_i^t}g_i^t \<br>\end{align}<br>$$<br>具体的更新过程可以有几种方式</p><h2 id="RMS"><a href="#RMS" class="headerlink" title="RMS"></a>RMS</h2><p>$$<br>\begin{align}<br>θ_i^1&amp;←θ_i^0−\frac η{σ_i^0}g_i^0 &amp; σ_i^0&amp;&#x3D;\sqrt{(g_i^0)^2} &#x3D; |g_i^0| \<br>θ_i^2&amp;←θ_i^1−\frac η{σ_i^1}g_i^1 &amp; σ_i^1&amp;&#x3D;\sqrt{\frac1 2(g_i^1)^2 + (g_i^0)^2} \<br>θ_i^3&amp;←θ_i^2−\frac η{σ_i^2}g_i^2 &amp; σ_i^2&amp;&#x3D;\sqrt{\frac1 3(g_i^2)^2 + (g_i^1)^2 + (g_i^0)^2} \<br>\vdots &amp;&amp;&amp; \<br>\end{align}<br>$$<br>但是RMS的问题是所有的梯度具有一样的权重，优化过程中，<strong>不同时间步的梯度可能差异极大</strong>（例如某些参数梯度大，某些梯度小），<strong>静态 RMS 无法自适应调整学习率</strong>，也就是无法敏锐捕捉当前梯度的变化趋势，可能造成震荡。因此诞生了RMS Prop</p><h2 id="RMS-Prop"><a href="#RMS-Prop" class="headerlink" title="RMS Prop"></a>RMS Prop</h2><p>自适应调整<br>$$<br>\begin{align}<br>\theta_i^1 &amp;\leftarrow \theta_i^0 - \frac{\eta}{\sigma_i^0} g_i^0 &amp; \sigma_i^0 &amp;&#x3D; \sqrt{(g_i^0)^2} \quad 0 &lt; \alpha &lt; 1 \<br>\theta_i^2 &amp;\leftarrow \theta_i^1 - \frac{\eta}{\sigma_i^1} g_i^1 &amp; \sigma_i^1 &amp;&#x3D; \sqrt{\alpha (\sigma_i^0)^2 + (1 - \alpha) (g_i^1)^2} \<br>\theta_i^3 &amp;\leftarrow \theta_i^2 - \frac{\eta}{\sigma_i^2} g_i^2 &amp; \sigma_i^2 &amp;&#x3D; \sqrt{\alpha (\sigma_i^1)^2 + (1 - \alpha) (g_i^2)^2} \<br>\vdots &amp;&amp;&amp; \<br>\theta_i^{t+1} &amp;\leftarrow \theta_i^t - \frac{\eta}{\sigma_i^t} g_i^t &amp; \sigma_i^t &amp;&#x3D; \sqrt{\alpha (\sigma_i^{t-1})^2 + (1 - \alpha) (g_i^t)^2}<br>\end{align}<br>$$</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><strong>Adam: RMSProp + Momentum</strong></p><p>值得一提的是，RMSProp 和 Momentum 都涉及到了历史梯度，是否重复呢？非也<br>因为RMS计算的是一个平凡根，是一个<code>scalar</code><br>而<strong>Momentum则是前一步移动的向量相加，不仅会有大小还会有方向上的影响</strong>。</p><h2 id="Learning-Rate-Scheduling"><a href="#Learning-Rate-Scheduling" class="headerlink" title="Learning Rate Scheduling"></a>Learning Rate Scheduling</h2><p>将参数$\eta$也随时间改变，变化的方式有两种<br>$$<br>\begin{align}<br>\eta&amp;\leftarrow\eta^t \ \theta_i^{t+1} &amp;\leftarrow \theta_i^t - \frac{\eta^t}{\sigma_i^t} g_i^t<br>\end{align}<br>$$</p><h3 id="Decay"><a href="#Decay" class="headerlink" title="Decay"></a>Decay</h3><h3 id="Warm-up"><a href="#Warm-up" class="headerlink" title="Warm up"></a>Warm up</h3><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250814214435975.png"></p><h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250822100430383.png"></p><ul><li>当不同feature的分布与值大小差别很大时，不同的weights 变化程度差异大，导致<code>error surface</code> 比较崎岖，难以训练、收敛，因此需要将数据归一化</li></ul>]]></content>
    
    
    <categories>
      
      <category>ML&amp;DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>ML</tag>
      
      <tag>Adam</tag>
      
      <tag>RMS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch Tutorial-Training Models</title>
    <link href="/2025/08/12/Training%20Models/"/>
    <url>/2025/08/12/Training%20Models/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习Pytorch时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>In past sections, we’ve discussed and demonstrated:</p><ul><li>Building models with the <code>neural network layers</code> and functions of the <code>torch.nn module</code></li><li>The mechanics of <code>automated gradient computation</code>, which is central to gradient-based model training</li><li>Using <code>TensorBoard</code> to visualize training progress and other activities</li></ul><p>In this section, we’ll be adding some new tools to your inventory:</p><ul><li>We’ll get familiar with the <code>dataset and dataloader</code> abstractions, and how they ease the process of feeding data to your model during a training loop</li><li>We’ll discuss specific <code>loss functions</code> and when to use them</li><li>We’ll look at <code>PyTorch optimizers</code>, which implement algorithms to adjust model weights based on the outcome of a loss function</li></ul><p>Finally, we’ll pull all of these together and see a full PyTorch training loop in action.</p><h1 id="Dataset-and-DataLoader"><a href="#Dataset-and-DataLoader" class="headerlink" title="Dataset and DataLoader"></a>Dataset and DataLoader<a href="http://localhost:8888/notebooks/Video+6+-+Model+Training+with+PyTorch.ipynb#Dataset-and-DataLoader"></a></h1><p>The <code>Dataset</code> and <code>DataLoader</code> classes encapsulate the process of pulling your data from storage and exposing it to your training loop in batches.</p><p>The <code>Dataset</code> is responsible for accessing and processing single instances of data.</p><p>The <code>DataLoader</code> pulls instances of data from the <code>Dataset</code> (either automatically or with a sampler that you define), collects them in batches, and returns them <strong>for consumption by your training</strong> loop. The <code>DataLoader</code> works with all kinds of datasets, regardless of the type of data they contain.</p><p>For this tutorial, we’ll be using the Fashion-MNIST dataset provided by TorchVision. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><br><span class="hljs-comment"># PyTorch TensorBoard support</span><br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime<br><br>transform = transforms.Compose(<br>    [transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.5</span>,), (<span class="hljs-number">0.5</span>,))])<br><br><span class="hljs-comment"># Create datasets for training &amp; validation, download if necessary</span><br>training_set = torchvision.datasets.FashionMNIST(<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">True</span>, transform=transform, download=<span class="hljs-literal">True</span>)<br>validation_set = torchvision.datasets.FashionMNIST(<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=transform, download=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># Create data loaders for our datasets; shuffle for training, not for validation</span><br>training_loader = torch.utils.data.DataLoader(training_set, batch_size=<span class="hljs-number">4</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=<span class="hljs-number">4</span>, shuffle=<span class="hljs-literal">False</span>, num_workers=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># Class labels</span><br>classes = (<span class="hljs-string">&#x27;T-shirt/top&#x27;</span>, <span class="hljs-string">&#x27;Trouser&#x27;</span>, <span class="hljs-string">&#x27;Pullover&#x27;</span>, <span class="hljs-string">&#x27;Dress&#x27;</span>, <span class="hljs-string">&#x27;Coat&#x27;</span>,<br>        <span class="hljs-string">&#x27;Sandal&#x27;</span>, <span class="hljs-string">&#x27;Shirt&#x27;</span>, <span class="hljs-string">&#x27;Sneaker&#x27;</span>, <span class="hljs-string">&#x27;Bag&#x27;</span>, <span class="hljs-string">&#x27;Ankle Boot&#x27;</span>)<br><br><span class="hljs-comment"># Report split sizes</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Training set has &#123;&#125; instances&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(training_set)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Validation set has &#123;&#125; instances&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(validation_set)))<br>Training <span class="hljs-built_in">set</span> has <span class="hljs-number">60000</span> instances<br>Validation <span class="hljs-built_in">set</span> has <span class="hljs-number">10000</span> instances<br></code></pre></td></tr></table></figure><p><strong>As always, let’s visualize the data as a sanity check:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Helper function for inline image display</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">matplotlib_imshow</span>(<span class="hljs-params">img, one_channel=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-keyword">if</span> one_channel:<br>        img = img.mean(dim=<span class="hljs-number">0</span>)<br>    img = img / <span class="hljs-number">2</span> + <span class="hljs-number">0.5</span>     <span class="hljs-comment"># unnormalize</span><br>    npimg = img.numpy()<br>    <span class="hljs-keyword">if</span> one_channel:<br>        plt.imshow(npimg, cmap=<span class="hljs-string">&quot;Greys&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        plt.imshow(np.transpose(npimg, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<br><br>dataiter = <span class="hljs-built_in">iter</span>(training_loader)<br>images, labels = dataiter.<span class="hljs-built_in">next</span>()<br><br><span class="hljs-comment"># Create a grid from the images and show them</span><br>img_grid = torchvision.utils.make_grid(images)<br>matplotlib_imshow(img_grid, one_channel=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;  &#x27;</span>.join(classes[labels[j]] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)))<br></code></pre></td></tr></table></figure><ul><li><code>make_grid</code> 是 PyTorch 中用于将多张图像拼接成网格的工具，常用于可视化批次数据或特征图</li></ul><h1 id="The-Model"><a href="#The-Model" class="headerlink" title="The Model"></a>The Model</h1><p>The model we’ll use in this example is a <code>variant of LeNet-5</code> - it should be familiar if you’ve watched the previous videos in this series.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-comment"># PyTorch models inherit from torch.nn.Module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GarmentClassifier</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(GarmentClassifier, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.pool = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">120</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv1(x)))<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv2(x)))<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">16</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>)<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br>    <br><br>model = GarmentClassifier()<br></code></pre></td></tr></table></figure><h1 id="Loss-Funtion"><a href="#Loss-Funtion" class="headerlink" title="Loss Funtion"></a>Loss Funtion</h1><p>For this example, we’ll be using a <code>cross-entropy loss</code>. For demonstration purposes, we’ll create batches of <strong>dummy output and label values</strong>, run them through the loss function, and examine the result.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_fn = torch.nn.CrossEntropyLoss()<br><br><span class="hljs-comment"># NB: Loss functions expect data in batches, so we&#x27;re creating batches of 4</span><br><span class="hljs-comment"># Represents the model&#x27;s confidence in each of the 10 classes for a given input</span><br>dummy_outputs = torch.rand(<span class="hljs-number">4</span>, <span class="hljs-number">10</span>)<br><span class="hljs-comment"># Represents the correct class among the 10 being tested</span><br>dummy_labels = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>])<br>    <br><span class="hljs-built_in">print</span>(dummy_outputs)<br><span class="hljs-built_in">print</span>(dummy_labels)<br><br>loss = loss_fn(dummy_outputs, dummy_labels)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Total loss for this batch: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(loss.item()))<br><br>tensor([[<span class="hljs-number">0.7915</span>, <span class="hljs-number">0.4766</span>, <span class="hljs-number">0.3735</span>, <span class="hljs-number">0.5340</span>, <span class="hljs-number">0.0799</span>, <span class="hljs-number">0.9948</span>, <span class="hljs-number">0.1870</span>, <span class="hljs-number">0.0507</span>, <span class="hljs-number">0.1183</span>,<br>         <span class="hljs-number">0.9106</span>],<br>        [<span class="hljs-number">0.9666</span>, <span class="hljs-number">0.3765</span>, <span class="hljs-number">0.4324</span>, <span class="hljs-number">0.7354</span>, <span class="hljs-number">0.1953</span>, <span class="hljs-number">0.8906</span>, <span class="hljs-number">0.6882</span>, <span class="hljs-number">0.1925</span>, <span class="hljs-number">0.7076</span>,<br>         <span class="hljs-number">0.8777</span>],<br>        [<span class="hljs-number">0.4412</span>, <span class="hljs-number">0.0325</span>, <span class="hljs-number">0.4886</span>, <span class="hljs-number">0.9350</span>, <span class="hljs-number">0.9792</span>, <span class="hljs-number">0.5580</span>, <span class="hljs-number">0.6199</span>, <span class="hljs-number">0.2478</span>, <span class="hljs-number">0.3619</span>,<br>         <span class="hljs-number">0.8307</span>],<br>        [<span class="hljs-number">0.3287</span>, <span class="hljs-number">0.8571</span>, <span class="hljs-number">0.6046</span>, <span class="hljs-number">0.6719</span>, <span class="hljs-number">0.5982</span>, <span class="hljs-number">0.0540</span>, <span class="hljs-number">0.7193</span>, <span class="hljs-number">0.4764</span>, <span class="hljs-number">0.7451</span>,<br>         <span class="hljs-number">0.8345</span>]])<br>tensor([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>])<br>Total loss <span class="hljs-keyword">for</span> this batch: <span class="hljs-number">2.196722984313965</span><br></code></pre></td></tr></table></figure><h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><p>For this example, we’ll be using simple <a href="https://pytorch.org/docs/stable/optim.html">stochastic gradient descent</a> with momentum.</p><p>It can be instructive to try some variations on this optimization scheme:</p><ul><li><code>Learning rate</code> determines the size of the steps the optimizer takes. What does a different learning rate do to the your training results, in terms of <strong>accuracy and convergence time</strong>?</li><li><code>Momentum</code> nudges the optimizer in the direction of strongest gradient over multiple steps. What does changing this value do to your results?</li><li>Try some different optimization algorithms, such as <code>averaged SGD</code>, <code>Adagrad</code>, or <code>Adam</code>. How do your results differ?</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Optimizers specified in the torch.optim package</span><br>optimizer = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><p><code>torch.optim.SGD</code> 是 PyTorch 中实现 <strong>随机梯度下降（Stochastic Gradient Descent）</strong> 的优化器，支持基础的 SGD 和带动量的 SGD（Momentum SGD）。以下是其核心参数、数学原理和使用方法。</p><hr><h2 id="1-核心参数"><a href="#1-核心参数" class="headerlink" title="1. 核心参数"></a>1. 核心参数</h2><table><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>params</code></td><td>iterable</td><td>-</td><td>待优化的模型参数（通常为 <code>model.parameters()</code>）。</td></tr><tr><td><code>lr</code></td><td>float</td><td>-</td><td><strong>学习率</strong>（必须指定），控制参数更新步长。</td></tr><tr><td><code>momentum</code></td><td>float</td><td>0</td><td>动量因子（0 表示普通 SGD），加速收敛并减少震荡。</td></tr><tr><td><code>dampening</code></td><td>float</td><td>0</td><td>动量阻尼（通常与 <code>momentum</code> 配合使用）。</td></tr><tr><td><code>weight_decay</code></td><td>float</td><td>0</td><td>L2 正则化系数（防止过拟合）。</td></tr><tr><td><code>nesterov</code></td><td>bool</td><td>False</td><td>是否启用 <strong>Nesterov 动量</strong>（需 <code>momentum &gt; 0</code>）。</td></tr></tbody></table><h2 id="2-数学原理"><a href="#2-数学原理" class="headerlink" title="2. 数学原理"></a>2. 数学原理</h2><h3 id="1-普通-SGD（无动量）"><a href="#1-普通-SGD（无动量）" class="headerlink" title="(1) 普通 SGD（无动量）"></a>(1) 普通 SGD（无动量）</h3><p>参数更新公式：<br>$$<br>\theta_{t+1} &#x3D; \theta_t - \eta \cdot \nabla_\theta J(\theta_t)<br>$$</p><ul><li>$\theta_t$：第 $t$ 步的参数。</li><li>$\eta$：学习率（<code>lr</code>）。</li><li>$\nabla_\theta J(\theta_t)$：损失函数对参数的梯度。</li></ul><h3 id="2-带动量的-SGD"><a href="#2-带动量的-SGD" class="headerlink" title="(2) 带动量的 SGD"></a>(2) 带动量的 SGD</h3><p>引入动量项 $v_t$：<br>$$<br>\begin{align}v_{t+1} &amp;&#x3D; \mu \cdot v_t + \nabla_\theta J(\theta_t)   \<br>\theta_{t+1} &amp;&#x3D; \theta_t - \eta \cdot v_{t+1}\end{align}<br>$$</p><ul><li>$\mu$：动量系数（<code>momentum</code>，通常设为 0.9）。</li><li><strong>动量通过累积历史梯度方向，加速收敛并抑制震荡</strong>。</li></ul><h3 id="3-Nesterov-动量"><a href="#3-Nesterov-动量" class="headerlink" title="(3) Nesterov 动量"></a>(3) Nesterov 动量</h3><p>在计算梯度时先进行“试探性”更新：<br>$$<br>v_{t+1} &#x3D; \mu \cdot v_t + \nabla_\theta J(\theta_t - \eta \mu v_t) \<br>\theta_{t+1} &#x3D; \theta_t - \eta \cdot v_{t+1}<br>$$</p><ul><li>相比普通动量，Nesterov 动量对梯度方向更敏感，收敛更快。</li></ul><hr><h2 id="3-适用场景"><a href="#3-适用场景" class="headerlink" title="3. 适用场景"></a>3. 适用场景</h2><table><thead><tr><th>场景</th><th>推荐配置</th><th>说明</th></tr></thead><tbody><tr><td><strong>简单任务</strong></td><td><code>lr=0.01</code>, <code>momentum=0</code></td><td>数据量小、模型简单时，普通 SGD 足够。</td></tr><tr><td><strong>深层网络训练</strong></td><td><code>lr=0.1</code>, <code>momentum=0.9</code></td><td>动量帮助加速收敛，避免陷入局部最优。</td></tr><tr><td><strong>对抗训练</strong></td><td><code>lr=0.01</code>, <code>momentum=0.9</code>, <code>nesterov=True</code></td><td>Nesterov 动量提升对抗样本生成效果。</td></tr><tr><td><strong>稀疏数据</strong></td><td><code>lr=0.001</code>, <code>weight_decay=1e-4</code></td><td>L2 正则化防止过拟合。</td></tr></tbody></table><hr><h2 id="6-与其他优化器对比"><a href="#6-与其他优化器对比" class="headerlink" title="6. 与其他优化器对比"></a>6. 与其他优化器对比</h2><table><thead><tr><th>优化器</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>SGD</strong></td><td>理论收敛性好，调参简单。</td><td>需手动调学习率，可能收敛慢。</td></tr><tr><td><strong>Adam</strong></td><td>自适应学习率，适合大多数任务。</td><td>可能在某些任务上泛化性差。</td></tr><tr><td><strong>RMSprop</strong></td><td>适合非平稳目标（如 RNN）。</td><td>对超参数敏感。</td></tr></tbody></table><hr><p>通过合理配置 <code>torch.optim.SGD</code>，你可以在<strong>训练速度和模型性能之间取得平衡</strong>。对于复杂任务，建议尝试 <code>Adam</code> 或 <code>SGD + 动量</code> 并对比效果。</p><h1 id="The-Training-Loop"><a href="#The-Training-Loop" class="headerlink" title="The Training Loop"></a>The Training Loop</h1><p>Below, we have a function that performs one training epoch. It enumerates data from the DataLoader, and on each pass of the loop does the following:</p><ul><li>Gets a batch of training data from the DataLoader</li><li>Zeros the optimizer’s gradients</li><li>Performs an inference - that is, gets predictions from the model for an input batch</li><li>Calculates the loss for that set of predictions vs. the labels on the dataset</li><li>Calculates the backward gradients over the learning weights</li><li>Tells the optimizer to perform one learning step - that is, adjust the model’s learning weights based on the observed gradients for this batch, according to the optimization algorithm we chose</li><li>It reports on the loss for every 1000 batches.</li><li>Finally, it reports the average per-batch loss for the last 1000 batches, for comparison with a validation run</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_one_epoch</span>(<span class="hljs-params">epoch_index, tb_writer</span>):<br>    running_loss = <span class="hljs-number">0.</span><br>    last_loss = <span class="hljs-number">0.</span><br>    <br>    <span class="hljs-comment"># Here, we use enumerate(training_loader) instead of</span><br>    <span class="hljs-comment"># iter(training_loader) so that we can track the batch</span><br>    <span class="hljs-comment"># index and do some intra-epoch reporting</span><br>    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(training_loader):<br>        <span class="hljs-comment"># Every data instance is an input + label pair</span><br>        inputs, labels = data<br>        <br>        <span class="hljs-comment"># Zero your gradients for every batch!</span><br>        optimizer.zero_grad()<br>        <br>        <span class="hljs-comment"># Make predictions for this batch</span><br>        outputs = model(inputs)<br>        <br>        <span class="hljs-comment"># Compute the loss and its gradients</span><br>        loss = loss_fn(outputs, labels)<br>        loss.backward()<br>        <br>        <span class="hljs-comment"># Adjust learning weights</span><br>        optimizer.step()<br>        <br>        <span class="hljs-comment"># Gather data and report</span><br>        running_loss += loss.item()<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">1000</span> == <span class="hljs-number">999</span>:<br>            last_loss = running_loss / <span class="hljs-number">1000</span> <span class="hljs-comment"># loss per batch</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;  batch &#123;&#125; loss: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i + <span class="hljs-number">1</span>, last_loss))<br>            tb_x = epoch_index * <span class="hljs-built_in">len</span>(training_loader) + i + <span class="hljs-number">1</span><br>            tb_writer.add_scalar(<span class="hljs-string">&#x27;Loss/train&#x27;</span>, last_loss, tb_x)<br>            running_loss = <span class="hljs-number">0.</span><br>            <br>    <span class="hljs-keyword">return</span> last_loss<br></code></pre></td></tr></table></figure><h2 id="Per-Epoch-Activity"><a href="#Per-Epoch-Activity" class="headerlink" title="Per-Epoch Activity"></a>Per-Epoch Activity</h2><p>There are a couple of things we’ll want to do once per epoch:</p><ul><li><strong>Perform validation</strong> by checking our relative loss on a set of data that was not used for training, and report this</li><li><strong>Save a copy of the model</strong></li></ul><p>Here, we’ll do our reporting in TensorBoard. This will require going to the command line to start TensorBoard, and opening it in another browser tab.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Initializing in a separate cell so we can easily add more epochs to the same run</span><br>timestamp = datetime.now().strftime(<span class="hljs-string">&#x27;%Y%m%d_%H%M%S&#x27;</span>)<br>writer = SummaryWriter(<span class="hljs-string">&#x27;runs/fashion_trainer_&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(timestamp))<br>epoch_number = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python">EPOCHS = <span class="hljs-number">5</span><br><br>best_vloss = <span class="hljs-number">1_000_000.</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;EPOCH &#123;&#125;:&#x27;</span>.<span class="hljs-built_in">format</span>(epoch_number + <span class="hljs-number">1</span>))<br>    <br>    <span class="hljs-comment"># Make sure gradient tracking is on, and do a pass over the data</span><br>    model.train(<span class="hljs-literal">True</span>)<br>    avg_loss = train_one_epoch(epoch_number, writer)<br>    <br>    <span class="hljs-comment"># We don&#x27;t need gradients on to do reporting</span><br>    model.train(<span class="hljs-literal">False</span>)<br>    <br>    running_vloss = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> i, vdata <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(validation_loader):<br>        vinputs, vlabels = vdata<br>        voutputs = model(vinputs)<br>        vloss = loss_fn(voutputs, vlabels)<br>        running_vloss += vloss<br>    <br>    avg_vloss = running_vloss / (i + <span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;LOSS train &#123;&#125; valid &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(avg_loss, avg_vloss))<br>    <br>    <span class="hljs-comment"># Log the running loss averaged per batch</span><br>    <span class="hljs-comment"># for both training and validation</span><br>    writer.add_scalars(<span class="hljs-string">&#x27;Training vs. Validation Loss&#x27;</span>,<br>                    &#123; <span class="hljs-string">&#x27;Training&#x27;</span> : avg_loss, <span class="hljs-string">&#x27;Validation&#x27;</span> : avg_vloss &#125;,<br>                    epoch_number + <span class="hljs-number">1</span>)<br>    writer.flush()<br>    <br>    <span class="hljs-comment"># Track best performance, and save the model&#x27;s state</span><br>    <span class="hljs-keyword">if</span> avg_vloss &lt; best_vloss:<br>        best_vloss = avg_vloss<br>        model_path = <span class="hljs-string">&#x27;model_&#123;&#125;_&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(timestamp, epoch_number)<br>        torch.save(model.state_dict(), model_path)<br>    <br>    epoch_number += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>To load a saved version of the model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">saved_model = GarmentClassifier()<br>saved_model.load_state_dict(torch.load(PATH))<br></code></pre></td></tr></table></figure><p>Once you’ve loaded the model, it’s ready for whatever you need it for - more training, inference, or analysis.</p><p>Note that if your model has constructor parameters that affect model structure, you’ll need to provide them and configure the model identically to the state in which it was saved.</p><h1 id="Other-Resources"><a href="#Other-Resources" class="headerlink" title="Other Resources"></a>Other Resources</h1><ul><li>Docs on the <a href="https://pytorch.org/docs/stable/data.html">data utilities</a>, including Dataset and DataLoader, at pytorch.org</li><li>A <a href="https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-pinning">note on the use of pinned memory</a> for GPU training</li><li>Documentation on the datasets available in <a href="https://pytorch.org/docs/stable/torchvision/datasets.html">TorchVision</a>, <a href="https://pytorch.org/text/datasets.html">TorchText</a>, and <a href="https://pytorch.org/audio/datasets.html">TorchAudio</a></li><li>Documentation on the <a href="https://pytorch.org/docs/stable/nn.html#loss-functions">loss functions</a> available in PyTorch</li><li>Documentation on the <a href="https://pytorch.org/docs/stable/optim.html">torch.optim package</a>, which includes optimizers and related tools, such as learning rate scheduling</li><li>A detailed <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">tutorial on saving and loading models</a></li><li>The <a href="https://pytorch.org/tutorials/">Tutorials section of pytorch.org</a> contains tutorials on a broad variety of training tasks, including classification in different domains, generative adversarial networks, reinforcement learning, and more</li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
      <tag>Python</tag>
      
      <tag>ML</tag>
      
      <tag>SGD</tag>
      
      <tag>Optimizer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch Tutorial-TensorBoard</title>
    <link href="/2025/08/12/TensorBoard/"/>
    <url>/2025/08/12/TensorBoard/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习Pytorch时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><p>TensorBoard 是 <strong>TensorFlow</strong> 提供的可视化工具，用于帮助开发者 <strong>理解、调试和优化</strong> 深度学习模型的训练过程。它最初是为 TensorFlow 设计的，但也可以通过适配器（如 <code>PyTorch</code> 的 <code>TensorBoardX</code> 或官方 <code>torch.utils.tensorboard</code>）支持 PyTorch</p><p><code>SummaryWriter</code> 是 PyTorch 中 <strong>TensorBoard</strong> 的日志记录工具，用于将训练过程中的数据（如标量、图像、模型结构等）保存到指定目录，方便通过 TensorBoard 可视化分析，比如<code>writer = SummaryWriter(&#39;runs/fashion_mnist_experiment_1&#39;)</code>。本节涉及到的<code>method()</code>:</p><table><thead><tr><th>方法</th><th>常用参数</th><th>主要用途</th><th>适用场景示例</th></tr></thead><tbody><tr><td><strong><code>add_image()</code></strong></td><td><code>tag</code>（标签名）, <code>img_tensor</code>（图像张量）</td><td>记录单张图像或图像网格（如输入数据、特征图、GAN生成结果）</td><td>- 可视化训练样本<br>- 显示卷积层的特征图</td></tr><tr><td><strong><code>add_scalars()</code></strong></td><td><code>main_tag</code>（主标签）, <code>tag_scalar_dict</code>（子标签-值字典）, <code>global_step</code></td><td>同时记录多个标量（如训练&#x2F;验证损失、准确率）</td><td>- 监控训练和验证损失曲线<br>- 对比不同超参数的效果</td></tr><tr><td><strong><code>add_graph()</code></strong></td><td><code>model</code>（模型）, <code>input_to_model</code>（示例输入）</td><td>可视化模型的计算图（包括网络结构和数据流）</td><td>- 调试模型结构<br>- 检查输入&#x2F;输出维度是否匹配</td></tr><tr><td><strong><code>add_embedding()</code></strong></td><td><code>mat</code>（特征矩阵）, <code>metadata</code>（标签列表）, <code>label_img</code>（对应图像）</td><td>将高维数据（如词向量、隐空间）降维可视化（PCA&#x2F;t-SNE）</td><td>- 词嵌入可视化（Word2Vec）<br>- 自编码器隐空间分析</td></tr></tbody></table><hr><p>In this notebook, we’ll be training a variant of <code>LeNet-5</code> against the <code>Fashion-MNIST</code> dataset. <code>Fashion-MNIST</code> is a set of image tiles depicting various garments, with ten class labels indicating the type of garment depicted.</p><h1 id="Showing-Images"><a href="#Showing-Images" class="headerlink" title="Showing Images"></a>Showing Images</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Gather datasets and prepare them for consumption</span><br>transform = transforms.Compose(<br>    [transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.5</span>,), (<span class="hljs-number">0.5</span>,))])<br><br><span class="hljs-comment"># Store separate training and validations splits in ./data</span><br>training_set = torchvision.datasets.FashionMNIST(<span class="hljs-string">&#x27;./data&#x27;</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    transform=transform)<br>validation_set = torchvision.datasets.FashionMNIST(<span class="hljs-string">&#x27;./data&#x27;</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    transform=transform)<br><br>training_loader = torch.utils.data.DataLoader(training_set,<br>                                              batch_size=<span class="hljs-number">4</span>,<br>                                              shuffle=<span class="hljs-literal">True</span>,<br>                                              num_workers=<span class="hljs-number">2</span>)<br><br><br>validation_loader = torch.utils.data.DataLoader(validation_set,<br>                                                batch_size=<span class="hljs-number">4</span>,<br>                                                shuffle=<span class="hljs-literal">False</span>,<br>                                                num_workers=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># Class labels</span><br>classes = (<span class="hljs-string">&#x27;T-shirt/top&#x27;</span>, <span class="hljs-string">&#x27;Trouser&#x27;</span>, <span class="hljs-string">&#x27;Pullover&#x27;</span>, <span class="hljs-string">&#x27;Dress&#x27;</span>, <span class="hljs-string">&#x27;Coat&#x27;</span>,<br>        <span class="hljs-string">&#x27;Sandal&#x27;</span>, <span class="hljs-string">&#x27;Shirt&#x27;</span>, <span class="hljs-string">&#x27;Sneaker&#x27;</span>, <span class="hljs-string">&#x27;Bag&#x27;</span>, <span class="hljs-string">&#x27;Ankle Boot&#x27;</span>)<br><br><span class="hljs-comment"># Helper function for inline image display</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">matplotlib_imshow</span>(<span class="hljs-params">img, one_channel=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-keyword">if</span> one_channel:<br>        img = img.mean(dim=<span class="hljs-number">0</span>)<br>    img = img / <span class="hljs-number">2</span> + <span class="hljs-number">0.5</span>     <span class="hljs-comment"># unnormalize</span><br>    npimg = img.numpy()<br>    <span class="hljs-keyword">if</span> one_channel:<br>        plt.imshow(npimg, cmap=<span class="hljs-string">&quot;Greys&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        plt.imshow(np.transpose(npimg, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<br>        <br><span class="hljs-comment"># Extract a batch of 4 images</span><br>dataiter = <span class="hljs-built_in">iter</span>(training_loader)<br>images, labels = <span class="hljs-built_in">next</span>(dataiter)<br><br><span class="hljs-comment"># Create a grid from the images and show them</span><br>img_grid = torchvision.utils.make_grid(images)<br>matplotlib_imshow(img_grid, one_channel=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># Default log_dir argument is &quot;runs&quot; - but it&#x27;s good to be specific</span><br><span class="hljs-comment"># torch.utils.tensorboard.SummaryWriter is imported above</span><br>writer = SummaryWriter(<span class="hljs-string">&#x27;runs/fashion_mnist_experiment_1&#x27;</span>)<br><br><span class="hljs-comment"># Write image data to TensorBoard log dir</span><br>writer.add_image(<span class="hljs-string">&#x27;Four Fashion-MNIST Images&#x27;</span>, img_grid)<br>writer.flush()<br><br><span class="hljs-comment"># To view, start TensorBoard on the command line with:</span><br><span class="hljs-comment">#   tensorboard --logdir=runs</span><br><span class="hljs-comment"># ...and open a browser tab to http://localhost:6006/</span><br></code></pre></td></tr></table></figure><ul><li><p><strong>start TensorBoard at the command line and open it in a new browser tab (usually at localhost:6006), you should see the image grid under the IMAGES tab.</strong></p></li><li><p><code>transforms.Compose</code>定义了一个 <strong>PyTorch 数据预处理流水线</strong>，用于将输入数据（如图像）转换为张量并进行标准化处理</p></li></ul><table><thead><tr><th align="center">步骤</th><th align="center">数据范围</th><th align="center">数据形状</th></tr></thead><tbody><tr><td align="center">原始图像（PIL）</td><td align="center">[0, 255]</td><td align="center">H×W×C (28×28×1)</td></tr><tr><td align="center"><code>ToTensor</code> 后</td><td align="center">[0.0, 1.0]</td><td align="center">C×H×W (1×28×28)</td></tr><tr><td align="center"><code>Normalize</code> 后</td><td align="center">[-1.0, 1.0]</td><td align="center">C×H×W (1×28×28)</td></tr></tbody></table><h1 id="Graphing-Scalars-to-Visualize-Training"><a href="#Graphing-Scalars-to-Visualize-Training" class="headerlink" title="Graphing Scalars to Visualize Training"></a>Graphing Scalars to Visualize Training</h1><p>TensorBoard is useful for <strong>tracking the progress and efficacy of your training.</strong> Below, we’ll run a training loop, track some metrics, and save the data for TensorBoard’s consumption.</p><p>Let’s define a model to <strong>categorize</strong> our image tiles, and an <strong>optimizer</strong> and <strong>loss function</strong> for training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Net, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.pool = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">120</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv1(x)))<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv2(x)))<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">16</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>)<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br>    <br><br>net = Net()<br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><ul><li>Now let’s train a single epoch, and evaluate the training vs. validation set losses every 1000 batches:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(validation_loader))<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):  <span class="hljs-comment"># loop over the dataset multiple times</span><br>    running_loss = <span class="hljs-number">0.0</span><br><br>    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(training_loader, <span class="hljs-number">0</span>):<br>        <span class="hljs-comment"># basic training loop</span><br>        inputs, labels = data<br>        optimizer.zero_grad()<br>        outputs = net(inputs)<br>        loss = criterion(outputs, labels)<br>        loss.backward()<br>        optimizer.step()<br><br>        running_loss += loss.item()<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">1000</span> == <span class="hljs-number">999</span>:    <span class="hljs-comment"># Every 1000 mini-batches...</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Batch &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i + <span class="hljs-number">1</span>))<br>            <span class="hljs-comment"># Check against the validation set</span><br>            running_vloss = <span class="hljs-number">0.0</span><br>            <br>            net.train(<span class="hljs-literal">False</span>) <span class="hljs-comment"># Don&#x27;t need to track gradents for validation</span><br>            <span class="hljs-keyword">for</span> j, vdata <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(validation_loader, <span class="hljs-number">0</span>):<br>                vinputs, vlabels = vdata<br>                voutputs = net(vinputs)<br>                vloss = criterion(voutputs, vlabels)<br>                running_vloss += vloss.item()<br>            net.train(<span class="hljs-literal">True</span>) <span class="hljs-comment"># Turn gradients back on for training</span><br>            <br>            avg_loss = running_loss / <span class="hljs-number">1000</span><br>            avg_vloss = running_vloss / <span class="hljs-built_in">len</span>(validation_loader)<br>            <br>            <span class="hljs-comment"># Log the running loss averaged per batch</span><br>            writer.add_scalars(<span class="hljs-string">&#x27;Training vs. Validation Loss&#x27;</span>,<br>                            &#123; <span class="hljs-string">&#x27;Training&#x27;</span> : avg_loss, <span class="hljs-string">&#x27;Validation&#x27;</span> : avg_vloss &#125;,<br>                            epoch * <span class="hljs-built_in">len</span>(training_loader) + i)<br><br>            running_loss = <span class="hljs-number">0.0</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished Training&#x27;</span>)<br><br>writer.flush()<br><br><span class="hljs-number">2500</span><br>Batch <span class="hljs-number">1000</span><br>Batch <span class="hljs-number">2000</span><br>……<br>Batch <span class="hljs-number">14000</span><br>Batch <span class="hljs-number">15000</span><br>Finished Training<br></code></pre></td></tr></table></figure></li><li><strong>Switch to your open TensorBoard and have a look at the SCALARS tab.</strong><br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250810182045760.svg"></li></ul><h1 id="Visualizing-Your-Model"><a href="#Visualizing-Your-Model" class="headerlink" title="Visualizing Your Model"></a>Visualizing Your Model</h1><p>TensorBoard can also be used to examine the data flow within your model. To do this, call the <code>add_graph()</code> method with a model and sample input.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Again, grab a single mini-batch of images</span><br>dataiter = <span class="hljs-built_in">iter</span>(training_loader)<br>images, labels = <span class="hljs-built_in">next</span>(dataiter)<br><br><span class="hljs-comment"># add_graph() will trace the sample input through your model,</span><br><span class="hljs-comment"># and render it as a graph.</span><br>writer.add_graph(net, images)<br>writer.flush()<br></code></pre></td></tr></table></figure><p><strong>add_graph() will trace the sample input through your model</strong><br>When you switch over to TensorBoard, you should see a GRAPHS tab. Double-click the “NET” node to see the <strong>layers and data flow</strong> within your model.<br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250810181729149.png"></p><h1 id="Visualizing-Your-Dataset"><a href="#Visualizing-Your-Dataset" class="headerlink" title="Visualizing Your Dataset"></a>Visualizing Your Dataset</h1><p>The 28-by-28 image tiles we’re using can be modeled as 784-dimensional vectors (28 * 28 &#x3D; 784). It can be instructive to <strong>project this to a lower-dimensional representation</strong>. The <code>add_embedding()</code> method will project a set of data onto the <strong>three dimensions with highest variance</strong>, and display them as an <strong>interactive 3D chart</strong>. The <code>add_embedding()</code> method does this automatically by projecting to the three dimensions with highest variance.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Select a random subset of data and corresponding labels</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">select_n_random</span>(<span class="hljs-params">data, labels, n=<span class="hljs-number">100</span></span>):<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(data) == <span class="hljs-built_in">len</span>(labels)<br><br>    perm = torch.randperm(<span class="hljs-built_in">len</span>(data))<br>    <span class="hljs-keyword">return</span> data[perm][:n], labels[perm][:n]<br><br><span class="hljs-comment"># Extract a random subset of data</span><br>images, labels = select_n_random(training_set.data, training_set.targets)<br><br><span class="hljs-comment"># get the class labels for each image</span><br>class_labels = [classes[label] <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]<br><br><span class="hljs-comment"># log embeddings</span><br>features = images.view(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>)<br>writer.add_embedding(features,<br>                    metadata=class_labels,<br>                    label_img=images.unsqueeze(<span class="hljs-number">1</span>))<br>writer.flush()<br>writer.close()<br></code></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250811102153098.png"></p><h1 id="Other-Resources"><a href="#Other-Resources" class="headerlink" title="Other Resources"></a>Other Resources</h1><p>For more information, have a look at:</p><ul><li><a href="https://pytorch.org/docs/stable/tensorboard.html?highlight=summarywriter">PyTorch documentation on <code>torch.utils.tensorboard.SummaryWriter</code></a> on <a href="https://pytorch.org/">PyTorch.org</a></li><li>Tensorboard tutorial content in the <a href="https://pytorch.org/tutorials/">PyTorch.org Tutorials</a></li><li>For more information about TensorBoard, see the <a href="https://www.tensorflow.org/tensorboard">TensorBoard documentation</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
      <tag>Python</tag>
      
      <tag>SummaryWritter</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch Tutorial-Building models</title>
    <link href="/2025/08/12/Building%20models/"/>
    <url>/2025/08/12/Building%20models/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习Pytorch时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="torch-nn-Module-torch-nn-Parameter"><a href="#torch-nn-Module-torch-nn-Parameter" class="headerlink" title="torch.nn.Module &amp; torch.nn.Parameter"></a>torch.nn.Module &amp; torch.nn.Parameter</h1><p>In this section, we’ll be discussing some of the tools PyTorch makes available for building deep learning networks.</p><p>Except for <code>Parameter</code>, the classes we discuss in this section are all subclasses of <code>torch.nn.Module</code>. This is the PyTorch base class meant to encapsulate(封装) behaviors specific to PyTorch Models and their components.</p><p>One important behavior of <code>torch.nn.Module</code> is <strong>registering parameters</strong>. If a particular <code>Module</code> subclass has learning weights, these weights are expressed as instances of <code>torch.nn.Parameter</code>. The <code>Parameter</code> class is a subclass of <code>torch.Tensor</code>, with the special behavior that when they are assigned as attributes of a <code>Module</code>, they are added to the list of that modules parameters. These parameters may be accessed through the <code>parameters()</code> method on the <code>Module</code> class.</p><p>As a simple example, here’s a very simple model with two linear layers and an activation function. We’ll create an instance of it and ask it to report on its parameters:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TinyModel</span>(torch.nn.Module):<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(TinyModel, <span class="hljs-variable language_">self</span>).__init__()<br>        <br>        <span class="hljs-variable language_">self</span>.linear1 = torch.nn.Linear(<span class="hljs-number">100</span>, <span class="hljs-number">200</span>)<br>        <span class="hljs-variable language_">self</span>.activation = torch.nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.linear2 = torch.nn.Linear(<span class="hljs-number">200</span>, <span class="hljs-number">10</span>)<br>        <span class="hljs-variable language_">self</span>.softmax = torch.nn.Softmax()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.linear1(x)<br>        x = <span class="hljs-variable language_">self</span>.activation(x)<br>        x = <span class="hljs-variable language_">self</span>.linear2(x)<br>        x = <span class="hljs-variable language_">self</span>.softmax(x)<br>        <span class="hljs-keyword">return</span> x<br><br>tinymodel = TinyModel()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The model:&#x27;</span>)<br><span class="hljs-built_in">print</span>(tinymodel)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nJust one layer:&#x27;</span>)<br><span class="hljs-built_in">print</span>(tinymodel.linear2)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nModel params:&#x27;</span>)<br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> tinymodel.parameters():<br>    <span class="hljs-built_in">print</span>(param)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nLayer params:&#x27;</span>)<br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> tinymodel.linear2.parameters():<br>    <span class="hljs-built_in">print</span>(param)<br><br>The model:<br>TinyModel(<br>  (linear1): Linear(in_features=<span class="hljs-number">100</span>, out_features=<span class="hljs-number">200</span>, bias=<span class="hljs-literal">True</span>)<br>  (activation): ReLU()<br>  (linear2): Linear(in_features=<span class="hljs-number">200</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br>  (softmax): Softmax(dim=<span class="hljs-literal">None</span>)<br>)<br><br>Just one layer:<br>Linear(in_features=<span class="hljs-number">200</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br><br>Model params:<br>Parameter containing:<br>tensor([[-<span class="hljs-number">0.0897</span>,  <span class="hljs-number">0.0595</span>,  <span class="hljs-number">0.0172</span>,  ...,  <span class="hljs-number">0.0947</span>, -<span class="hljs-number">0.0384</span>, -<span class="hljs-number">0.0024</span>],<br>        [ <span class="hljs-number">0.0102</span>, -<span class="hljs-number">0.0393</span>, -<span class="hljs-number">0.0865</span>,  ...,  <span class="hljs-number">0.0961</span>,  <span class="hljs-number">0.0694</span>,  <span class="hljs-number">0.0555</span>],<br>        [-<span class="hljs-number">0.0251</span>, -<span class="hljs-number">0.0372</span>,  <span class="hljs-number">0.0264</span>,  ...,  <span class="hljs-number">0.0535</span>, -<span class="hljs-number">0.0535</span>,  <span class="hljs-number">0.0745</span>],<br>        ...,<br>        [-<span class="hljs-number">0.0554</span>, -<span class="hljs-number">0.0434</span>, -<span class="hljs-number">0.0032</span>,  ..., -<span class="hljs-number">0.0441</span>, -<span class="hljs-number">0.0671</span>,  <span class="hljs-number">0.0100</span>],<br>        [ <span class="hljs-number">0.0469</span>, -<span class="hljs-number">0.0174</span>,  <span class="hljs-number">0.0883</span>,  ..., -<span class="hljs-number">0.0825</span>, -<span class="hljs-number">0.0478</span>,  <span class="hljs-number">0.0232</span>],<br>        [ <span class="hljs-number">0.0877</span>, -<span class="hljs-number">0.0416</span>, -<span class="hljs-number">0.0567</span>,  ..., -<span class="hljs-number">0.0455</span>, -<span class="hljs-number">0.0185</span>, -<span class="hljs-number">0.0559</span>]],<br>       requires_grad=<span class="hljs-literal">True</span>)<br>Parameter containing:<br>tensor([ <span class="hljs-number">0.0692</span>, -<span class="hljs-number">0.0213</span>,  <span class="hljs-number">0.0033</span>,  <span class="hljs-number">0.0528</span>,  <span class="hljs-number">0.0394</span>, -<span class="hljs-number">0.0518</span>, -<span class="hljs-number">0.0535</span>, -<span class="hljs-number">0.0269</span>,<br>         <span class="hljs-number">0.0172</span>, -<span class="hljs-number">0.0897</span>,  <span class="hljs-number">0.0809</span>,  <span class="hljs-number">0.0125</span>, -<span class="hljs-number">0.0566</span>, -<span class="hljs-number">0.0490</span>, -<span class="hljs-number">0.0566</span>,  <span class="hljs-number">0.0478</span>,<br>        -<span class="hljs-number">0.0488</span>,  <span class="hljs-number">0.0989</span>, -<span class="hljs-number">0.0641</span>, -<span class="hljs-number">0.0068</span>,  <span class="hljs-number">0.0420</span>,  <span class="hljs-number">0.0358</span>,  <span class="hljs-number">0.0186</span>,  <span class="hljs-number">0.0748</span>,<br>        -<span class="hljs-number">0.0308</span>,  <span class="hljs-number">0.0472</span>,  <span class="hljs-number">0.0568</span>,  <span class="hljs-number">0.0026</span>, -<span class="hljs-number">0.0920</span>, -<span class="hljs-number">0.0553</span>,  <span class="hljs-number">0.0737</span>,  <span class="hljs-number">0.0881</span>,<br>        -<span class="hljs-number">0.0992</span>,  <span class="hljs-number">0.0300</span>, -<span class="hljs-number">0.0234</span>, -<span class="hljs-number">0.0443</span>,  <span class="hljs-number">0.0221</span>, -<span class="hljs-number">0.0552</span>, -<span class="hljs-number">0.0067</span>,  <span class="hljs-number">0.0612</span>,<br>         <span class="hljs-number">0.0281</span>, -<span class="hljs-number">0.0199</span>, -<span class="hljs-number">0.0818</span>,  <span class="hljs-number">0.0608</span>,  <span class="hljs-number">0.0975</span>, -<span class="hljs-number">0.0069</span>,  <span class="hljs-number">0.0923</span>, -<span class="hljs-number">0.0741</span>,<br>         <span class="hljs-number">0.0516</span>, -<span class="hljs-number">0.0787</span>, -<span class="hljs-number">0.0593</span>, -<span class="hljs-number">0.0303</span>,  <span class="hljs-number">0.0115</span>,  <span class="hljs-number">0.0701</span>, -<span class="hljs-number">0.0171</span>,  <span class="hljs-number">0.0291</span>,<br>         <span class="hljs-number">0.0152</span>,  <span class="hljs-number">0.0424</span>, -<span class="hljs-number">0.0106</span>, -<span class="hljs-number">0.0568</span>,  <span class="hljs-number">0.0689</span>,  <span class="hljs-number">0.0308</span>,  <span class="hljs-number">0.0863</span>, -<span class="hljs-number">0.0436</span>,<br>         <span class="hljs-number">0.0061</span>,  <span class="hljs-number">0.0822</span>, -<span class="hljs-number">0.0556</span>, -<span class="hljs-number">0.0668</span>,  <span class="hljs-number">0.0828</span>,  <span class="hljs-number">0.0758</span>,  <span class="hljs-number">0.0888</span>, -<span class="hljs-number">0.0535</span>,<br>         <span class="hljs-number">0.0648</span>,  <span class="hljs-number">0.0160</span>, -<span class="hljs-number">0.0932</span>,  <span class="hljs-number">0.0787</span>,  <span class="hljs-number">0.0546</span>, -<span class="hljs-number">0.0973</span>,  <span class="hljs-number">0.0973</span>,  <span class="hljs-number">0.0908</span>,<br>         <span class="hljs-number">0.0108</span>, -<span class="hljs-number">0.0090</span>,  <span class="hljs-number">0.0644</span>,  <span class="hljs-number">0.0990</span>,  <span class="hljs-number">0.0384</span>,  <span class="hljs-number">0.0852</span>,  <span class="hljs-number">0.0864</span>,  <span class="hljs-number">0.0565</span>,<br>        -<span class="hljs-number">0.0974</span>,  <span class="hljs-number">0.0768</span>,  <span class="hljs-number">0.0337</span>,  <span class="hljs-number">0.0590</span>, -<span class="hljs-number">0.0362</span>,  <span class="hljs-number">0.0914</span>,  <span class="hljs-number">0.0038</span>,  <span class="hljs-number">0.0516</span>,<br>        -<span class="hljs-number">0.0632</span>, -<span class="hljs-number">0.0569</span>, -<span class="hljs-number">0.0475</span>, -<span class="hljs-number">0.0564</span>, -<span class="hljs-number">0.0192</span>,  <span class="hljs-number">0.0279</span>, -<span class="hljs-number">0.0243</span>, -<span class="hljs-number">0.0621</span>,<br>        -<span class="hljs-number">0.0559</span>,  <span class="hljs-number">0.0921</span>, -<span class="hljs-number">0.0583</span>, -<span class="hljs-number">0.0508</span>,  <span class="hljs-number">0.0401</span>,  <span class="hljs-number">0.0414</span>, -<span class="hljs-number">0.0770</span>, -<span class="hljs-number">0.0378</span>,<br>        -<span class="hljs-number">0.0786</span>, -<span class="hljs-number">0.0110</span>, -<span class="hljs-number">0.0289</span>, -<span class="hljs-number">0.0778</span>,  <span class="hljs-number">0.0427</span>, -<span class="hljs-number">0.0105</span>,  <span class="hljs-number">0.0680</span>,  <span class="hljs-number">0.0146</span>,<br>        -<span class="hljs-number">0.0859</span>,  <span class="hljs-number">0.0440</span>, -<span class="hljs-number">0.0420</span>,  <span class="hljs-number">0.0613</span>,  <span class="hljs-number">0.0321</span>,  <span class="hljs-number">0.0289</span>,  <span class="hljs-number">0.0668</span>, -<span class="hljs-number">0.0028</span>,<br>        -<span class="hljs-number">0.0421</span>, -<span class="hljs-number">0.0372</span>,  <span class="hljs-number">0.0391</span>,  <span class="hljs-number">0.0479</span>, -<span class="hljs-number">0.0232</span>, -<span class="hljs-number">0.0610</span>, -<span class="hljs-number">0.0355</span>, -<span class="hljs-number">0.0896</span>,<br>         <span class="hljs-number">0.0864</span>,  <span class="hljs-number">0.0345</span>, -<span class="hljs-number">0.0252</span>, -<span class="hljs-number">0.0385</span>,  <span class="hljs-number">0.0832</span>,  <span class="hljs-number">0.0868</span>, -<span class="hljs-number">0.0514</span>,  <span class="hljs-number">0.0178</span>,<br>         <span class="hljs-number">0.0716</span>,  <span class="hljs-number">0.0796</span>, -<span class="hljs-number">0.0794</span>, -<span class="hljs-number">0.0538</span>, -<span class="hljs-number">0.0163</span>, -<span class="hljs-number">0.0929</span>, -<span class="hljs-number">0.0643</span>,  <span class="hljs-number">0.0782</span>,<br>        -<span class="hljs-number">0.0047</span>,  <span class="hljs-number">0.0024</span>, -<span class="hljs-number">0.0610</span>, -<span class="hljs-number">0.0259</span>,  <span class="hljs-number">0.0719</span>,  <span class="hljs-number">0.0840</span>,  <span class="hljs-number">0.0946</span>, -<span class="hljs-number">0.0291</span>,<br>         <span class="hljs-number">0.0131</span>, -<span class="hljs-number">0.0157</span>,  <span class="hljs-number">0.0309</span>, -<span class="hljs-number">0.0375</span>, -<span class="hljs-number">0.0800</span>, -<span class="hljs-number">0.0594</span>, -<span class="hljs-number">0.0233</span>, -<span class="hljs-number">0.0928</span>,<br>        -<span class="hljs-number">0.0028</span>, -<span class="hljs-number">0.0729</span>,  <span class="hljs-number">0.0889</span>, -<span class="hljs-number">0.0377</span>, -<span class="hljs-number">0.0685</span>,  <span class="hljs-number">0.0974</span>, -<span class="hljs-number">0.0860</span>, -<span class="hljs-number">0.0819</span>,<br>        -<span class="hljs-number">0.0918</span>, -<span class="hljs-number">0.0750</span>, -<span class="hljs-number">0.0327</span>, -<span class="hljs-number">0.0245</span>, -<span class="hljs-number">0.0058</span>, -<span class="hljs-number">0.0875</span>, -<span class="hljs-number">0.0667</span>, -<span class="hljs-number">0.0569</span>,<br>         <span class="hljs-number">0.0075</span>,  <span class="hljs-number">0.0986</span>,  <span class="hljs-number">0.0977</span>, -<span class="hljs-number">0.0291</span>,  <span class="hljs-number">0.0081</span>,  <span class="hljs-number">0.0127</span>,  <span class="hljs-number">0.0544</span>,  <span class="hljs-number">0.0711</span>,<br>         <span class="hljs-number">0.0910</span>,  <span class="hljs-number">0.0522</span>, -<span class="hljs-number">0.0874</span>, -<span class="hljs-number">0.0217</span>,  <span class="hljs-number">0.0454</span>, -<span class="hljs-number">0.0726</span>,  <span class="hljs-number">0.0791</span>, -<span class="hljs-number">0.0459</span>],<br>       requires_grad=<span class="hljs-literal">True</span>)<br>Parameter containing:<br>tensor([[-<span class="hljs-number">0.0508</span>,  <span class="hljs-number">0.0529</span>,  <span class="hljs-number">0.0234</span>,  ..., -<span class="hljs-number">0.0385</span>,  <span class="hljs-number">0.0078</span>, -<span class="hljs-number">0.0030</span>],<br>        [ <span class="hljs-number">0.0281</span>,  <span class="hljs-number">0.0437</span>, -<span class="hljs-number">0.0461</span>,  ..., -<span class="hljs-number">0.0655</span>, -<span class="hljs-number">0.0253</span>, -<span class="hljs-number">0.0222</span>],<br>        [ <span class="hljs-number">0.0243</span>,  <span class="hljs-number">0.0178</span>, -<span class="hljs-number">0.0009</span>,  ...,  <span class="hljs-number">0.0383</span>, -<span class="hljs-number">0.0507</span>, -<span class="hljs-number">0.0083</span>],<br>        ...,<br>        [-<span class="hljs-number">0.0700</span>, -<span class="hljs-number">0.0090</span>,  <span class="hljs-number">0.0153</span>,  ...,  <span class="hljs-number">0.0161</span>,  <span class="hljs-number">0.0610</span>,  <span class="hljs-number">0.0687</span>],<br>        [-<span class="hljs-number">0.0509</span>, -<span class="hljs-number">0.0291</span>, -<span class="hljs-number">0.0591</span>,  ...,  <span class="hljs-number">0.0173</span>, -<span class="hljs-number">0.0191</span>, -<span class="hljs-number">0.0705</span>],<br>        [-<span class="hljs-number">0.0090</span>,  <span class="hljs-number">0.0428</span>, -<span class="hljs-number">0.0528</span>,  ...,  <span class="hljs-number">0.0278</span>, -<span class="hljs-number">0.0153</span>, -<span class="hljs-number">0.0266</span>]],<br>       requires_grad=<span class="hljs-literal">True</span>)<br>Parameter containing:<br>tensor([-<span class="hljs-number">0.0357</span>, -<span class="hljs-number">0.0617</span>,  <span class="hljs-number">0.0027</span>, -<span class="hljs-number">0.0098</span>, -<span class="hljs-number">0.0083</span>, -<span class="hljs-number">0.0461</span>, -<span class="hljs-number">0.0076</span>,  <span class="hljs-number">0.0510</span>,<br>        -<span class="hljs-number">0.0564</span>,  <span class="hljs-number">0.0298</span>], requires_grad=<span class="hljs-literal">True</span>)<br><br>Layer params:<br>Parameter containing:<br>tensor([[-<span class="hljs-number">0.0508</span>,  <span class="hljs-number">0.0529</span>,  <span class="hljs-number">0.0234</span>,  ..., -<span class="hljs-number">0.0385</span>,  <span class="hljs-number">0.0078</span>, -<span class="hljs-number">0.0030</span>],<br>        [ <span class="hljs-number">0.0281</span>,  <span class="hljs-number">0.0437</span>, -<span class="hljs-number">0.0461</span>,  ..., -<span class="hljs-number">0.0655</span>, -<span class="hljs-number">0.0253</span>, -<span class="hljs-number">0.0222</span>],<br>        [ <span class="hljs-number">0.0243</span>,  <span class="hljs-number">0.0178</span>, -<span class="hljs-number">0.0009</span>,  ...,  <span class="hljs-number">0.0383</span>, -<span class="hljs-number">0.0507</span>, -<span class="hljs-number">0.0083</span>],<br>        ...,<br>        [-<span class="hljs-number">0.0700</span>, -<span class="hljs-number">0.0090</span>,  <span class="hljs-number">0.0153</span>,  ...,  <span class="hljs-number">0.0161</span>,  <span class="hljs-number">0.0610</span>,  <span class="hljs-number">0.0687</span>],<br>        [-<span class="hljs-number">0.0509</span>, -<span class="hljs-number">0.0291</span>, -<span class="hljs-number">0.0591</span>,  ...,  <span class="hljs-number">0.0173</span>, -<span class="hljs-number">0.0191</span>, -<span class="hljs-number">0.0705</span>],<br>        [-<span class="hljs-number">0.0090</span>,  <span class="hljs-number">0.0428</span>, -<span class="hljs-number">0.0528</span>,  ...,  <span class="hljs-number">0.0278</span>, -<span class="hljs-number">0.0153</span>, -<span class="hljs-number">0.0266</span>]],<br>       requires_grad=<span class="hljs-literal">True</span>)<br>Parameter containing:<br>tensor([-<span class="hljs-number">0.0357</span>, -<span class="hljs-number">0.0617</span>,  <span class="hljs-number">0.0027</span>, -<span class="hljs-number">0.0098</span>, -<span class="hljs-number">0.0083</span>, -<span class="hljs-number">0.0461</span>, -<span class="hljs-number">0.0076</span>,  <span class="hljs-number">0.0510</span>,<br>        -<span class="hljs-number">0.0564</span>,  <span class="hljs-number">0.0298</span>], requires_grad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>This shows the fundamental structure of a PyTorch model: there is an <code>__init__()</code> method that defines the layers and other components of a model, and a <code>forward()</code> method where the computation gets done. Note that we can print the model, or any of its submodules, to learn about its structure.</strong></li></ul><h1 id="Common-Layer-Types"><a href="#Common-Layer-Types" class="headerlink" title="Common Layer Types"></a>Common Layer Types</h1><h2 id="Linear-Layers"><a href="#Linear-Layers" class="headerlink" title="Linear Layers"></a>Linear Layers</h2><p>The most basic type of neural network layer is a <em>linear</em> or <em>fully connected</em> layer. This is a layer where <strong>every input influences every output</strong> of the layer to a degree specified by the layer’s <code>weights</code>. If a model has <em>m</em> inputs and <em>n</em> outputs, the weights will be an <em>m * n</em> matrix. For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python">lin = torch.nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>x = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Input:&#x27;</span>)<br><span class="hljs-built_in">print</span>(x)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nWeight and Bias parameters:&#x27;</span>)<br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> lin.parameters():<br>    <span class="hljs-built_in">print</span>(param)<br><br>y = lin(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nOutput:&#x27;</span>)<br><span class="hljs-built_in">print</span>(y)<br><br>Input:<br>tensor([[<span class="hljs-number">0.2807</span>, <span class="hljs-number">0.5842</span>, <span class="hljs-number">0.7967</span>]])<br><br>Weight <span class="hljs-keyword">and</span> Bias parameters:<br>Parameter containing:<br>tensor([[-<span class="hljs-number">0.1719</span>,  <span class="hljs-number">0.4691</span>, -<span class="hljs-number">0.0654</span>],<br>        [-<span class="hljs-number">0.2522</span>,  <span class="hljs-number">0.5453</span>, -<span class="hljs-number">0.5438</span>]], requires_grad=<span class="hljs-literal">True</span>)<br>Parameter containing:<br>tensor([<span class="hljs-number">0.2956</span>, <span class="hljs-number">0.2001</span>], requires_grad=<span class="hljs-literal">True</span>)<br><br>Output:<br>tensor([[<span class="hljs-number">0.4693</span>, <span class="hljs-number">0.0146</span>]], grad_fn=&lt;AddmmBackward&gt;)<br></code></pre></td></tr></table></figure><ul><li>Parameter会自动开启<code>autograd</code></li><li>Linear layers are used widely in deep learning models. One of the most common places you’ll see them is in <code>classifier models</code></li></ul><hr><h2 id="Convolutional-Layers"><a href="#Convolutional-Layers" class="headerlink" title="Convolutional Layers"></a>Convolutional Layers</h2><ul><li><em>Convolutional</em> layers are built to handle data with <strong>a high degree of spatial correlation</strong>. They are very commonly used in computer vision, where they detect close groupings of features which the compose into higher-level features. They pop up in other contexts too - for example, in NLP applications, where the a word’s immediate context (that is, the other words nearby in the sequence) can affect the meaning of a sentence.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LeNet</span>(torch.nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(LeNet, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 1 input image channel (black &amp; white), 6 output channels, 3x3 square convolution</span><br>        <span class="hljs-comment"># kernel</span><br>        <span class="hljs-variable language_">self</span>.conv1 = torch.nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = torch.nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>)<br>        <span class="hljs-comment"># an affine operation: y = Wx + b</span><br>        <span class="hljs-variable language_">self</span>.fc1 = torch.nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">6</span> * <span class="hljs-number">6</span>, <span class="hljs-number">120</span>)  <span class="hljs-comment"># 6*6 from image dimension</span><br>        <span class="hljs-variable language_">self</span>.fc2 = torch.nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = torch.nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># Max pooling over a (2, 2) window</span><br>        x = F.max_pool2d(F.relu(<span class="hljs-variable language_">self</span>.conv1(x)), (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>        <span class="hljs-comment"># If the size is a square you can only specify a single number</span><br>        x = F.max_pool2d(F.relu(<span class="hljs-variable language_">self</span>.conv2(x)), <span class="hljs-number">2</span>)<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.num_flat_features(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">num_flat_features</span>(<span class="hljs-params">self, x</span>):<br>        size = x.size()[<span class="hljs-number">1</span>:]  <span class="hljs-comment"># all dimensions except the batch dimension</span><br>        num_features = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> size:<br>            num_features *= s<br>        <span class="hljs-keyword">return</span> num_features<br></code></pre></td></tr></table></figure><ul><li>卷积层构造函数的<strong>第一个参数是输入通道的数量</strong>，<strong>第二个参数是输出特征的数量</strong>，<strong>第三个参数是窗口或<code>kernel</code>内核大小</strong></li></ul><hr><p>关于卷积神经网络（CNN）中 <strong>卷积层（Convolutional Layer）</strong>、<strong>ReLU激活函数</strong> 和 <strong>最大池化层（Max Pooling Layer）</strong> 的处理流程及其作用。以下是逐步解析：</p><h3 id="1-卷积层（Convolutional-Layer）的输出"><a href="#1-卷积层（Convolutional-Layer）的输出" class="headerlink" title="1. 卷积层（Convolutional Layer）的输出"></a>1. 卷积层（Convolutional Layer）的输出</h3><ul><li><strong>输入假设</strong>：假设输入是一个单通道（灰度）的 <code>32x32</code> 图像，经过第一层卷积操作：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.conv1 = torch.nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)  <span class="hljs-comment"># 输入通道1，输出通道6，卷积核5x5</span><br></code></pre></td></tr></table></figure></li><li><strong>输出尺寸</strong>：<br>卷积后输出的尺寸计算公式为：<br>$$<br>\text{输出尺寸} &#x3D; \left\lfloor \frac{\text{输入尺寸} - \text{卷积核尺寸} + 2 \times \text{填充}}{\text{步长}} \right\rfloor + 1<br>$$<br>默认情况下，<code>padding=0</code>（无填充），<code>stride=1</code>（步长为1），因此：32 - 5 + 1 &#x3D; 28<ul><li><strong>输出张量形状</strong>：<code>[batch_size, 6, 28, 28]</code><br>（6个通道，每个通道的激活图大小为 <code>28x28</code>）。</li></ul></li></ul><h3 id="2-ReLU激活函数的作用"><a href="#2-ReLU激活函数的作用" class="headerlink" title="2. ReLU激活函数的作用"></a>2. ReLU激活函数的作用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">F.relu(<span class="hljs-variable language_">self</span>.conv1(x))<br></code></pre></td></tr></table></figure><ul><li><strong>ReLU（Rectified Linear Unit）</strong>：定义为  $\text{ReLU}(x) &#x3D; \max(0, x)$ 。</li><li><strong>功能</strong>：<ol><li><strong>引入非线性</strong>：使模型能够学习复杂的非线性关系。</li><li><strong>稀疏激活</strong>：将负值置零，保留正值，增强模型的稀疏性。</li><li><strong>缓解梯度消失</strong>：相比 Sigmoid&#x2F;Tanh，ReLU 的梯度在正区间恒为1，避免梯度消失问题。</li></ol></li><li><strong>输出形状</strong>：与输入相同，仍为 <code>[batch_size, 6, 28, 28]</code>。</li></ul><h3 id="3-最大池化层（Max-Pooling）的细节"><a href="#3-最大池化层（Max-Pooling）的细节" class="headerlink" title="3. 最大池化层（Max Pooling）的细节"></a>3. 最大池化层（Max Pooling）的细节</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">F.max_pool2d(..., (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))  <span class="hljs-comment"># 2x2的池化窗口</span><br></code></pre></td></tr></table></figure><ul><li><strong>目的</strong>：降低空间维度（下采样），减少计算量并增强平移不变性。</li><li><strong>操作规则</strong>：<ul><li>将输入激活图划分为不重叠的 <code>2x2</code> 区域。</li><li>对每个区域取最大值，作为输出。</li><li><strong>步长默认等于池化窗口大小</strong>（即 <code>stride=2</code>），因此输出尺寸减半。</li></ul></li><li><strong>计算示例</strong>：<ul><li>输入尺寸：<code>[batch_size, 6, 28, 28]</code>。</li><li>输出尺寸：<br>$\left\lfloor \frac{28 - 2}{2} \right\rfloor + 1 &#x3D; 14$</li><li><strong>输出张量形状</strong>：<code>[batch_size, 6, 14, 14]</code>。</li></ul></li></ul><h4 id="为什么选择最大值？"><a href="#为什么选择最大值？" class="headerlink" title="为什么选择最大值？"></a>为什么选择最大值？</h4><ul><li><strong>保留最显著特征</strong>：最大值代表该区域最强烈的激活响应，有助于保留重要特征（如边缘、纹理）。</li><li><strong>抑制噪声</strong>：忽略非最大值，降低噪声干扰。</li></ul><h3 id="4-维度变化的直观理解"><a href="#4-维度变化的直观理解" class="headerlink" title="4. 维度变化的直观理解"></a>4. 维度变化的直观理解</h3><table><thead><tr><th>操作</th><th>输入形状</th><th>输出形状</th><th>关键作用</th></tr></thead><tbody><tr><td>卷积（Conv1）</td><td><code>[1, 1, 32, 32]</code></td><td><code>[1, 6, 28, 28]</code></td><td>提取局部特征，增加通道数</td></tr><tr><td>ReLU</td><td><code>[1, 6, 28, 28]</code></td><td><code>[1, 6, 28, 28]</code></td><td>引入非线性，过滤负值</td></tr><tr><td>最大池化</td><td><code>[1, 6, 28, 28]</code></td><td><code>[1, 6, 14, 14]</code></td><td>降低分辨率，增强鲁棒性</td></tr></tbody></table><h3 id="5-为什么需要这些步骤？"><a href="#5-为什么需要这些步骤？" class="headerlink" title="5. 为什么需要这些步骤？"></a>5. 为什么需要这些步骤？</h3><ol><li><strong>卷积层</strong>：<ul><li>通过局部感受野<strong>提取空间特征</strong>（如边缘、角点）。</li><li>使用多个卷积核（通道）<strong>捕捉不同特征模式</strong>。</li></ul></li><li><strong>ReLU</strong>：<ul><li>解决线性模型的局限性，使网络能<strong>拟合复杂函数</strong>。</li></ul></li><li><strong>池化层</strong>：<ul><li>减少参数数量，<strong>防止过拟合</strong></li><li>使模型对输入的小平移&#x2F;形变更鲁棒（“近似不变性”）。</li></ul></li></ol><ul><li>There are convolutional layers for addressing 1D, 2D, and 3D tensors. There are also many more optional arguments for a conv layer constructor, including <code>stride length</code>(e.g., only <strong>scanning every second or every third position</strong>) in the input, <code>padding</code> (so you can <strong>scan out to the edges of the input</strong>), and more. See the <a href="https://pytorch.org/docs/stable/nn.html#convolution-layers">documentation</a> for more information.</li></ul><hr><h2 id="Recurrent-Layers"><a href="#Recurrent-Layers" class="headerlink" title="Recurrent Layers"></a>Recurrent Layers</h2><ul><li><em>Recurrent neural networks</em> (or <em>RNNs)</em> are used for <code>sequential data</code> - anything from <strong>time-series measurements</strong> from a scientific instrument to natural language sentences to DNA nucleotides. An RNN does this by maintaining a <em>hidden state</em> that acts as a sort of memory for what it has seen in the sequence so far.</li><li>The internal structure of an RNN layer - or its variants, the <code>LSTM</code> (long short-term memory) and <code>GRU</code> (gated recurrent unit) - is moderately complex and beyond the scope of this video, but we’ll show you what one looks like in action with an <code>LSTM-based part-of-speech tagger</code> (a type of classifier that tells you if a word is a noun, verb, etc.):</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LSTMTagger</span>(torch.nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, embedding_dim, hidden_dim, vocab_size, tagset_size</span>):<br>        <span class="hljs-built_in">super</span>(LSTMTagger, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.hidden_dim = hidden_dim<br><br>        <span class="hljs-variable language_">self</span>.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)<br><br>        <span class="hljs-comment"># The LSTM takes word embeddings as inputs, and outputs hidden states</span><br>        <span class="hljs-comment"># with dimensionality hidden_dim.</span><br>        <span class="hljs-variable language_">self</span>.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)<br><br>        <span class="hljs-comment"># The linear layer that maps from hidden state space to tag space</span><br>        <span class="hljs-variable language_">self</span>.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sentence</span>):<br>        embeds = <span class="hljs-variable language_">self</span>.word_embeddings(sentence)<br>        lstm_out, _ = <span class="hljs-variable language_">self</span>.lstm(embeds.view(<span class="hljs-built_in">len</span>(sentence), <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))<br>        tag_space = <span class="hljs-variable language_">self</span>.hidden2tag(lstm_out.view(<span class="hljs-built_in">len</span>(sentence), -<span class="hljs-number">1</span>))<br>        tag_scores = F.log_softmax(tag_space, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> tag_scores<br></code></pre></td></tr></table></figure><ul><li><p>The constructor has four arguments:</p></li><li><p><code>vocab_size</code> is the number of words in the input vocabulary. Each word is a <code>one-hot vector</code> (or unit vector) in a <code>vocab_size</code>-dimensional space.</p></li><li><p><code>tagset_size</code> is the number of tags in the output set.</p></li><li><p><code>embedding_dim</code> is the size of the <em>embedding</em> space for the vocabulary. An embedding <strong>maps a vocabulary onto a low-dimensional space</strong>, where words with similar meanings are close together in the space.</p></li><li><p><code>hidden_dim</code> is the size of the LSTM’s memory.</p></li></ul><p>The input will be a sentence with the <strong>words represented as indices  of one-hot vectors</strong>. The embedding layer will then map these down to an <code>embedding_dim</code>-dimensional space. The LSTM takes this sequence of embeddings and iterates over it, fielding an output vector of length <code>hidden_dim</code>. The final linear layer <strong>acts as a classifier</strong>; applying <code>log_softmax()</code> to the output of the final layer converts the output into <strong>a normalized set of estimated probabilities</strong> that a given word maps to a given tag.</p><p>If you’d like to see this network in action, check out the <a href="https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html">Sequence Models and LSTM Networks</a> tutorial on pytorch.org.</p><hr><h2 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h2><p><em>Transformers</em> are multi-purpose networks that have taken over the state of the art in NLP with models like <code>BERT</code>. A discussion of transformer architecture is beyond the scope of this video, but PyTorch has a <code>Transformer</code> class that allows you to <strong>define the overall parameters of a transformer model</strong> - the number of attention heads, the number of encoder &amp; decoder layers, dropout and activation functions, etc. (You can even build the BERT model from this single class, with the right parameters!) The <code>torch.nn.Transformer</code> class also has classes to encapsulate the individual components (<code>TransformerEncoder</code>, <code>TransformerDecoder</code>) and subcomponents (<code>TransformerEncoderLayer</code>, <code>TransformerDecoderLayer</code>). For details, check out the <a href="https://pytorch.org/docs/stable/nn.html#transformer">documentation</a> on transformer classes, and the relevant <a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">tutorial</a> on pytorch.org.</p><h1 id="Other-Layers-and-Functions"><a href="#Other-Layers-and-Functions" class="headerlink" title="Other Layers and Functions"></a>Other Layers and Functions</h1><h2 id="Data-Manipulation-Layers"><a href="#Data-Manipulation-Layers" class="headerlink" title="Data Manipulation Layers"></a>Data Manipulation Layers</h2><p>There are other layer types that perform important functions in models, but don’t participate in the learning process themselves.</p><h3 id="Max-pooling"><a href="#Max-pooling" class="headerlink" title="Max pooling"></a>Max pooling</h3><p><code>Max pooling</code> (and its twin, <code>min pooling</code>) <strong>reduce a tensor by combining cells, and assigning the maximum value</strong> of the input cells to the output cell. (We saw this ) For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">my_tensor = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>)<br><span class="hljs-built_in">print</span>(my_tensor)<br>maxpool_layer = torch.nn.MaxPool2d(<span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(maxpool_layer(my_tensor))<br><br>tensor([[[<span class="hljs-number">0.8160</span>, <span class="hljs-number">0.1406</span>, <span class="hljs-number">0.5950</span>, <span class="hljs-number">0.0883</span>, <span class="hljs-number">0.5464</span>, <span class="hljs-number">0.3993</span>],<br>         [<span class="hljs-number">0.0623</span>, <span class="hljs-number">0.6626</span>, <span class="hljs-number">0.3991</span>, <span class="hljs-number">0.4878</span>, <span class="hljs-number">0.7548</span>, <span class="hljs-number">0.2426</span>],<br>         [<span class="hljs-number">0.9081</span>, <span class="hljs-number">0.4207</span>, <span class="hljs-number">0.8590</span>, <span class="hljs-number">0.3784</span>, <span class="hljs-number">0.6931</span>, <span class="hljs-number">0.5609</span>],<br>         [<span class="hljs-number">0.6182</span>, <span class="hljs-number">0.8588</span>, <span class="hljs-number">0.3766</span>, <span class="hljs-number">0.9734</span>, <span class="hljs-number">0.9662</span>, <span class="hljs-number">0.9880</span>],<br>         [<span class="hljs-number">0.0599</span>, <span class="hljs-number">0.8338</span>, <span class="hljs-number">0.6750</span>, <span class="hljs-number">0.0829</span>, <span class="hljs-number">0.3554</span>, <span class="hljs-number">0.3998</span>],<br>         [<span class="hljs-number">0.6159</span>, <span class="hljs-number">0.7129</span>, <span class="hljs-number">0.8945</span>, <span class="hljs-number">0.8717</span>, <span class="hljs-number">0.9930</span>, <span class="hljs-number">0.9059</span>]]])<br>tensor([[[<span class="hljs-number">0.9081</span>, <span class="hljs-number">0.7548</span>],<br>         [<span class="hljs-number">0.8945</span>, <span class="hljs-number">0.9930</span>]]])<br></code></pre></td></tr></table></figure><p>If you look closely at the values above, you’ll see that each of the values in the maxpooled output is the maximum value of each quadrant of the 6x6 input.</p><h3 id="Normalization-layers"><a href="#Normalization-layers" class="headerlink" title="Normalization layers"></a>Normalization layers</h3><p><code>Normalization layers</code> <strong>re-center and normalize the output</strong> of one layer before feeding it to another. <strong>Centering  and scaling</strong> the intermediate tensors has a number of beneficial effects, such as letting you <strong>use higher learning rates without exploding&#x2F;vanishing gradients</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">my_tensor = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>) * <span class="hljs-number">20</span> + <span class="hljs-number">5</span><br><span class="hljs-built_in">print</span>(my_tensor)<br><span class="hljs-built_in">print</span>(my_tensor.mean())<br>norm_layer = torch.nn.BatchNorm1d(<span class="hljs-number">4</span>)<br>normed_tensor = norm_layer(my_tensor)<br><span class="hljs-built_in">print</span>(normed_tensor)<br><span class="hljs-built_in">print</span>(normed_tensor.mean())<br><br>tensor([[[<span class="hljs-number">18.0634</span>,  <span class="hljs-number">5.6720</span>,  <span class="hljs-number">5.7805</span>, <span class="hljs-number">12.3243</span>],<br>         [ <span class="hljs-number">9.3712</span>, <span class="hljs-number">19.7366</span>,  <span class="hljs-number">6.4853</span>, <span class="hljs-number">22.8629</span>],<br>         [<span class="hljs-number">14.6223</span>, <span class="hljs-number">21.5803</span>, <span class="hljs-number">17.8267</span>, <span class="hljs-number">20.3997</span>],<br>         [<span class="hljs-number">21.7664</span>,  <span class="hljs-number">5.0936</span>, <span class="hljs-number">19.5952</span>, <span class="hljs-number">11.8554</span>]]])<br>tensor(<span class="hljs-number">14.5647</span>)<br>tensor([[[ <span class="hljs-number">1.4762</span>, -<span class="hljs-number">0.9296</span>, -<span class="hljs-number">0.9086</span>,  <span class="hljs-number">0.3619</span>],<br>         [-<span class="hljs-number">0.7650</span>,  <span class="hljs-number">0.7475</span>, -<span class="hljs-number">1.1862</span>,  <span class="hljs-number">1.2037</span>],<br>         [-<span class="hljs-number">1.4918</span>,  <span class="hljs-number">1.1130</span>, -<span class="hljs-number">0.2922</span>,  <span class="hljs-number">0.6710</span>],<br>         [ <span class="hljs-number">1.0893</span>, -<span class="hljs-number">1.4371</span>,  <span class="hljs-number">0.7603</span>, -<span class="hljs-number">0.4125</span>]]],<br>       grad_fn=&lt;NativeBatchNormBackward&gt;)<br>tensor(<span class="hljs-number">1.3039e-08</span>, grad_fn=&lt;MeanBackward0&gt;)<br></code></pre></td></tr></table></figure><p>Running the cell above, we’ve added <strong>a large scaling factor and offset</strong> to an input tensor; you should see the input tensor’s <code>mean()</code> somewhere in the neighborhood of 15. After running it through the normalization layer, you can see that <strong>the values are smaller, and grouped around zero</strong> - in fact, the mean should be very small (&gt; 1e-8).</p><p>This is beneficial because many activation functions (discussed below) have their strongest gradients near 0, but sometimes <strong>suffer from</strong> vanishing or exploding gradients for inputs that <strong>drive them far away from zero</strong>. Keeping the data centered around the area of steepest gradient will tend to mean <strong>faster, better learning and higher feasible learning rates</strong>.</p><h3 id="Dropout-layers"><a href="#Dropout-layers" class="headerlink" title="Dropout layers"></a>Dropout layers</h3><p><strong>Dropout layers</strong> are a tool for encouraging <em>sparse representations</em> 稀疏表示 in your model - that is, pushing it to do inference with less data.</p><p>Dropout layers work by <strong>randomly setting</strong> parts of the input tensor <strong>zero</strong> <em>during training</em> - dropout layers are always turned off for inference 推理. This forces the model to learn against this <strong>masked or reduced dataset</strong>. For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">my_tensor = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br>dropout = torch.nn.Dropout(p=<span class="hljs-number">0.4</span>)<br><span class="hljs-built_in">print</span>(dropout(my_tensor))<br><span class="hljs-built_in">print</span>(dropout(my_tensor))<br><br>tensor([[[<span class="hljs-number">0.0000</span>, <span class="hljs-number">1.1702</span>, <span class="hljs-number">0.5911</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.1932</span>, <span class="hljs-number">1.4928</span>, <span class="hljs-number">1.2912</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.1236</span>, <span class="hljs-number">1.3672</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>]]])<br>tensor([[[<span class="hljs-number">1.5033</span>, <span class="hljs-number">1.1702</span>, <span class="hljs-number">0.5911</span>, <span class="hljs-number">0.9341</span>],<br>         [<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">1.5020</span>],<br>         [<span class="hljs-number">0.1236</span>, <span class="hljs-number">1.3672</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.4993</span>, <span class="hljs-number">0.9576</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">1.6664</span>]]])<br></code></pre></td></tr></table></figure><p>Above, you can see the effect of dropout on a sample tensor. You can use the optional <code>p</code> argument to set the probability of an individual weight dropping out; if you don’t it defaults to 0.5.</p><hr><h2 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h2><p>Activation functions make deep learning possible. <strong>A neural network is really a program</strong> - with many parameters - that <em>simulates a mathematical function</em>. If all we did was multiple tensors by layer weights repeatedly, we could only simulate <em>linear functions;</em> further, there would be no point to having many layers, as the whole network  could be reduced to a single matrix multiplication. Inserting <em>non-linear</em> activation functions between layers is what allows a deep learning model to <strong>simulate any function, rather than just linear ones.</strong></p><p><code>torch.nn.Module</code> has objects encapsulating 封装 all of the major activation functions including <code>ReLU</code> and its many variants, <code>Tanh</code>, <code>Hardtanh</code>, <code>sigmoid</code>, and more. It also includes other functions, such as <code>Softmax</code>, that are most useful at the output stage of a model.</p><hr><h2 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h2><p>Loss functions tell us how far a model’s prediction is from the correct answer. PyTorch contains a variety of loss functions, including common <code>MSE</code> (mean squared error &#x3D; L2 norm), <code>Cross Entropy Loss</code> and <code>Negative Likelihood Loss</code> (useful for classifiers), and others.</p><h1 id="Advanced-Replacing-Layers"><a href="#Advanced-Replacing-Layers" class="headerlink" title="Advanced: Replacing Layers"></a>Advanced: Replacing Layers</h1><ul><li>waiting to be updated😀</li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
      <tag>Python</tag>
      
      <tag>ML</tag>
      
      <tag>CNN</tag>
      
      <tag>RNN</tag>
      
      <tag>ReLU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch Tutorial-Autograd</title>
    <link href="/2025/08/12/Autograd/"/>
    <url>/2025/08/12/Autograd/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习Pytorch时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><h1 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h1><blockquote><p>autograd 的用武之地：它跟踪每次计算的历史记录。PyTorch 模型中的每个计算张量都带有其输入张量和用于创建张量的函数的历史记录。结合用于作用于张量的 PyTorch 函数，每个函数都有一个用于计算自己的导数的内置实现，这大大加快了学习所需的局部导数的计算速度。</p></blockquote><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib.ticker <span class="hljs-keyword">as</span> ticker<br><span class="hljs-keyword">import</span> math<br><br>a = torch.linspace(<span class="hljs-number">0.</span>, <span class="hljs-number">2.</span> * math.pi, steps=<span class="hljs-number">25</span>, requires_grad=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(a)<br>tensor([<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.2618</span>, <span class="hljs-number">0.5236</span>, <span class="hljs-number">0.7854</span>, <span class="hljs-number">1.0472</span>, <span class="hljs-number">1.3090</span>, <span class="hljs-number">1.5708</span>, <span class="hljs-number">1.8326</span>, <span class="hljs-number">2.0944</span>,<br>        <span class="hljs-number">2.3562</span>, <span class="hljs-number">2.6180</span>, <span class="hljs-number">2.8798</span>, <span class="hljs-number">3.1416</span>, <span class="hljs-number">3.4034</span>, <span class="hljs-number">3.6652</span>, <span class="hljs-number">3.9270</span>, <span class="hljs-number">4.1888</span>, <span class="hljs-number">4.4506</span>,<br>        <span class="hljs-number">4.7124</span>, <span class="hljs-number">4.9742</span>, <span class="hljs-number">5.2360</span>, <span class="hljs-number">5.4978</span>, <span class="hljs-number">5.7596</span>, <span class="hljs-number">6.0214</span>, <span class="hljs-number">6.2832</span>],<br>       requires_grad=<span class="hljs-literal">True</span>)<br>       <br>b = torch.sin(a)<br>c = <span class="hljs-number">2</span> * b<br>d = c + <span class="hljs-number">1</span><br>out = d.<span class="hljs-built_in">sum</span>()<br>out.backward()<br><span class="hljs-built_in">print</span>(a.grad)<br>plt.plot(a.detach(), a.grad.detach())<br><br>tensor([ <span class="hljs-number">2.0000e+00</span>,  <span class="hljs-number">1.9319e+00</span>,  <span class="hljs-number">1.7321e+00</span>,  <span class="hljs-number">1.4142e+00</span>,  <span class="hljs-number">1.0000e+00</span>,<br>         <span class="hljs-number">5.1764e-01</span>, -<span class="hljs-number">8.7423e-08</span>, -<span class="hljs-number">5.1764e-01</span>, -<span class="hljs-number">1.0000e+00</span>, -<span class="hljs-number">1.4142e+00</span>,<br>        -<span class="hljs-number">1.7321e+00</span>, -<span class="hljs-number">1.9319e+00</span>, -<span class="hljs-number">2.0000e+00</span>, -<span class="hljs-number">1.9319e+00</span>, -<span class="hljs-number">1.7321e+00</span>,<br>        -<span class="hljs-number">1.4142e+00</span>, -<span class="hljs-number">1.0000e+00</span>, -<span class="hljs-number">5.1764e-01</span>,  <span class="hljs-number">2.3850e-08</span>,  <span class="hljs-number">5.1764e-01</span>,<br>         <span class="hljs-number">1.0000e+00</span>,  <span class="hljs-number">1.4142e+00</span>,  <span class="hljs-number">1.7321e+00</span>,  <span class="hljs-number">1.9319e+00</span>,  <span class="hljs-number">2.0000e+00</span>])<br></code></pre></td></tr></table></figure><p>Be aware than only <em>leaf nodes</em> of the computation have their gradients computed. If you tried, for example, <code>print(c.grad)</code> you’d get back <code>None</code>. In this simple example, only the input is a leaf node, so only it has gradients computed.</p><h2 id="Jacobian"><a href="#Jacobian" class="headerlink" title="Jacobian"></a>Jacobian</h2><p>If you have a function with an <code>n-dimensional input</code> and <code>m-dimensional output</code>, , the complete gradient is a matrix of the derivative of <strong>every output with respect to every input</strong>, called the <em>Jacobian:</em><br>$$<br>\begin{align}J&#x3D;\left(\begin{array}{ccc}<br>   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\<br>   \vdots &amp; \ddots &amp; \vdots\<br>   \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>   \end{array}\right)\end{align}<br>$$<br>If you have a second function, $l&#x3D;g\left(\vec{y}\right)$ that takes <code>m-dimensional input</code> (that is, the same dimensionality as the output above), and returns a <code>scalar output</code>, you can <strong>express its gradients</strong> with respect to $\vec{y}$ as a column vector, $v&#x3D;\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}$ - which is really just a one-column Jacobian.</p><p>More concretely, imagine the first function as your <code>PyTorch model</code> (with potentially many inputs and many outputs) and the second function as a <code>loss function</code> (with the model’s output as input, and the loss value as the scalar output).</p><p>If we multiply the first function’s Jacobian by the gradient of the second function, and a<strong>pply the chain rule</strong>, we get:</p><p>$$<br>\begin{align}J^{T}\cdot v&#x3D;\left(\begin{array}{ccc}<br>   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{1}}\<br>   \vdots &amp; \ddots &amp; \vdots\<br>   \frac{\partial y_{1}}{\partial x_{n}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>   \end{array}\right)\left(\begin{array}{c}<br>   \frac{\partial l}{\partial y_{1}}\<br>   \vdots\<br>   \frac{\partial l}{\partial y_{m}}<br>   \end{array}\right)&#x3D;\left(\begin{array}{c}<br>   \frac{\partial l}{\partial x_{1}}\<br>   \vdots\<br>   \frac{\partial l}{\partial x_{n}}<br>   \end{array}\right)\end{align}<br>$$</p><p><em>Note: You could also use the equivalent operation $v^{T}\cdot J$, and get back a row vector.</em></p><p>The resulting column vector is the <em>gradient of the second function with respect to the inputs of the first</em> - or in the case of our model and loss function, <em>the gradient of the loss with respect to the model inputs</em>.</p><p><strong><code>torch.autograd</code> is an engine for computing these products.</strong> This is how we accumulate the gradients over the learning weights during the backward pass.</p><p>For this reason, the <code>backward()</code> call can <em>also</em> take an optional vector input. This vector represents a set of gradients over the tensor, which are multiplied by the Jacobian of the autograd-traced tensor that precedes it. Let’s try a specific example with a small vector:</p><p>嗯，这也就是后面的<strong>示例2</strong>所演示的，<a href="#AI%E6%80%BB%E7%BB%93##3.%E7%A4%BA%E4%BE%8B%E8%AF%A6%E8%A7%A3">戳我直达</a></p><h2 id="High-Level-API"><a href="#High-Level-API" class="headerlink" title="High-Level API"></a>High-Level API</h2><p>There is an API on autograd that gives you direct access to important differential matrix and vector operations. In particular, it allows you to calculate the <em>Jacobian</em> and the <em>Hessian</em> matrices of a particular function for particular inputs. (The Hessian is like the Jacobian, but expresses all partial <em>second</em> derivatives.) It also provides methods for taking vector products with these matrices.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">exp_adder</span>(<span class="hljs-params">x, y</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * x.exp() + <span class="hljs-number">3</span> * y<br><br>inputs = (torch.rand(<span class="hljs-number">1</span>), torch.rand(<span class="hljs-number">1</span>)) <span class="hljs-comment"># arguments for the function</span><br><span class="hljs-built_in">print</span>(inputs)<br>torch.autograd.functional.jacobian(exp_adder, inputs)<br>(tensor([<span class="hljs-number">0.7327</span>]), tensor([<span class="hljs-number">0.8072</span>]))<br>(tensor([[<span class="hljs-number">4.1616</span>]]), tensor([[<span class="hljs-number">3.</span>]]))<br><br><br>inputs = (torch.rand(<span class="hljs-number">3</span>), torch.rand(<span class="hljs-number">3</span>)) <span class="hljs-comment"># arguments for the function</span><br><span class="hljs-built_in">print</span>(inputs)<br>torch.autograd.functional.jacobian(exp_adder, inputs)<br>(tensor([<span class="hljs-number">0.3723</span>, <span class="hljs-number">0.7282</span>, <span class="hljs-number">0.0756</span>]), tensor([<span class="hljs-number">0.9602</span>, <span class="hljs-number">0.6986</span>, <span class="hljs-number">0.2260</span>]))<br><br>(tensor([[<span class="hljs-number">2.9022</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.0000</span>, <span class="hljs-number">4.1428</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">2.1570</span>]]),<br> tensor([[<span class="hljs-number">3.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>         [<span class="hljs-number">0.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">0.</span>],<br>         [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">3.</span>]]))<br></code></pre></td></tr></table></figure><ul><li>还有能够直接计算向量-雅各比乘积的function：The <code>torch.autograd.functional.jvp()</code> method performs the same matrix multiplication as <code>vjp()</code> with the operands reversed</li></ul><hr><ul><li>全文重点感觉不够突出，我让AI给了一份笔记</li></ul><h1 id="AI总结"><a href="#AI总结" class="headerlink" title="AI总结"></a>AI总结</h1><h2 id="PyTorch-Autograd-功能总结"><a href="#PyTorch-Autograd-功能总结" class="headerlink" title="PyTorch Autograd 功能总结"></a><strong>PyTorch Autograd 功能总结</strong></h2><p>PyTorch 的 <code>autograd</code>（自动微分）是其核心功能之一，用于 <strong>自动计算张量的梯度</strong>，支持动态计算图（Dynamic Computation Graph），是训练神经网络的基础。以下是关键点总结和示例：</p><hr><h2 id="1-Autograd-的核心机制"><a href="#1-Autograd-的核心机制" class="headerlink" title="1. Autograd 的核心机制"></a><strong>1. Autograd 的核心机制</strong></h2><table><thead><tr><th>功能</th><th>说明</th></tr></thead><tbody><tr><td><strong>动态计算图</strong></td><td>在运行时构建计算图，灵活支持控制流（如循环、条件语句）。</td></tr><tr><td><strong>自动梯度计算</strong></td><td>通过 <code>backward()</code> 自动计算梯度，存储在 <code>.grad</code> 属性中。</td></tr><tr><td><strong>叶子张量</strong></td><td>直接创建的张量（如 <code>torch.tensor()</code>）是叶子节点，可累积梯度。</td></tr><tr><td><strong>非叶子张量</strong></td><td>由运算产生的张量，默认不保留梯度（除非显式设置 <code>retain_grad()</code>）。</td></tr></tbody></table><hr><h2 id="2-关键函数与方法"><a href="#2-关键函数与方法" class="headerlink" title="2. 关键函数与方法"></a><strong>2. 关键函数与方法</strong></h2><table><thead><tr><th>方法&#x2F;类</th><th>作用</th><th>示例</th></tr></thead><tbody><tr><td><code>requires_grad=True</code></td><td>启用梯度追踪</td><td><code>x = torch.tensor(1.0, requires_grad=True)</code></td></tr><tr><td><code>backward()</code></td><td>反向传播计算梯度</td><td><code>y.backward()</code></td></tr><tr><td><code>grad</code> 属性</td><td>存储梯度值</td><td><code>x.grad</code></td></tr><tr><td><code>torch.no_grad()</code></td><td>临时禁用梯度计算</td><td><code>with torch.no_grad():</code></td></tr><tr><td><code>detach()</code></td><td>分离张量，阻止梯度传播</td><td><code>y_detached = y.detach()</code></td></tr><tr><td><code>torch.autograd.grad()</code></td><td>直接计算梯度（不修改 <code>.grad</code>）</td><td><code>grad = torch.autograd.grad(y, x)</code></td></tr></tbody></table><hr><h2 id="3-示例详解"><a href="#3-示例详解" class="headerlink" title="3. 示例详解"></a><strong>3. 示例详解</strong></h2><h3 id="示例-1：基本梯度计算"><a href="#示例-1：基本梯度计算" class="headerlink" title="示例 1：基本梯度计算"></a><strong>示例 1：基本梯度计算</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 定义叶子张量（启用梯度）</span><br>x = torch.tensor(<span class="hljs-number">2.0</span>, requires_grad=<span class="hljs-literal">True</span>)<br>y = x ** <span class="hljs-number">2</span> + <span class="hljs-number">3</span> * x  <span class="hljs-comment"># y = x² + 3x</span><br><br><span class="hljs-comment"># 计算梯度</span><br>y.backward()  <span class="hljs-comment"># dy/dx = 2x + 3</span><br><br><span class="hljs-built_in">print</span>(x.grad)  <span class="hljs-comment"># 输出: tensor(7.0) （x=2时，dy/dx=2*2+3=7）</span><br></code></pre></td></tr></table></figure><h3 id="示例-2：非标量输出的梯度（需指定-gradient-参数）"><a href="#示例-2：非标量输出的梯度（需指定-gradient-参数）" class="headerlink" title="示例 2：非标量输出的梯度（需指定 gradient 参数）"></a><strong>示例 2：非标量输出的梯度（需指定 <code>gradient</code> 参数）</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>], requires_grad=<span class="hljs-literal">True</span>)<br>y = x * <span class="hljs-number">2</span>  <span class="hljs-comment"># y = [2.0, 4.0]</span><br><br><span class="hljs-comment"># 定义梯度权重（模拟损失对各分量的敏感度）</span><br>gradient = torch.tensor([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.01</span>])<br>y.backward(gradient)  <span class="hljs-comment"># 等价于加权求和后的反向传播</span><br><br><span class="hljs-built_in">print</span>(x.grad)  <span class="hljs-comment"># 输出: tensor([0.2000, 0.0200]) （2*0.1=0.2, 2*0.01=0.02）</span><br></code></pre></td></tr></table></figure><h3 id="示例-3：冻结部分参数（detach-或-requires-grad-False）"><a href="#示例-3：冻结部分参数（detach-或-requires-grad-False）" class="headerlink" title="示例 3：冻结部分参数（detach 或 requires_grad=False）"></a><strong>示例 3：冻结部分参数（<code>detach</code> 或 <code>requires_grad=False</code>）</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 场景：冻结模型的一部分参数</span><br>model = torch.nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():<br>    param.requires_grad = <span class="hljs-literal">False</span>  <span class="hljs-comment"># 冻结所有参数</span><br><br><span class="hljs-comment"># 或者分离中间张量</span><br>x = torch.tensor(<span class="hljs-number">1.0</span>, requires_grad=<span class="hljs-literal">True</span>)<br>y = x * <span class="hljs-number">2</span><br>z = y.detach()  <span class="hljs-comment"># z 不参与梯度计算</span><br></code></pre></td></tr></table></figure><h3 id="示例-4：高阶导数（create-graph-True）"><a href="#示例-4：高阶导数（create-graph-True）" class="headerlink" title="示例 4：高阶导数（create_graph=True）"></a><strong>示例 4：高阶导数（<code>create_graph=True</code>）</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor(<span class="hljs-number">3.0</span>, requires_grad=<span class="hljs-literal">True</span>)<br>y = x ** <span class="hljs-number">3</span>  <span class="hljs-comment"># y = x³</span><br><br><span class="hljs-comment"># 一阶导数</span><br>grad1 = torch.autograd.grad(y, x, create_graph=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># dy/dx = 3x² → 27</span><br><span class="hljs-comment"># 二阶导数</span><br>grad2 = torch.autograd.grad(grad1, x)               <span class="hljs-comment"># d²y/dx² = 6x → 18</span><br><span class="hljs-built_in">print</span>(grad2[<span class="hljs-number">0</span>])  <span class="hljs-comment"># 输出: tensor(18.0)</span><br></code></pre></td></tr></table></figure><h3 id="示例-5：梯度清零与累加"><a href="#示例-5：梯度清零与累加" class="headerlink" title="示例 5：梯度清零与累加"></a><strong>示例 5：梯度清零与累加</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor(<span class="hljs-number">1.0</span>, requires_grad=<span class="hljs-literal">True</span>)<br>y = x ** <span class="hljs-number">2</span><br><br><span class="hljs-comment"># 第一次反向传播</span><br>y.backward()<br><span class="hljs-built_in">print</span>(x.grad)  <span class="hljs-comment"># 输出: tensor(2.0) （dy/dx=2x）</span><br><br><span class="hljs-comment"># 梯度会累加！</span><br>y.backward()<br><span class="hljs-built_in">print</span>(x.grad)  <span class="hljs-comment"># 输出: tensor(4.0) （2.0 + 2.0）</span><br><br><span class="hljs-comment"># 手动清零梯度</span><br>x.grad.zero_()<br></code></pre></td></tr></table></figure><hr><h2 id="4-Autograd-的注意事项"><a href="#4-Autograd-的注意事项" class="headerlink" title="4. Autograd 的注意事项"></a><strong>4. Autograd 的注意事项</strong></h2><ol><li><p><strong>内存管理</strong>：</p><ul><li>默认情况下，<code>backward()</code> 会释放计算图。若需多次反向传播，需设置 <code>retain_graph=True</code>。</li><li>示例：<code>loss.backward(retain_graph=True)</code>。</li></ul></li><li><p><strong>性能优化</strong>：</p><ul><li>推理时用 <code>torch.no_grad()</code> 禁用梯度以节省内存：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> torch.no_grad():<br>    output = model(<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>梯度截断</strong>：</p><ul><li>防止梯度爆炸：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="hljs-number">1.0</span>)<br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>非叶子张量的梯度</strong>：</p><ul><li>默认不保留，如需保留需调用 <code>retain_grad()</code>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">y = x * <span class="hljs-number">2</span><br>y.retain_grad()  <span class="hljs-comment"># 保留 y 的梯度</span><br></code></pre></td></tr></table></figure></li></ul></li></ol><hr><h2 id="5-动态计算图的优势"><a href="#5-动态计算图的优势" class="headerlink" title="5. 动态计算图的优势"></a><strong>5. 动态计算图的优势</strong></h2><ul><li><p><strong>支持控制流</strong>：计算图在运行时动态构建，可包含 <code>if</code>、<code>for</code> 等语句。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor(<span class="hljs-number">1.0</span>, requires_grad=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">if</span> x &gt; <span class="hljs-number">0</span>:<br>    y = x ** <span class="hljs-number">2</span><br><span class="hljs-keyword">else</span>:<br>    y = x ** <span class="hljs-number">3</span><br>y.backward()  <span class="hljs-comment"># 自动处理分支</span><br></code></pre></td></tr></table></figure></li><li><p><strong>灵活性</strong>：适用于变长输入（如RNN）或复杂架构（如动态网络）。</p></li></ul><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><table><thead><tr><th>场景</th><th>操作</th><th>代码示例</th></tr></thead><tbody><tr><td><strong>基本梯度计算</strong></td><td><code>backward()</code></td><td><code>y.backward()</code></td></tr><tr><td><strong>非标量输出</strong></td><td><code>backward(gradient=...)</code></td><td><code>y.backward(torch.tensor([0.1, 0.01]))</code></td></tr><tr><td><strong>冻结参数</strong></td><td><code>detach()</code> 或 <code>requires_grad=False</code></td><td><code>param.detach()</code></td></tr><tr><td><strong>高阶导数</strong></td><td><code>create_graph=True</code></td><td><code>torch.autograd.grad(..., create_graph=True)</code></td></tr><tr><td><strong>避免梯度累加</strong></td><td><code>zero_grad()</code></td><td><code>optimizer.zero_grad()</code></td></tr></tbody></table><p>PyTorch 的 <code>autograd</code> 通过动态计算图和自动微分，极大简化了梯度计算过程，是深度学习模型训练的核心工具。</p>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
      <tag>Python</tag>
      
      <tag>ML</tag>
      
      <tag>Autograd</tag>
      
      <tag>Tensor</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch Tutorial-Tensors</title>
    <link href="/2025/08/03/Tensors/"/>
    <url>/2025/08/03/Tensors/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我学习Pytorch时记录的一些笔记 ，希望能对你有所帮助😊 </p>          </div><ul><li>You will sometimes see a 1-dimensional tensor called a <em>vector.</em></li><li>Likewise, a 2-dimensional tensor is often referred to as a <em>matrix.</em></li><li>Anything with more than two dimensions is generally just called a tensor</li></ul><h1 id="Creating-Tensors"><a href="#Creating-Tensors" class="headerlink" title="Creating Tensors"></a>Creating Tensors</h1><ul><li>using factory methods<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(x)) <span class="hljs-comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><br>zeros = torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(zeros)<br><br>ones = torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(ones)<br><br>torch.manual_seed(<span class="hljs-number">1729</span>)<br>random = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(random)<br><br><span class="hljs-comment"># The output</span><br>tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]])<br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br>tensor([[<span class="hljs-number">0.3126</span>, <span class="hljs-number">0.3791</span>, <span class="hljs-number">0.3087</span>],<br>        [<span class="hljs-number">0.0736</span>, <span class="hljs-number">0.4216</span>, <span class="hljs-number">0.0691</span>]])<br></code></pre></td></tr></table></figure></li></ul><h2 id="Random-Tensors-and-Seeding"><a href="#Random-Tensors-and-Seeding" class="headerlink" title="Random Tensors and Seeding"></a>Random Tensors and Seeding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">1729</span>)<br>random1 = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(random1)<br><br>random2 = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(random2)<br><br>torch.manual_seed(<span class="hljs-number">1729</span>)<br>random3 = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(random3)<br><br>random4 = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(random4)<br><br>tensor([[<span class="hljs-number">0.3126</span>, <span class="hljs-number">0.3791</span>, <span class="hljs-number">0.3087</span>],<br>        [<span class="hljs-number">0.0736</span>, <span class="hljs-number">0.4216</span>, <span class="hljs-number">0.0691</span>]])<br>tensor([[<span class="hljs-number">0.2332</span>, <span class="hljs-number">0.4047</span>, <span class="hljs-number">0.2162</span>],<br>        [<span class="hljs-number">0.9927</span>, <span class="hljs-number">0.4128</span>, <span class="hljs-number">0.5938</span>]])<br>tensor([[<span class="hljs-number">0.3126</span>, <span class="hljs-number">0.3791</span>, <span class="hljs-number">0.3087</span>],<br>        [<span class="hljs-number">0.0736</span>, <span class="hljs-number">0.4216</span>, <span class="hljs-number">0.0691</span>]])<br>tensor([[<span class="hljs-number">0.2332</span>, <span class="hljs-number">0.4047</span>, <span class="hljs-number">0.2162</span>],<br>        [<span class="hljs-number">0.9927</span>, <span class="hljs-number">0.4128</span>, <span class="hljs-number">0.5938</span>]])<br></code></pre></td></tr></table></figure><ul><li>可以发现random1,2 与 3,4完全一致，随机数生成器的种子都是1729，这样可以保证“结果可复现”</li><li>r1与r2的值不同，因为生成r1后随机数生成器的状态更新了，而r3相当于重置了，类似于execve</li></ul><h2 id="Tensor-Shapes"><a href="#Tensor-Shapes" class="headerlink" title="Tensor Shapes"></a>Tensor Shapes</h2><ul><li> when you’re performing operations on two or more tensors, they will <strong>need to be of the same <em>shape</em></strong> - that is, having the same number of dimensions and the same number of cells in each dimension. For that, we have the <code>torch.*_like()</code> methods:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.empty(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(x.shape)<br><span class="hljs-built_in">print</span>(x)<br><br>empty_like_x = torch.empty_like(x)<br><span class="hljs-built_in">print</span>(empty_like_x.shape)<br><span class="hljs-built_in">print</span>(empty_like_x)<br><br>zeros_like_x = torch.zeros_like(x)<br><span class="hljs-built_in">print</span>(zeros_like_x.shape)<br><span class="hljs-built_in">print</span>(zeros_like_x)<br><br>rand_like_x = torch.rand_like(x)<br><span class="hljs-built_in">print</span>(rand_like_x.shape)<br><span class="hljs-built_in">print</span>(rand_like_x)<br><br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>tensor([[[ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">1.0842e-19</span>,  <span class="hljs-number">4.9628e-26</span>],<br>         [-<span class="hljs-number">2.5250e-29</span>,  <span class="hljs-number">9.8091e-45</span>,  <span class="hljs-number">0.0000e+00</span>]],<br><br>        [[ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>],<br>         [ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>]]])<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>tensor([[[ <span class="hljs-number">2.5776e-33</span>,  <span class="hljs-number">1.4013e-45</span>,         nan],<br>         [ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">1.4013e-45</span>,  <span class="hljs-number">0.0000e+00</span>]],<br><br>        [[ <span class="hljs-number">4.9477e-26</span>, -<span class="hljs-number">3.6902e+19</span>,  <span class="hljs-number">2.6082e-33</span>],<br>         [ <span class="hljs-number">1.4013e-45</span>,  <span class="hljs-number">4.9633e-26</span>, -<span class="hljs-number">8.5920e+09</span>]]])<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>tensor([[[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>         [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]],<br><br>        [[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>         [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]]])<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>tensor([[[<span class="hljs-number">0.6128</span>, <span class="hljs-number">0.1519</span>, <span class="hljs-number">0.0453</span>],<br>         [<span class="hljs-number">0.5035</span>, <span class="hljs-number">0.9978</span>, <span class="hljs-number">0.3884</span>]],<br><br>        [[<span class="hljs-number">0.6929</span>, <span class="hljs-number">0.1703</span>, <span class="hljs-number">0.1384</span>],<br>         [<span class="hljs-number">0.4759</span>, <span class="hljs-number">0.7481</span>, <span class="hljs-number">0.0361</span>]]])<br></code></pre></td></tr></table></figure><ul><li>Below that, we call the <code>.empty_like()</code>, <code>.zeros_like()</code>,  and <code>.rand_like()</code> methods. Using the <code>.shape</code> property, we can verify that <strong>each of these methods returns a tensor of identical dimensionality and extent</strong>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">some_integers = torch.tensor((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">13</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>))<br><span class="hljs-built_in">print</span>(some_integers)<br><br>more_integers = torch.tensor(((<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>), [<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>]))<br><span class="hljs-built_in">print</span>(more_integers)<br><br>tensor([ <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">13</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>])<br>tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>],<br>        [<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>]])<br></code></pre></td></tr></table></figure><ul><li>Using <code>torch.tensor()</code> is the most straightforward way to create a tensor if you already have data in a Python tuple or list. As shown above, nesting the collections will result in a multi-dimensional tensor.(嵌套集合将产生多维张量)<br><em>Note: <code>torch.tensor()</code> creates a copy of the data.</em></li></ul><h2 id="Tensor-Data-Types"><a href="#Tensor-Data-Types" class="headerlink" title="Tensor Data Types"></a>Tensor Data Types</h2><p>默认是<code>float32,</code> Available data types include:  </p><ul><li><code>torch.bool</code></li><li><code>torch.int8</code></li><li><code>torch.uint8</code></li><li><code>torch.int16</code></li><li><code>torch.int32</code></li><li><code>torch.int64</code></li><li><code>torch.half</code></li><li><code>torch.float</code></li><li><code>torch.double</code></li><li><code>torch.bfloat</code></li></ul><p>设置数据类型有两种方式，一是在创建时设置，二是使用<code>.to()</code>函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=torch.int16)<br><span class="hljs-built_in">print</span>(a)<br><br>b = torch.rand((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=torch.float64) * <span class="hljs-number">20.</span><br><span class="hljs-built_in">print</span>(b)<br><br>c = b.to(torch.int32)<br><span class="hljs-built_in">print</span>(c)<br><br>tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.int16) <span class="hljs-comment"># 这里如果不指定数据类型会显示 1.</span><br>tensor([[ <span class="hljs-number">0.9956</span>,  <span class="hljs-number">1.4148</span>,  <span class="hljs-number">5.8364</span>],<br>        [<span class="hljs-number">11.2406</span>, <span class="hljs-number">11.2083</span>, <span class="hljs-number">11.6692</span>]], dtype=torch.float64)<br>tensor([[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">5</span>],<br>        [<span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>]], dtype=torch.int32)<br></code></pre></td></tr></table></figure><h1 id="Math-Logic-with-Tensors"><a href="#Math-Logic-with-Tensors" class="headerlink" title="Math &amp; Logic with Tensors"></a>Math &amp; Logic with Tensors</h1><ul><li>大部分运算都和你的直觉差不多，而且比较“聪明”<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">ones = torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) + <span class="hljs-number">1</span><br>twos = torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) * <span class="hljs-number">2</span><br>threes = (torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) * <span class="hljs-number">7</span> - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span><br>fours = twos ** <span class="hljs-number">2</span><br>sqrt2s = twos ** <span class="hljs-number">0.5</span><br><br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br>tensor([[<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>],<br>        [<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>]])<br>tensor([[<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>],<br>        [<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>]])<br>tensor([[<span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>],<br>        [<span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>]])<br>tensor([[<span class="hljs-number">1.4142</span>, <span class="hljs-number">1.4142</span>],<br>        [<span class="hljs-number">1.4142</span>, <span class="hljs-number">1.4142</span>]])<br>        <br>powers2 = twos ** torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-built_in">print</span>(powers2)<br><br>fives = ones + fours<br><span class="hljs-built_in">print</span>(fives)<br><br>dozens = threes * fours<br><span class="hljs-built_in">print</span>(dozens)<br>tensor([[ <span class="hljs-number">2.</span>,  <span class="hljs-number">4.</span>],<br>        [ <span class="hljs-number">8.</span>, <span class="hljs-number">16.</span>]])<br>tensor([[<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>],<br>        [<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>]])<br>tensor([[<span class="hljs-number">12.</span>, <span class="hljs-number">12.</span>],<br>        [<span class="hljs-number">12.</span>, <span class="hljs-number">12.</span>]])<br></code></pre></td></tr></table></figure></li></ul><h2 id="In-Brief-Tensor-Broadcasting"><a href="#In-Brief-Tensor-Broadcasting" class="headerlink" title="In Brief: Tensor Broadcasting"></a>In Brief: Tensor Broadcasting</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">rand = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>)<br>doubled = rand * (torch.ones(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>) * <span class="hljs-number">2</span>)<br><br><span class="hljs-built_in">print</span>(rand)<br><span class="hljs-built_in">print</span>(doubled)<br><br>tensor([[<span class="hljs-number">0.2024</span>, <span class="hljs-number">0.5731</span>, <span class="hljs-number">0.7191</span>, <span class="hljs-number">0.4067</span>],<br>        [<span class="hljs-number">0.7301</span>, <span class="hljs-number">0.6276</span>, <span class="hljs-number">0.7357</span>, <span class="hljs-number">0.0381</span>]])<br>tensor([[<span class="hljs-number">0.4049</span>, <span class="hljs-number">1.1461</span>, <span class="hljs-number">1.4382</span>, <span class="hljs-number">0.8134</span>],<br>        [<span class="hljs-number">1.4602</span>, <span class="hljs-number">1.2551</span>, <span class="hljs-number">1.4715</span>, <span class="hljs-number">0.0762</span>]])<br></code></pre></td></tr></table></figure><p><strong>The rules for broadcasting are:</strong></p><ul><li>Each tensor must have at least one dimension - no empty tensors.</li><li>Comparing the dimension sizes of the two tensors, <em>going from last to first 从后往前:</em><ul><li>Each dimension must be equal, <em>or</em></li><li>One of the dimensions must be of size 1, <em>or</em></li><li>The dimension does not exist in one of the tensors</li></ul></li><li>some correct examples:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">a =     torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br><br>b = a * torch.rand(   <span class="hljs-number">3</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># 3rd &amp; 2nd dims identical to a, dim 1 absent</span><br><span class="hljs-built_in">print</span>(b)<br><br>c = a * torch.rand(   <span class="hljs-number">3</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># 3rd dim = 1, 2nd dim identical to a</span><br><span class="hljs-built_in">print</span>(c)<br><br>d = a * torch.rand(   <span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># 3rd dim identical to a, 2nd dim = 1</span><br><span class="hljs-built_in">print</span>(d)<br></code></pre></td></tr></table></figure>For more information on broadcasting, see the <a href="https://pytorch.org/docs/stable/notes/broadcasting.html">PyTorch documentation</a> on the topic.</li></ul><h2 id="More-Math-with-Tensors"><a href="#More-Math-with-Tensors" class="headerlink" title="More Math with Tensors"></a>More Math with Tensors</h2><p>This is a small sample of For more details and the full inventory of math functions, have a look at the <a href="https://pytorch.org/docs/stable/torch.html#math-operations">documentation</a>.<br>给出一些经典示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># common functions</span><br>a = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>) * <span class="hljs-number">2</span> - <span class="hljs-number">1</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Common functions:&#x27;</span>)<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">abs</span>(a))<br><span class="hljs-built_in">print</span>(torch.ceil(a))<br><span class="hljs-built_in">print</span>(torch.floor(a))<br><span class="hljs-built_in">print</span>(torch.clamp(a, -<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>)) <span class="hljs-comment"># `torch.clamp()` 是一个张量裁剪函数，用于限制张量 `a` 中的元素值在指定的范围内 `[min, max]`</span><br><br><span class="hljs-comment"># trigonometric functions and their inverses</span><br>angles = torch.tensor([<span class="hljs-number">0</span>, math.pi / <span class="hljs-number">4</span>, math.pi / <span class="hljs-number">2</span>, <span class="hljs-number">3</span> * math.pi / <span class="hljs-number">4</span>])<br>sines = torch.sin(angles)<br>inverses = torch.asin(sines)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nSine and arcsine:&#x27;</span>)<br><span class="hljs-built_in">print</span>(angles)<br><span class="hljs-built_in">print</span>(sines)<br><span class="hljs-built_in">print</span>(inverses)<br><br><span class="hljs-comment"># bitwise operations</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nBitwise XOR:&#x27;</span>)<br>b = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">11</span>])<br>c = torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">7</span>, <span class="hljs-number">10</span>])<br><span class="hljs-built_in">print</span>(torch.bitwise_xor(b, c))<br><br><span class="hljs-comment"># comparisons:</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nBroadcasted, element-wise equality comparison:&#x27;</span>)<br>d = torch.tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>], [<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>]])<br>e = torch.ones(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># many comparison ops support broadcasting!</span><br><span class="hljs-built_in">print</span>(torch.eq(d, e)) <span class="hljs-comment"># returns a tensor of type bool</span><br><br><span class="hljs-comment"># reductions:</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nReduction ops:&#x27;</span>)<br><span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">max</span>(d))        <span class="hljs-comment"># returns a single-element tensor</span><br><span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">max</span>(d).item()) <span class="hljs-comment"># extracts the value from the returned tensor</span><br><span class="hljs-built_in">print</span>(torch.mean(d))       <span class="hljs-comment"># average</span><br><span class="hljs-built_in">print</span>(torch.std(d))        <span class="hljs-comment"># standard deviation 标准差</span><br><span class="hljs-built_in">print</span>(torch.prod(d))       <span class="hljs-comment"># product of all numbers 所有元素相乘</span><br><span class="hljs-built_in">print</span>(torch.unique(torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]))) <span class="hljs-comment"># filter unique elements</span><br><br><span class="hljs-comment"># vector and linear algebra operations</span><br>v1 = torch.tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>])         <span class="hljs-comment"># x unit vector</span><br>v2 = torch.tensor([<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>])         <span class="hljs-comment"># y unit vector</span><br>m1 = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)                   <span class="hljs-comment"># random matrix</span><br>m2 = torch.tensor([[<span class="hljs-number">3.</span>, <span class="hljs-number">0.</span>], [<span class="hljs-number">0.</span>, <span class="hljs-number">3.</span>]]) <span class="hljs-comment"># three times identity matrix</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nVectors &amp; Matrices:&#x27;</span>)<br><span class="hljs-built_in">print</span>(torch.cross(v2, v1)) <span class="hljs-comment"># negative of z unit vector 叉乘</span><br><span class="hljs-built_in">print</span>(m1)<br>m3 = torch.matmul(m1, m2)  <span class="hljs-comment"># mm: matrix multiplication</span><br><span class="hljs-built_in">print</span>(m3)                  <span class="hljs-comment"># 3 times m1</span><br><span class="hljs-built_in">print</span>(torch.svd(m3))       <span class="hljs-comment"># singular value decomposition,奇异值分解</span><br><br><br>Common functions:<br>tensor([[ <span class="hljs-number">0.2719</span>,  <span class="hljs-number">0.1760</span>,  <span class="hljs-number">0.8550</span>,  <span class="hljs-number">0.0404</span>],<br>        [-<span class="hljs-number">0.5709</span>,  <span class="hljs-number">0.4227</span>, -<span class="hljs-number">0.9504</span>, -<span class="hljs-number">0.8692</span>]])<br>tensor([[<span class="hljs-number">0.2719</span>, <span class="hljs-number">0.1760</span>, <span class="hljs-number">0.8550</span>, <span class="hljs-number">0.0404</span>],<br>        [<span class="hljs-number">0.5709</span>, <span class="hljs-number">0.4227</span>, <span class="hljs-number">0.9504</span>, <span class="hljs-number">0.8692</span>]])<br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [-<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, -<span class="hljs-number">0.</span>, -<span class="hljs-number">0.</span>]])<br>tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>],<br>        [-<span class="hljs-number">1.</span>,  <span class="hljs-number">0.</span>, -<span class="hljs-number">1.</span>, -<span class="hljs-number">1.</span>]])<br>tensor([[ <span class="hljs-number">0.2719</span>,  <span class="hljs-number">0.1760</span>,  <span class="hljs-number">0.5000</span>,  <span class="hljs-number">0.0404</span>],<br>        [-<span class="hljs-number">0.5000</span>,  <span class="hljs-number">0.4227</span>, -<span class="hljs-number">0.5000</span>, -<span class="hljs-number">0.5000</span>]])<br><br>Sine <span class="hljs-keyword">and</span> arcsine:<br>tensor([<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.7854</span>, <span class="hljs-number">1.5708</span>, <span class="hljs-number">2.3562</span>])<br>tensor([<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.7071</span>, <span class="hljs-number">1.0000</span>, <span class="hljs-number">0.7071</span>]) <span class="hljs-comment"># x比较小的时候和sinx差不多</span><br>tensor([<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.7854</span>, <span class="hljs-number">1.5708</span>, <span class="hljs-number">0.7854</span>])<br><br>Bitwise XOR:<br>tensor([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br><br>Broadcasted, element-wise equality comparison:<br>tensor([[ <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>],<br>        [<span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>]])<br><br>Reduction ops:<br>tensor(<span class="hljs-number">4.</span>)<br><span class="hljs-number">4.0</span><br>tensor(<span class="hljs-number">2.5000</span>)<br>tensor(<span class="hljs-number">1.2910</span>)<br>tensor(<span class="hljs-number">24.</span>)<br>tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><br>Vectors &amp; Matrices:<br>tensor([ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>, -<span class="hljs-number">1.</span>])<br>tensor([[<span class="hljs-number">0.8629</span>, <span class="hljs-number">0.6615</span>],<br>        [<span class="hljs-number">0.8881</span>, <span class="hljs-number">0.5365</span>]])<br>tensor([[<span class="hljs-number">2.5888</span>, <span class="hljs-number">1.9844</span>],<br>        [<span class="hljs-number">2.6643</span>, <span class="hljs-number">1.6095</span>]])<br>torch.return_types.svd(<br>U=tensor([[-<span class="hljs-number">0.7236</span>, -<span class="hljs-number">0.6903</span>],<br>        [-<span class="hljs-number">0.6903</span>,  <span class="hljs-number">0.7236</span>]]),<br>S=tensor([<span class="hljs-number">4.5019</span>, <span class="hljs-number">0.2488</span>]),<br>V=tensor([[-<span class="hljs-number">0.8246</span>,  <span class="hljs-number">0.5657</span>],<br>        [-<span class="hljs-number">0.5657</span>, -<span class="hljs-number">0.8246</span>]]))<br></code></pre></td></tr></table></figure><h1 id="Copying-Tensors"><a href="#Copying-Tensors" class="headerlink" title="Copying Tensors"></a>Copying Tensors</h1><ul><li>clone()<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>b = a.clone()<br><br><span class="hljs-keyword">assert</span> b <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> a      <span class="hljs-comment"># different objects in memory...</span><br><span class="hljs-built_in">print</span>(torch.eq(a, b))  <span class="hljs-comment"># ...but still with the same contents!</span><br><br>a[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] = <span class="hljs-number">561</span>          <span class="hljs-comment"># a changes...</span><br><span class="hljs-built_in">print</span>(b)               <span class="hljs-comment"># ...but b is still all ones</span><br></code></pre></td></tr></table></figure></li><li><strong>There is an important thing to be aware of when using <code>clone()</code>.</strong> If your source tensor has autograd, enabled then so will the clone. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, requires_grad=<span class="hljs-literal">True</span>) <span class="hljs-comment"># turn on autograd</span><br><span class="hljs-built_in">print</span>(a)<br><br>b = a.clone()<br><span class="hljs-built_in">print</span>(b)<br><br>c = a.detach().clone()<br><span class="hljs-built_in">print</span>(c)<br><br><span class="hljs-built_in">print</span>(a)<br><br>tensor([[<span class="hljs-number">0.5461</span>, <span class="hljs-number">0.5396</span>],<br>        [<span class="hljs-number">0.3053</span>, <span class="hljs-number">0.1973</span>]], requires_grad=<span class="hljs-literal">True</span>)<br>tensor([[<span class="hljs-number">0.5461</span>, <span class="hljs-number">0.5396</span>],<br>        [<span class="hljs-number">0.3053</span>, <span class="hljs-number">0.1973</span>]], grad_fn=&lt;CloneBackward&gt;)<br>tensor([[<span class="hljs-number">0.5461</span>, <span class="hljs-number">0.5396</span>],<br>        [<span class="hljs-number">0.3053</span>, <span class="hljs-number">0.1973</span>]])<br>tensor([[<span class="hljs-number">0.5461</span>, <span class="hljs-number">0.5396</span>],<br>        [<span class="hljs-number">0.3053</span>, <span class="hljs-number">0.1973</span>]], requires_grad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure></li><li>The <code>detach()</code> method <em>detaches the tensor from its computation history.</em> It says, “do whatever comes next as if autograd was off.” It does this <em>without</em> changing <code>a</code> - you can see that when we print <code>a</code> again at the end, it retains its <code>requires_grad=True</code> property.</li></ul><h1 id="Moving-to-GPU"><a href="#Moving-to-GPU" class="headerlink" title="Moving to GPU"></a>Moving to GPU</h1><p>引入：One of the major advantages of PyTorch is its robust acceleration on CUDA-compatible Nvidia GPUs. (“CUDA” stands for <em>Compute Unified Device Architecture</em>, which is Nvidia’s platform for parallel computing.) So far, everything we’ve done has been on CPU. How do we move to the faster hardware?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;We have a GPU!&#x27;</span>)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Sorry, CPU only.&#x27;</span>)<br></code></pre></td></tr></table></figure><p>CPU在RAM中进行计算，而GPU提供了附加内存，用GPU计算时需要将数据迁移到GPU</p><h2 id="创建时迁移"><a href="#创建时迁移" class="headerlink" title="创建时迁移"></a>创建时迁移</h2><ul><li><p>张量默认创建在CPU上，你可以使用<code>device</code>来制定设备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    gpu_rand = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(gpu_rand)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Sorry, CPU only.&#x27;</span>)<br></code></pre></td></tr></table></figure></li><li><p>一个好的习惯是使用<code>my_device</code>来表征自己的设备，而不直接使用字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    my_device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br><span class="hljs-keyword">else</span>:<br>    my_device = torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Device: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(my_device))<br></code></pre></td></tr></table></figure></li></ul><h2 id="to-迁移"><a href="#to-迁移" class="headerlink" title="to()迁移"></a>to()迁移</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">y = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>y = y.to(my_device)<br></code></pre></td></tr></table></figure><p><strong>It is important to know that</strong> in order to do computation involving two or more tensors, <em>all of the tensors must be on the same device</em>.</p><h1 id="Manipulating-Tensor-Shapes"><a href="#Manipulating-Tensor-Shapes" class="headerlink" title="Manipulating Tensor Shapes"></a>Manipulating Tensor Shapes</h1><p>PyTorch models generally expect <em>batches</em> of input.For example, imagine having a model that works on 3 x 226 x 226 images - a 226-pixel square with 3 color channels. When you load and transform it, you’ll get a tensor of shape <code>(3, 226, 226)</code>. Your model, though, is expecting input of shape <code>(N, 3, 226, 226)</code>, where <code>N</code> is the number of images in the batch. So how do you make a batch of one?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.rand(<span class="hljs-number">3</span>, <span class="hljs-number">226</span>, <span class="hljs-number">226</span>)<br>b = a.unsqueeze(<span class="hljs-number">0</span>)<br><br><span class="hljs-built_in">print</span>(a.shape)<br><span class="hljs-built_in">print</span>(b.shape)<br><br>torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">226</span>, <span class="hljs-number">226</span>])<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">226</span>, <span class="hljs-number">226</span>])<br></code></pre></td></tr></table></figure><ul><li>The <code>unsqueeze()</code> method adds a dimension of extent 1. <code>unsqueeze(0)</code> adds it as a new zeroth dimension - now you have a batch of one!</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>)<br><span class="hljs-built_in">print</span>(a.shape)<br><span class="hljs-built_in">print</span>(a)<br><br>b = a.squeeze(<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(b.shape)<br><span class="hljs-built_in">print</span>(b)<br><br>c = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(c.shape)<br><br>d = c.squeeze(<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(d.shape)<br><br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">20</span>])<br>tensor([[<span class="hljs-number">0.4887</span>, <span class="hljs-number">0.8625</span>, <span class="hljs-number">0.6191</span>, <span class="hljs-number">0.9935</span>, <span class="hljs-number">0.1844</span>, <span class="hljs-number">0.6138</span>, <span class="hljs-number">0.6854</span>, <span class="hljs-number">0.0438</span>, <span class="hljs-number">0.0636</span>,<br>         <span class="hljs-number">0.2884</span>, <span class="hljs-number">0.4362</span>, <span class="hljs-number">0.2368</span>, <span class="hljs-number">0.1394</span>, <span class="hljs-number">0.1721</span>, <span class="hljs-number">0.1751</span>, <span class="hljs-number">0.3851</span>, <span class="hljs-number">0.0732</span>, <span class="hljs-number">0.3118</span>,<br>         <span class="hljs-number">0.9180</span>, <span class="hljs-number">0.7293</span>]])<br>torch.Size([<span class="hljs-number">20</span>])<br>tensor([<span class="hljs-number">0.4887</span>, <span class="hljs-number">0.8625</span>, <span class="hljs-number">0.6191</span>, <span class="hljs-number">0.9935</span>, <span class="hljs-number">0.1844</span>, <span class="hljs-number">0.6138</span>, <span class="hljs-number">0.6854</span>, <span class="hljs-number">0.0438</span>, <span class="hljs-number">0.0636</span>,<br>        <span class="hljs-number">0.2884</span>, <span class="hljs-number">0.4362</span>, <span class="hljs-number">0.2368</span>, <span class="hljs-number">0.1394</span>, <span class="hljs-number">0.1721</span>, <span class="hljs-number">0.1751</span>, <span class="hljs-number">0.3851</span>, <span class="hljs-number">0.0732</span>, <span class="hljs-number">0.3118</span>,<br>        <span class="hljs-number">0.9180</span>, <span class="hljs-number">0.7293</span>])<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br></code></pre></td></tr></table></figure><ul><li>其实增加或减少一个范围为1的维度，其实对数据本身并没有影响，这也能提示我们： Calls to <code>squeeze()</code> and <code>unsqueeze()</code> can only act on dimensions of extent 1 because to do otherwise would change the number of elements in the tensor.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>b = torch.rand(   <span class="hljs-number">3</span>)     <span class="hljs-comment"># trying to multiply a * b will give a runtime error</span><br>c = b.unsqueeze(<span class="hljs-number">1</span>)       <span class="hljs-comment"># change to a 2-dimensional tensor, adding new dim at the end</span><br><span class="hljs-built_in">print</span>(c.shape)<br><span class="hljs-built_in">print</span>(a * c)             <span class="hljs-comment"># broadcasting works again!</span><br></code></pre></td></tr></table></figure><p>Sometimes you’ll want to change the shape of a tensor more radically, while still preserving the number of elements and their contents. One case where this happens is at the interface between a convolutional layer of a model and a linear layer of the model - this is common in image classification models. A convolution kernel will yield an output tensor of shape <em>features x width x height,</em> but the following linear layer expects a 1-dimensional input. <code>reshape()</code> will do this for you, provided that the dimensions you request yield the same number of elements as the input tensor has:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">output3d = torch.rand(<span class="hljs-number">6</span>, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>)<br><span class="hljs-built_in">print</span>(output3d.shape)<br><br>input1d = output3d.reshape(<span class="hljs-number">6</span> * <span class="hljs-number">20</span> * <span class="hljs-number">20</span>)<br><span class="hljs-built_in">print</span>(input1d.shape)<br><br><span class="hljs-comment"># can also call it as a method on the torch module:</span><br><span class="hljs-built_in">print</span>(torch.reshape(output3d, (<span class="hljs-number">6</span> * <span class="hljs-number">20</span> * <span class="hljs-number">20</span>,)).shape)<br><br>torch.Size([<span class="hljs-number">6</span>, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>])<br>torch.Size([<span class="hljs-number">2400</span>])<br>torch.Size([<span class="hljs-number">2400</span>])<br></code></pre></td></tr></table></figure><h1 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h1><p>NumPy 有和张量比较想通的地方，比如<code>Broadcast</code>，所以在二者之间转换很容易</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>numpy_array = np.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br><span class="hljs-built_in">print</span>(numpy_array)<br><br>pytorch_tensor = torch.from_numpy(numpy_array)<br><span class="hljs-built_in">print</span>(pytorch_tensor)<br><br>[[<span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span>]<br> [<span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span>]]<br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]], dtype=torch.float64)<br></code></pre></td></tr></table></figure><ul><li>PyTorch 创建了一个与 NumPy 数组具有相同形状并包含相同数据的张量，甚至保留了 NumPy 默认的 64 位浮点数据类型。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">pytorch_rand = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(pytorch_rand)<br><br>numpy_rand = pytorch_rand.numpy()<br><span class="hljs-built_in">print</span>(numpy_rand)<br></code></pre></td></tr></table></figure><p>这些都不重要，重要的是要知道<strong>这些转换后的对象使用与其源对象有相同的底层内存_ ，这意味着对一个对象的更改会反映在另一个对象中</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy_array[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>] = <span class="hljs-number">23</span><br><span class="hljs-built_in">print</span>(pytorch_tensor)<br><br>pytorch_rand[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>] = <span class="hljs-number">17</span><br><span class="hljs-built_in">print</span>(numpy_rand)<br><br>tensor([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">1.</span>, <span class="hljs-number">23.</span>,  <span class="hljs-number">1.</span>]], dtype=torch.float64)<br>[[ <span class="hljs-number">0.5646949</span>   <span class="hljs-number">0.91600937</span>  <span class="hljs-number">0.77828014</span>]<br> [ <span class="hljs-number">0.82769746</span> <span class="hljs-number">17.</span>          <span class="hljs-number">0.6381657</span> ]]<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
      <tag>Python</tag>
      
      <tag>ML</tag>
      
      <tag>Tensor</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TUH数据集下载</title>
    <link href="/2025/07/30/TUH%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/"/>
    <url>/2025/07/30/TUH%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p> 如果你正在从事EEG&#x2F;BCI 等方向的研究，或许会用到TUH的数据集，like TUSZ\TUEG\TUAB……<br>本文旨在提供提供一点个人经验，希望能对你有所帮助😊 </p>          </div><p>不得不说，这个数据集下载确实费了我老半天劲😭</p><ul><li>数据集官网网址：<a href="https://isip.piconepress.com/projects/nedc/html/tuh_eeg/">this site</a></li><li>可提供下载的资源：<img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250730100734556.png"></li><li>接下来我就分几步来说说了</li></ul><h1 id="账号申请"><a href="#账号申请" class="headerlink" title="账号申请"></a>账号申请</h1><p>进入主页第一段便是：</p><blockquote><p>To request access to the TUH EEG Corpus, please fill out <a href="https://isip.piconepress.com/projects/nedc/forms/tuh_eeg.pdf">this form</a> and email a signed copy to <a href="mailto:help@nedcdata.org?Subject=Download%20The%20TUH%20EEG%20Corpus">help@nedcdata.org</a>. Please include “Download The TUH EEG Corpus” in the subject line or click on this <a href="mailto:help@nedcdata.org?Subject=Download%20The%20TUH%20EEG%20Corpus">link</a>.</p></blockquote><p>你要下载数据集，后面需要一个账号登录，所以先申请一个账号，大致需要我们完成以下几步：</p><ul><li>填写表格</li><li>插入签名</li><li>发送邮件</li></ul><p>当你下载附件后，可以看到真正意义上的表格前面的全部是注意事项。也意味着表格的填写必须<code>formatted correctly</code>，<strong>人工审核比较严苛</strong>，我第一轮就被reject了，这里说一下：</p><table><thead><tr><th>Your Legal Name</th><th>填写你的英文名</th></tr></thead><tbody><tr><td>Your Institutional Affiliation<br>(Complete Name – No Abbreviations)</td><td>你所在的机构名称，填完整名字，比如<br>Beihang University, Beijing, China</td></tr><tr><td>Your Personal Post Office Address At<br>The Above Institution</td><td>这个最麻烦，官方给了一个模板，可以改成自己的</td></tr><tr><td>Telephone Number<br>(Include Your Country Code)</td><td>需要添加国家代码，CN就是+86 你的电话</td></tr><tr><td>Email Address</td><td>最好填学校或者机构的邮箱，到时候也用这个邮箱发</td></tr><tr><td>第三列需要我们提供在机构中的<strong>非常详细的个人办公位置</strong>或者叫个人邮局位置，作者给出的示例是自己：</td><td></td></tr></tbody></table><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs text">Joseph Picone<br>ENGR 718<br>College of Engineering<br>Temple University<br>1947 North 12th Street<br>Philadelphia, Pennsylvania, USA 19111<br></code></pre></td></tr></table></figure><p>可以看到，第一行填名字，第二行需要填你的办公室位置，我这里就填的实验室位置，第三行填学院，第四行填学校，<strong>最后两行填具体位置与邮编，这个需要非常准确且正式，把握不准的话可以问一下师长</strong>。可以发现，倒着连起来就是一个能够定位你的精确位置。这里也给出我的：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs text">帅哥一枚<br>New Main Building G720<br>School of Computer Science and Engineering<br>Beihang University<br>37 Xueyuan Road, Haidian District,<br>Beijing, P.R.China, 100191<br></code></pre></td></tr></table></figure><p>最后插入日期以及你的签名就好了，然后把这个作为附件，发一封主题为”Download The TUH EEG Corpus“的邮件给 <a href="mailto:&#x68;&#101;&#x6c;&#x70;&#x40;&#x6e;&#x65;&#100;&#x63;&#x64;&#97;&#x74;&#x61;&#46;&#x6f;&#x72;&#x67;">&#x68;&#101;&#x6c;&#x70;&#x40;&#x6e;&#x65;&#100;&#x63;&#x64;&#97;&#x74;&#x61;&#46;&#x6f;&#x72;&#x67;</a> 就好，大概一两天就会给你回复，这样你就有一个账号密码了。</p><h1 id="windows下载器安装"><a href="#windows下载器安装" class="headerlink" title="windows下载器安装"></a>windows下载器安装</h1><p>&#x3D;&#x3D;Note: 如果你是 Linux&#x2F;Mac 用户，跳过这一步，当然我估计大部分朋友还是用的服务器&#x3D;&#x3D;</p><p>win用户需要下载MobaXterm,from <a href="https://mobaxterm.mobatek.net/">this site</a> 。这是一款 <strong>功能强大的 Windows 远程管理工具</strong>，集成了多种网络工具（SSH、X11、RDP、VNC、FTP、SFTP 等），并内置了 <strong>Linux 命令环境</strong>（Cygwin）<br>解压后随便安装，然后打开，然后打开本地终端，好像就这样</p><h1 id="下载命令"><a href="#下载命令" class="headerlink" title="下载命令"></a>下载命令</h1><p>Linux用户直接打开终端即可，官网先给了一个测试指令，看看连接有无问题，回车后输入密码，就是你先前账号的密码即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rsync -auxvL nedc-tuh-eeg@www.isip.piconepress.com:data/tuh_eeg/TEST<br></code></pre></td></tr></table></figure><p>如果下载成功，那么可以继续<br>一个常见的下载指令就是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rsync -auxvL nedc-tuh-eeg@www.isip.piconepress.com:data/tuh_eeg/AAAA/<br></code></pre></td></tr></table></figure><p>你可以把你想要下载的数据集替换AAAA即可，这里有个坑，比如你想下载TUSZ，不能直接替换，而是要在<a href="https://isip.piconepress.com/projects/nedc/data/tuh_eeg/">这里</a>找到全名替换，其他也是。然后你应该不想保存到默认目录，还可以再后面加上你想要下载的目录。最后的完整的指令就是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rsync -auxvL nedc-tuh-eeg@www.isip.piconepress.com:data/tuh_eeg/tuh_eeg_seizure/    your/path/for/data<br></code></pre></td></tr></table></figure><p>当然，你也可以根据官网上的目录结构选择只下载某版本或某文件夹，变换路径即可</p><h1 id="之后"><a href="#之后" class="headerlink" title="之后"></a>之后</h1><p>如果数据集比较大，你就可以开始无尽地等待了，如果是服务器的话，挂着就行，反正我当时还下了挺久。。<br>另外我没用🪜，也没断连</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://blog.csdn.net/JY_Wang1996/article/details/147655277?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522a3f06aac17bebaa9234d98994c0f46a6%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=a3f06aac17bebaa9234d98994c0f46a6&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-147655277-null-null.142%5Ev102%5Epc_search_result_base1&utm_term=TUH%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD&spm=1018.2226.3001.4187">Fergieeee</a><br><a href="https://blog.csdn.net/weixin_54050455/article/details/146293617?ops_request_misc=%257B%2522request%255Fid%2522%253A%252281d56fdea1baa3a666ea38e7125a780f%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=81d56fdea1baa3a666ea38e7125a780f&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-146293617-null-null.142%5Ev102%5Epc_search_result_base1&utm_term=TUH%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD&spm=1018.2226.3001.4187">嘻嘻哈哈好好生活</a></p>]]></content>
    
    
    <categories>
      
      <category>嗑盐</category>
      
    </categories>
    
    
    <tags>
      
      <tag>嗑盐</tag>
      
      <tag>数据集</tag>
      
      <tag>EEG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL</title>
    <link href="/2025/07/20/SQL/"/>
    <url>/2025/07/20/SQL/</url>
    
    <content type="html"><![CDATA[<p>本笔记旨在全面梳理SQL的核心概念，从数据定义、基础查询到复杂的多表连接、子查询及高级特性。所有概念均结合了课程讲义中的理论和项目实践中的具体应用。</p><h1 id="一、-SQL与关系模型概述"><a href="#一、-SQL与关系模型概述" class="headerlink" title="一、 SQL与关系模型概述"></a>一、 SQL与关系模型概述</h1><h2 id="1-SQL的历史与地位"><a href="#1-SQL的历史与地位" class="headerlink" title="1. SQL的历史与地位"></a>1. SQL的历史与地位</h2><ul><li><p><strong>起源</strong>: SQL (Structured Query Language) 于20世纪70年代在IBM的System R项目中诞生。</p></li><li><p><strong>持久性</strong>: 尽管经历了面向对象、XML和NoSQL等技术的挑战，SQL凭借其强大的功能和标准化，至今仍是数据查询领域的核心语言。</p></li><li><p><strong>核心优势</strong>: SQL是一种<strong>声明式</strong>语言，用户只需说明“想要什么”，而无需关心“如何获取”，具体的执行策略由数据库管理系统（DBMS）负责优化。</p></li></ul><h2 id="2-关系模型术语"><a href="#2-关系模型术语" class="headerlink" title="2. 关系模型术语"></a>2. 关系模型术语</h2><ul><li><p><strong>数据库 (Database)</strong>: 一系列命名表（Relation）的集合。</p></li><li><p><strong>表 (Table&#x2F;Relation)</strong>: 由两部分组成：</p><ul><li><p><strong>模式 (Schema)</strong>: 对表的结构描述，即元数据，定义了列名和数据类型。例如<code>people</code>表储存了球员信息。</p></li><li><p><strong>实例 (Instance)</strong>: 在某一时刻，表中存储的具体数据，是行的集合。</p></li></ul></li><li><p><strong>列 (Column&#x2F;Attribute&#x2F;Field)</strong>: 表的垂直部分，具有唯一的名称和原子数据类型（如<code>INTEGER</code>, <code>TEXT</code>, <code>FLOAT</code>）。</p></li><li><p><strong>行 (Row&#x2F;Tuple&#x2F;Record)</strong>: 表的水平部分，代表一条具体的记录。</p></li><li><p><strong>多重集 (Multiset)</strong>: 关系数据库中的表是行的“多重集”，意味着允许存在完全相同的行（重复元组）。</p></li></ul><h2 id="3-SQL语言的组成"><a href="#3-SQL语言的组成" class="headerlink" title="3. SQL语言的组成"></a>3. SQL语言的组成</h2><ul><li><p><strong>DDL (Data Definition Language)</strong>: 数据定义语言，用于创建和修改数据库的模式。</p><ul><li><strong><code>CREATE TABLE</code></strong>: 定义新表的结构。</li></ul></li><li><p><strong>DML (Data Manipulation Language)</strong>: 数据操作语言，用于查询和修改数据。</p><ul><li>**<code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>**。</li></ul></li></ul><h1 id="二、-DDL-定义数据结构"><a href="#二、-DDL-定义数据结构" class="headerlink" title="二、 DDL: 定义数据结构"></a>二、 DDL: 定义数据结构</h1><p>使用<code>CREATE TABLE</code>来定义表的模式，包括列名、数据类型和约束。在<code>proj1.sql</code>的题目描述中，就给出了<code>dogs</code>和<code>users</code>表的创建语句。</p><p><strong>完整示例</strong>:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> users (<br>    userid <span class="hljs-type">INTEGER</span>,<br>    name <span class="hljs-type">VARCHAR</span>,<br>    age <span class="hljs-type">INTEGER</span>,<br>    <span class="hljs-keyword">PRIMARY</span> KEY (userid)<br>);<br><br><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> dogs (<br>    dogid <span class="hljs-type">INTEGER</span>,<br>    ownerid <span class="hljs-type">INTEGER</span>,<br>    name <span class="hljs-type">VARCHAR</span>,<br>    breed <span class="hljs-type">VARCHAR</span>,<br>    age <span class="hljs-type">INTEGER</span>,<br>    <span class="hljs-keyword">PRIMARY</span> KEY (dogid),<br>    <span class="hljs-keyword">FOREIGN</span> KEY (ownerid) <span class="hljs-keyword">REFERENCES</span> users (userid)<br>);<br></code></pre></td></tr></table></figure><p><strong>核心概念</strong>:</p><ul><li><p><strong><code>PRIMARY KEY</code> (主键)</strong>: 用于唯一标识表中的每一行。主键列的值不能重复，也不能为<code>NULL</code>。可以由单个列或多个列（复合主键）组成。</p></li><li><p><strong><code>FOREIGN KEY</code> (外键)</strong>: 在一个表中建立与另一个表主键的引用链接。用于强制表间的引用完整性，确保关联数据的一致性。</p></li></ul><h1 id="三、-DML-基础查询与单表操作"><a href="#三、-DML-基础查询与单表操作" class="headerlink" title="三、 DML: 基础查询与单表操作"></a>三、 DML: 基础查询与单表操作</h1><h2 id="1-基础查询结构-SELECT-和-FROM"><a href="#1-基础查询结构-SELECT-和-FROM" class="headerlink" title="1. 基础查询结构: SELECT 和 FROM"></a>1. 基础查询结构: <code>SELECT</code> 和 <code>FROM</code></h2><p>这是所有查询的基石，用于指定从哪个表中选取哪些列的数据。</p><p><strong>语法</strong>:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">&lt;</span>columns<span class="hljs-operator">&gt;</span><br><span class="hljs-keyword">FROM</span> <span class="hljs-operator">&lt;</span><span class="hljs-keyword">table</span><span class="hljs-operator">&gt;</span>;<br></code></pre></td></tr></table></figure><p><strong>示例</strong>: 从<code>people</code>表中选取球员的ID、名和姓。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">-- &quot;proj1.sql&quot; 中的一个基础操作</span><br><span class="hljs-keyword">SELECT</span> playerid, namefirst, namelast <span class="hljs-keyword">FROM</span> people;<br></code></pre></td></tr></table></figure><h2 id="2-过滤数据-WHERE-子句"><a href="#2-过滤数据-WHERE-子句" class="headerlink" title="2. 过滤数据: WHERE 子句"></a>2. 过滤数据: <code>WHERE</code> 子句</h2><p><code>WHERE</code>子句用于根据指定的条件（谓词）筛选出我们感兴趣的行。</p><p><strong>语法</strong>:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">&lt;</span>columns<span class="hljs-operator">&gt;</span><br><span class="hljs-keyword">FROM</span> <span class="hljs-operator">&lt;</span><span class="hljs-keyword">table</span><span class="hljs-operator">&gt;</span><br><span class="hljs-keyword">WHERE</span> <span class="hljs-operator">&lt;</span>predicate<span class="hljs-operator">&gt;</span>;<br></code></pre></td></tr></table></figure><p><strong>示例</strong>: 在<code>proj1.sql</code>的<code>q1i</code>中，查询体重超过300磅的球员。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">-- Query from q1i</span><br><span class="hljs-keyword">SELECT</span> namefirst, namelast, birthyear<br><span class="hljs-keyword">FROM</span> people p<br><span class="hljs-keyword">WHERE</span> p.weight <span class="hljs-operator">&gt;</span> <span class="hljs-number">300</span>;<br></code></pre></td></tr></table></figure><p><code>WHERE</code>子句中还可以使用布尔运算符 <code>AND</code>, <code>OR</code>, <code>NOT</code> 来组合多个条件。建议使用括号<code>()</code>明确指定运算优先级。</p><h2 id="3-结果排序-ORDER-BY"><a href="#3-结果排序-ORDER-BY" class="headerlink" title="3. 结果排序: ORDER BY"></a>3. 结果排序: <code>ORDER BY</code></h2><p><code>ORDER BY</code> 用于对输出结果进行排序。默认为升序 (<code>ASC</code>)，可指定降序 (<code>DESC</code>)。可以按多个列排序。</p><p><strong>示例</strong>: 在<code>proj1.sql</code>的<code>q2i</code>中，将名人堂成员按入选年份降序排列，年份相同时按球员ID升序排列。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">-- Query from q2i</span><br><span class="hljs-keyword">SELECT</span> namefirst, namelast, p.playerid, yearid<br>...<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> yearid <span class="hljs-keyword">DESC</span>, p.playerid;<br></code></pre></td></tr></table></figure><h2 id="4-限制输出行数-LIMIT"><a href="#4-限制输出行数-LIMIT" class="headerlink" title="4. 限制输出行数: LIMIT"></a>4. 限制输出行数: <code>LIMIT</code></h2><p><code>LIMIT</code> 子句用于限制查询返回的行数，常与 <code>ORDER BY</code> 结合使用以获取“Top N”结果。</p><p><strong>示例</strong>: 在<code>proj1.sql</code>的<code>q3i</code>中，获取单赛季打击率（slg）排名前10的记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-comment">-- Query from q3i</span><br><span class="hljs-keyword">SELECT</span> P.playerid, namefirst, namelast, yearid, slg<br>...<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> slg <span class="hljs-keyword">DESC</span>, yearid, P.playerid<br>LIMIT <span class="hljs-number">10</span>;<br></code></pre></td></tr></table></figure><h1 id="四、-分组与聚合"><a href="#四、-分组与聚合" class="headerlink" title="四、 分组与聚合"></a>四、 分组与聚合</h1><h2 id="1-聚合函数"><a href="#1-聚合函数" class="headerlink" title="1. 聚合函数"></a>1. 聚合函数</h2><p>SQL提供了一系列内置的聚合函数，用于对一组值进行计算并返回单个值。</p><ul><li><p><strong><code>COUNT()</code></strong>: <code>COUNT(*)</code>计算所有行数，<code>COUNT(&lt;column&gt;)</code>计算该列非<code>NULL</code>值的数量。</p></li><li><p><strong><code>SUM()</code></strong>, <strong><code>AVG()</code></strong>, <strong><code>MAX()</code></strong>, **<code>MIN()</code>**。</p></li><li><p>除<code>COUNT(*)</code>外，所有聚合函数都会忽略<code>NULL</code>值。</p></li></ul><p><strong>示例</strong>: 在<code>proj1.sql</code>的<code>q0</code>中，查找历史最高自责分率（era）。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-comment">-- Query from q0</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">MAX</span>(era)<br><span class="hljs-keyword">FROM</span> pitching;<br></code></pre></td></tr></table></figure><h2 id="2-GROUP-BY-与-HAVING"><a href="#2-GROUP-BY-与-HAVING" class="headerlink" title="2. GROUP BY 与 HAVING"></a>2. <code>GROUP BY</code> 与 <code>HAVING</code></h2><ul><li><p><strong><code>GROUP BY</code></strong>: 将数据按指定列的值进行分组，以便对每个组独立进行聚合计算。</p></li><li><p><strong><code>HAVING</code></strong>: 在数据分组后，对这些组进行过滤。<code>WHERE</code>过滤行，<code>HAVING</code>过滤组（聚合之后其实也是行😀）</p></li></ul><p>重要规则 (<code>The Aggregation Rule</code>) :</p><p><strong>如果查询中使用了GROUP BY，那么SELECT子句中只能包含分组列和聚合函数。</strong></p><p><strong>示例</strong>: 在<code>proj1.sql</code>的<code>q1iv</code>中，找出平均身高大于70的出生年份组。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-comment">-- Query from q1iv</span><br><span class="hljs-keyword">SELECT</span> birthyear, <span class="hljs-built_in">AVG</span>(height), <span class="hljs-built_in">COUNT</span>(<span class="hljs-operator">*</span>)<br><span class="hljs-keyword">FROM</span> people<br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> birthyear<br><span class="hljs-keyword">HAVING</span> <span class="hljs-built_in">AVG</span>(height) <span class="hljs-operator">&gt;</span> <span class="hljs-number">70</span><br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> birthyear;<br></code></pre></td></tr></table></figure><h1 id="五、-多表查询-Joins"><a href="#五、-多表查询-Joins" class="headerlink" title="五、 多表查询: Joins"></a>五、 多表查询: Joins</h1><p>当所需信息分布在多个表中时，需要使用 <code>JOIN</code> 将它们连接起来。</p><h2 id="1-连接的类型与语法"><a href="#1-连接的类型与语法" class="headerlink" title="1. 连接的类型与语法"></a>1. 连接的类型与语法</h2><table><thead><tr><th>连接类型</th><th>描述</th></tr></thead><tbody><tr><td><strong>内连接 (INNER JOIN)</strong></td><td>(默认连接类型) 只返回两个表中连接键能匹配上的行。</td></tr><tr><td><strong>左外连接 (LEFT OUTER JOIN)</strong></td><td>保留左表的所有行。若右表无匹配，则右表列填充为<code>NULL</code>。</td></tr><tr><td><strong>右外连接 (RIGHT OUTER JOIN)</strong></td><td>保留右表的所有行。若左表无匹配，则左表列填充为<code>NULL</code>。</td></tr><tr><td><strong>全外连接 (FULL OUTER JOIN)</strong></td><td>保留两个表的所有行，在无匹配处用<code>NULL</code>填充。</td></tr></tbody></table><p><strong>示例 (INNER JOIN)</strong>: 在<code>proj1.sql</code>的<code>q2i</code>中，连接<code>people</code>和<code>halloffame</code>表找出名人堂成员。</p><ul><li><strong>值得一提的是，其实FROM多个表默认的就是 INNER JOIN</strong>，<code>INNER / OUTER</code> 可省略</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-comment">-- Query from q2i (implicit syntax)</span><br><span class="hljs-keyword">SELECT</span> namefirst, namelast, p.playerid, yearid<br><span class="hljs-keyword">FROM</span> people p, halloffame ha<br><span class="hljs-keyword">WHERE</span> p.playerid <span class="hljs-operator">=</span> ha.playerid <span class="hljs-keyword">AND</span> ha.inducted <span class="hljs-keyword">LIKE</span> <span class="hljs-string">&#x27;Y&#x27;</span>;<br></code></pre></td></tr></table></figure><p><strong>示例 (LEFT OUTER JOIN)</strong>: 在<code>proj1.sql</code>的<code>q2iii</code>中，查询所有名人堂成员及其学校，即使没有上过大学也要包括。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-comment">-- Query from q2iii</span><br><span class="hljs-keyword">SELECT</span> q2i.playerid, namefirst, namelast, schoolid<br><span class="hljs-keyword">FROM</span> q2i<br><span class="hljs-keyword">LEFT</span> <span class="hljs-keyword">OUTER</span> <span class="hljs-keyword">JOIN</span> collegeplaying C <span class="hljs-keyword">ON</span> C.playerid <span class="hljs-operator">=</span> q2i.playerid<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> q2i.playerid <span class="hljs-keyword">DESC</span>, schoolid;<br></code></pre></td></tr></table></figure><h2 id="2-自连接-Self-Join"><a href="#2-自连接-Self-Join" class="headerlink" title="2. 自连接 (Self-Join)"></a>2. 自连接 (Self-Join)</h2><p>将一个表与其自身进行连接，通常需要使用表别名来区分。</p><p><strong>示例</strong>: 在<code>proj1.sql</code>的<code>q4iii</code>中，通过自连接<code>q4i</code>视图计算每年薪资相对于上一年的变化。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-comment">-- Query from q4iii</span><br><span class="hljs-keyword">SELECT</span> Q2.yearid, Q2.min <span class="hljs-operator">-</span> Q1.min, Q2.max <span class="hljs-operator">-</span> Q1.max, Q2.avg <span class="hljs-operator">-</span> Q1.avg<br><span class="hljs-keyword">FROM</span> q4i Q1, q4i Q2<br><span class="hljs-keyword">WHERE</span> Q1.yearid <span class="hljs-operator">+</span> <span class="hljs-number">1</span> <span class="hljs-operator">=</span> Q2.yearid<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> Q2.yearid;<br></code></pre></td></tr></table></figure><h1 id="六、-子查询、集合运算与CTE"><a href="#六、-子查询、集合运算与CTE" class="headerlink" title="六、 子查询、集合运算与CTE"></a>六、 子查询、集合运算与CTE</h1><h2 id="1-子查询-Subquery"><a href="#1-子查询-Subquery" class="headerlink" title="1. 子查询 (Subquery)"></a>1. 子查询 (Subquery)</h2><p>子查询是嵌套在另一个SQL查询中的查询。</p><ul><li><p><strong>在 <code>WHERE</code> &#x2F; <code>HAVING</code> 中</strong>: 常用于根据一个聚合结果进行过滤。</p></li><li><p><strong>在 <code>FROM</code> 中</strong>: 子查询的结果可以作为一个临时的、可供外部查询的表。</p></li><li><p><strong>相关子查询 (Correlated Subquery)</strong>: 子查询的执行依赖于主查询当前处理的行。</p></li></ul><p><strong>示例</strong>: 在<code>proj1.sql</code>的<code>q3iii</code>中，查找终身打击率比特定球员<code>&#39;mayswi01&#39;</code>更高的球员。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-comment">-- Snippet from q3iii</span><br>... <span class="hljs-keyword">WHERE</span> L.lslg <span class="hljs-operator">&gt;</span> (<span class="hljs-keyword">SELECT</span> lslg <span class="hljs-keyword">FROM</span> goat)<br></code></pre></td></tr></table></figure><p>这里的 <code>(SELECT lslg FROM goat)</code> 就是一个子查询。</p><h2 id="2-集合运算-Set-Operations"><a href="#2-集合运算-Set-Operations" class="headerlink" title="2. 集合运算 (Set Operations)"></a>2. 集合运算 (Set Operations)</h2><p>用于合并两个或多个<code>SELECT</code>语句的结果集。</p><ul><li><p><strong><code>UNION</code></strong>: 合并结果集并<strong>移除重复行</strong>。</p></li><li><p><strong><code>UNION ALL</code></strong>: 合并结果集并<strong>保留所有行</strong>。</p></li><li><p><strong><code>INTERSECT</code></strong>: 返回两个结果集的交集。</p></li><li><p><strong><code>EXCEPT</code></strong>: 返回在第一个结果集中存在，但在第二个结果集中不存在的行。</p></li></ul><h2 id="3-通用表表达式-Common-Table-Expression-CTE"><a href="#3-通用表表达式-Common-Table-Expression-CTE" class="headerlink" title="3. 通用表表达式 (Common Table Expression - CTE)"></a>3. 通用表表达式 (Common Table Expression - CTE)</h2><p>使用 <code>WITH</code> 关键字定义的临时命名结果集。CTE可以极大地提高复杂查询的<strong>可读性和模块化程度</strong>，在<code>proj1.sql</code>中被广泛使用。</p><p><strong>示例</strong>: 在<code>proj1.sql</code>的<code>q3i</code>中，先用CTE (<code>slg_cal</code>) 计算打击率，再从这个临时结果中查询。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-comment">-- Query from q3i</span><br><span class="hljs-keyword">WITH</span> slg_cal <span class="hljs-keyword">AS</span> (<br>  <span class="hljs-keyword">SELECT</span> playerid, yearid, ((H <span class="hljs-operator">-</span> H2B <span class="hljs-operator">-</span> H3B <span class="hljs-operator">-</span> HR) <span class="hljs-operator">+</span> <span class="hljs-number">2</span><span class="hljs-operator">*</span>H2B <span class="hljs-operator">+</span> <span class="hljs-number">3</span><span class="hljs-operator">*</span>H3B <span class="hljs-operator">+</span> <span class="hljs-number">4</span><span class="hljs-operator">*</span>HR) <span class="hljs-operator">/</span> <span class="hljs-built_in">CAST</span>(AB <span class="hljs-keyword">AS</span> <span class="hljs-type">FLOAT</span>) <span class="hljs-keyword">AS</span> slg<br>  <span class="hljs-keyword">FROM</span> batting<br>  <span class="hljs-keyword">WHERE</span> AB <span class="hljs-operator">&gt;</span> <span class="hljs-number">50</span><br>)<br><span class="hljs-keyword">SELECT</span> P.playerid, namefirst, namelast, yearid, slg<br><span class="hljs-keyword">FROM</span> people P, slg_cal S<br><span class="hljs-keyword">WHERE</span> P.playerid <span class="hljs-operator">=</span> S.playerid<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> slg <span class="hljs-keyword">DESC</span>, yearid, P.playerid<br>LIMIT <span class="hljs-number">10</span>;<br></code></pre></td></tr></table></figure><h1 id="七、-其他重要概念"><a href="#七、-其他重要概念" class="headerlink" title="七、 其他重要概念"></a>七、 其他重要概念</h1><h2 id="1-NULL值与三值逻辑"><a href="#1-NULL值与三值逻辑" class="headerlink" title="1. NULL值与三值逻辑"></a>1. <code>NULL</code>值与三值逻辑</h2><ul><li><p><strong><code>NULL</code>的含义</strong>: 代表“未知”、“缺失”或“不适用”的值。</p></li><li><p><strong>三值逻辑 (Three-Valued Logic)</strong>: <code>WHERE</code>子句只保留计算结果为<code>TRUE</code>的行，<code>FALSE</code>和<code>NULL</code>的行都会被丢弃。</p></li><li><p><strong>检查<code>NULL</code></strong>: 必须使用 <code>IS NULL</code> 或 <code>IS NOT NULL</code>。</p></li></ul><h2 id="2-类型转换-CAST"><a href="#2-类型转换-CAST" class="headerlink" title="2. 类型转换: CAST"></a>2. 类型转换: <code>CAST</code></h2><p>这是一个<strong>类型转换</strong>的关键字 or 关键函数 ？，用法也很简单，如下示例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-built_in">CAST</span>(expression <span class="hljs-keyword">AS</span> target_type)<br><br><span class="hljs-comment">-- 将字符串转换为整数</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">CAST</span>(<span class="hljs-string">&#x27;123&#x27;</span> <span class="hljs-keyword">AS</span> <span class="hljs-type">INTEGER</span>);<br><br><span class="hljs-comment">-- 将浮点数转换为整数（会截断小数部分）</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">CAST</span>(<span class="hljs-number">3.14</span> <span class="hljs-keyword">AS</span> <span class="hljs-type">INT</span>);<br><br><span class="hljs-comment">-- 将整数转换为浮点数（避免整数除法问题）</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">CAST</span>(<span class="hljs-number">5</span> <span class="hljs-keyword">AS</span> <span class="hljs-type">FLOAT</span>) <span class="hljs-operator">/</span> <span class="hljs-number">2</span>; <span class="hljs-comment">-- 结果是2.5</span><br><br><span class="hljs-keyword">SELECT</span> <span class="hljs-number">5</span> <span class="hljs-operator">/</span> <span class="hljs-number">2</span>;                <span class="hljs-comment">-- 结果是2（整数除法）</span><br></code></pre></td></tr></table></figure><h2 id="3-模式匹配-LIKE-and"><a href="#3-模式匹配-LIKE-and" class="headerlink" title="3. 模式匹配: LIKE and ~"></a>3. 模式匹配: <code>LIKE</code> and <code>~</code></h2><p><code>LIKE</code> 操作符用于在<code>WHERE</code>子句中搜索列中的指定模式。</p><ul><li><p><code>%</code>: 匹配任意数量的字符。</p></li><li><p><code>_</code>: 匹配单个字符。</p></li></ul><p><strong>示例</strong>: 在<code>proj1.sql</code>的<code>q1ii</code>中，查找<code>namefirst</code>字段中包含空格的球员。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-comment">-- Query from q1ii</span><br>... <span class="hljs-keyword">WHERE</span> namefirst <span class="hljs-keyword">LIKE</span> <span class="hljs-string">&#x27;% %&#x27;</span> ...<br></code></pre></td></tr></table></figure><h3 id="SQLite"><a href="#SQLite" class="headerlink" title="SQLite"></a>SQLite</h3><p>在SQLite中，并没有<code>~</code>这一种匹配模式，而它是使用[[UNIX_Review_2#^1 | 正则表达式]] 匹配的，示例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-comment">-- 匹配以数字结尾的球员ID</span><br><br><span class="hljs-number">2</span><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> players <span class="hljs-keyword">WHERE</span> player_id <span class="hljs-operator">~</span> <span class="hljs-string">&#x27;[0-9]$&#x27;</span>;<br></code></pre></td></tr></table></figure><h2 id="4-视图-CREATE-VIEW"><a href="#4-视图-CREATE-VIEW" class="headerlink" title="4. 视图: CREATE VIEW"></a>4. 视图: <code>CREATE VIEW</code></h2><p>视图是一个虚拟表，其内容由查询定义。<code>proj1.sql</code>中的所有问题都是通过创建视图来解答的，这允许将复杂的查询保存为命名的、可重用的对象。</p><p><strong>示例</strong>: <code>proj1.sql</code>的<code>q0</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">VIEW</span> q0(era)<br><span class="hljs-keyword">AS</span><br>  <span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">MAX</span>(era)<br>  <span class="hljs-keyword">FROM</span> pitching;<br></code></pre></td></tr></table></figure><h1 id="八、-SQL查询逻辑执行顺序"><a href="#八、-SQL查询逻辑执行顺序" class="headerlink" title="八、 SQL查询逻辑执行顺序"></a>八、 SQL查询逻辑执行顺序</h1><p>理解逻辑执行顺序有助于编写和调试复杂查询。</p><ol><li><p><strong><code>FROM</code></strong>: 处理<code>FROM</code>子句及所有<code>JOIN</code>操作。</p></li><li><p><strong><code>WHERE</code></strong>: 过滤行。</p></li><li><p><strong><code>GROUP BY</code></strong>: 将过滤后的行分组。</p></li><li><p><strong><code>HAVING</code></strong>: 过滤分组。</p></li><li><p><strong><code>SELECT</code></strong>: 计算<code>SELECT</code>列表中的表达式。</p></li><li><p><strong><code>DISTINCT</code></strong>: 移除结果集中的重复行。</p></li><li><p><strong><code>ORDER BY</code></strong>: 对最终结果集进行排序。</p></li><li><p><strong><code>LIMIT</code></strong>: 从排序后的结果集中选取指定数量的行。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>CS-186</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>MySQL</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大二下总结</title>
    <link href="/2025/07/10/%E5%A4%A7%E4%BA%8C%E4%B8%8B%E6%80%BB%E7%BB%93/"/>
    <url>/2025/07/10/%E5%A4%A7%E4%BA%8C%E4%B8%8B%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是本人大二下学期的总结 ，希望能对你有所帮助😊</p>          </div><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/%E9%BE%99%E7%8C%AB3.jpg"></p><p><strong>学长学姐们口中魔鬼般的大二下学期总算是结束了，这是一个极度劳累极度充实的学期。在这学期里，我完成了诸多任务与挑战，也学习收获了很多新的知识，同时抗压能力up up up !</strong></p><h1 id="OO-OS"><a href="#OO-OS" class="headerlink" title="OO &amp; OS"></a>OO &amp; OS</h1><ul><li><p>其实这两门课在课上学习阶段，体验都还挺不错的，wty老师总是细致入微地讲解每一个知识点，rwg老师总是幽默风趣地逗大家开心。</p></li><li><p>OS课下发了官方答案，我一般也都是边抄边理解边看学长博客边问AI，大概需要一天时间完成并理解课下内容，<strong>课上测试的话</strong>，exam纯翻译即可，extra就随缘了，有几次都非常困难。最后对整个MOS的框架以及具体的细节还是很不清楚，OS-Challenge也是完成的非常吃力，最后一顿借鉴才得以通过。理论的话其实我还是很感兴趣的，从最早的Multics到UNIX再到Linux，我惊叹于他们的天才智慧与惊人的创造力，也被操作系统的复杂性、神秘性、兼容性所吸引。<strong>只可惜最后的成绩不太如意……😭</strong></p></li><li><p>OO的话<strong>第一单元过得非常痛苦</strong>，前两次因为极其愚蠢的bug没能进互测，当时对Java的语法还有特性也不熟悉，更是没时间造评测机，整个单元都被硬控。<strong>第二单元还是很喜欢的</strong>，不过前两次也是因为一些小bug，比如未控制轿厢人数、打印顺序，强测寄的比较惨，最后bug修复却能稍微需改两三行就通过，当时简直要气死我了😤😤第三单元也还不错，不过Junit测试比较阴间，最后的第四单元后两次都是用的cursor，没时间整了。</p></li><li><p>从当初对Java，对OOP一无所知，到现在能写出一个简单的面相对象代码的小型系统，我从OO课中学到了很多，也极大提升了自己的抗压能力，<strong>我大概永远不会忘记U2通宵的那些周五的夜晚</strong>😀</p></li></ul><h1 id="其他课程"><a href="#其他课程" class="headerlink" title="其他课程"></a>其他课程</h1><ul><li>ML这门课比较难评，在这样高压的学期中再塞进来一门机器学习，并且试图在短时间内教会我们海量的内容，好在最后考试的时候还是基本拟合了往年题，<strong>不过其实我啥也没学到</strong>……</li><li>其他的可，比如航概军理等，也都是期末突击背一下，然后考试了，其实我复习的还不错，不过最后分数不尽人意，ε&#x3D;(´ο｀*)))唉</li></ul><h1 id="学习以外"><a href="#学习以外" class="headerlink" title="学习以外"></a>学习以外</h1><p>其实这学期还是有很多其他收获的</p><ul><li>实践队取得寒假实践团队一等奖</li><li>冯如杯哲社组临危受命准备PPT、参加答辩</li><li>美赛被队友带飞，五一出成绩：F奖</li><li>政治身份转变，成为一名预备dang yuan</li><li>答辩获得计院标兵团支部</li></ul><p>种种这些，让我取得了不小的进步，也收获了不一样的成长，<strong>期待明天会更好 ！</strong>😁</p><h1 id="招生活动"><a href="#招生活动" class="headerlink" title="招生活动"></a>招生活动</h1><ul><li><p>参加了泥航的湖南招生组，前几天回母校招生后我回家休息了两天，之后启程长沙，参与后阶段的招生工作，与各位亲爱的导导们一起奋战在招生一线。</p></li><li><p>我们常常忙碌到晚上九十点才吃晚饭，之后才开始一天的娱乐活动，与大家一起闲逛五一广场，漫步橘子洲头，登临岳麓山顶……<strong>各位导导们亲切友好、幽默风趣、活力十足，也深深地感染了我</strong>😊😊</p></li></ul><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><ul><li><p>其实这学期成绩很糟糕<del>（好吧，本来就挺糟糕的）</del>，也因此焦虑了不少时间，<strong>但想想后又觉得可笑</strong>，我什么时候也变成分奴了，我什么时候也变成自己曾经最讨厌的模样了？我只想借用几句话与大家共勉：</p><ul><li><strong>何须多虑盈亏事，终归小满胜万全</strong></li><li><strong>但行好事，莫问前程</strong></li></ul></li><li><p><strong>其实还有很多很多话想写，还有很多很多人和事值得写，但故事总要停在某个逗号，让未完的思绪像蒲公英般散开，飘向读者心里的那片自留地，如同暮色中渐渐隐去的山峦轮廓，留给世界一个温柔的悬念……</strong></p></li><li><p><strong>继续奔跑，继续加油吧，少年！</strong>😊😊</p></li></ul><h1 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h1><ul><li>2025.7.12  Update:  昨晚wty老师联系我说分数已更新，今天教务审核过后，OS加了5分，<strong>我宣布wty老师就是我的男神</strong>😁😆</li><li>想起之前看的斗破苍穹里的一句话：“<strong>锦上添花固然可喜，但雪中送炭方才是真正的难能可贵</strong>”</li></ul>]]></content>
    
    
    <categories>
      
      <category>日常</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BUAA-OO-Unit4</title>
    <link href="/2025/06/15/BUAA-OO-Unit4/"/>
    <url>/2025/06/15/BUAA-OO-Unit4/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是北航2025OO课程的总结，希望能对你有所帮助😊</p>          </div><blockquote><p>[!TIP]</p><p><strong>针对第四单元的三次作业和2025OO课程全部内容，撰写技术博客</strong></p><ul><li>总结本单元所实践的正向建模与开发</li><li>总结本单元作业的架构设计，并对比分析最终的代码设计和UML模型设计之间的追踪关系</li><li>根据使用大模型辅助正向建模的体验，总结分析如何引导大模型在复杂场景中完成架构设计任务</li><li>总结自己在四个单元中架构设计思维的演进</li><li>总结自己在四个单元中测试思维的演进</li><li>总结自己的课程收获</li></ul><p><strong>注意：</strong>在编写完博客并发布在CSDN之后，请在<a href="https://bbs.csdn.net/topics/619745505">对应作业处</a>选择要作为作业提交的博客，点击提交。</p></blockquote><h1 id="结束了！"><a href="#结束了！" class="headerlink" title="结束了！"></a>结束了！</h1><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/%E9%BE%99%E7%8C%AB3.jpg"></p><h1 id="本单元的正向建模与开发"><a href="#本单元的正向建模与开发" class="headerlink" title="本单元的正向建模与开发"></a>本单元的正向建模与开发</h1><ul><li>本单元的正向建模需要我们在具体写代码之前设计好架构，并由此画出类图，这需要我们在初始进行充分的思考，设计出一个相对灵活扩展性高的架构</li><li>但在具体实践过程中比较困难，因为很难在一开始就设计出比较好的架构，在实际写代码过程中还需要不断地修改完善</li></ul><h1 id="架构设计思维"><a href="#架构设计思维" class="headerlink" title="架构设计思维"></a>架构设计思维</h1><ul><li>除了Unit3，每单元的第一次作业都需要我们从0开始构建起一个基本的框架，这就需要我们在动手coding前必须要进行充分合理的设计，设计出一个友好的扩展性强的架构，以适应未来的迭代开发需求。</li><li>在我完成各单元架构设计的过程中，我一般采取”阅读指导书 &gt; 分析理清需求与目标 &gt; 阅读学长博客学习优秀架构 &gt;  根据作业变化与要求设计出自己的架构 “这样的一套流程来完成单元架构的设计，既站在了“学长的肩膀上”，也有自己独特创新的设计</li></ul><h2 id="Unit1"><a href="#Unit1" class="headerlink" title="Unit1"></a>Unit1</h2><ul><li>Unit1的架构设计我主要参考的是第一次实验以及OOpre的最后一次代码作业，为了实现递归下降解析，我沿用了实验和作业的”lexer-parser”结构，自顶向下，从表达式到项再到因子；而为了将不同的因子统一起来，归一化处理，我使用了单项式-多项式的表征方式，将各种因子转化为多项式再化简。最终设计出”Lexer &gt; Parser &gt; Poly &gt; Optimizer”的四维结构，实现文法解析到语法树解析，多项式转化到合并输出优化的流程，从而构建起来一个相对合理的递归下降与归一处理的代码架构</li></ul><h2 id="Unit2"><a href="#Unit2" class="headerlink" title="Unit2"></a>Unit2</h2><ul><li>Unit2我是基于实验代码和生产者-消费者模式搭建的，同时大胆合理地推测迭代需求，会从“指定电梯”演变为“自定义策略”，因此一开始设计架构就考虑了<code>Dispatcher</code>这一个当时看起来没有任何作用的类，同时也将电梯运行策略进行解耦，参考<a href="https://hyggge.github.io/2022/03/24/oo/oo-di-yi-dan-yuan-zong-jie/">hyggge的博客</a>学习并采用了LOOK算法，将输入、分派、决策、执行四个逻辑进行了解耦与分离。</li></ul><h2 id="Unit3"><a href="#Unit3" class="headerlink" title="Unit3"></a>Unit3</h2><ul><li>根据jml实现指定的类和方法即可，并不需要自己进行架构设计，主要关注点还是在性能、算法的优化上面</li></ul><h2 id="Unit4"><a href="#Unit4" class="headerlink" title="Unit4"></a>Unit4</h2><ul><li>第四单元我是直接根据指导书，然后自己构思了一下，将用户、书、书架、借还台等设计为单独的类，然后一点一点根据指导书和官方包完成自己的设计</li></ul><h1 id="测试思维"><a href="#测试思维" class="headerlink" title="测试思维"></a>测试思维</h1><ul><li>我的测试主要分为以下几个阶段<ul><li>写完代码后用题面数据和自己捏造的数据进行最基本的测试</li><li>利用中测数据进行进一步的测试</li><li>利用评测机对程序进行更加全面的测试</li></ul></li><li>这里主要说一说我的<strong>数据构造策略</strong></li><li>纯随机生成或者略加限定的随机生成，生成的数据强度低，无法<strong>测试极限情况、边界情况</strong>，也无法对程序进行<strong>压力测试</strong>，检验程序的性能，因此我的数据生成会<strong>设定一定的逻辑</strong>。</li><li>根据不同单元的特点选择验证正确性的程序，比如Unit1可以借助现有的<code>sympy</code>库来进行化简比较，然后和同学对拍即可；Unit2的输出因人而异，无法对拍，因此需要自行设计验证程序来判断输出是否正确；Unit3的输出基本固定，可以直接和好友进行对拍。</li><li>为了便于修改数据参数（数据条数，各种常数限制等），我会选择将这些常数定义在文件的开头</li></ul><h2 id="Unit2-1"><a href="#Unit2-1" class="headerlink" title="Unit2"></a>Unit2</h2><h3 id="常规测试"><a href="#常规测试" class="headerlink" title="常规测试"></a>常规测试</h3><ul><li>我选择分组多次生成，每组生成8-12条请求，组内请求间隔随机在1s内，组间间隔随机在0.1 - 6s内</li><li>对于每一组数据，我设置了几种场景，通过指定不同的概率来随机选择场景进行生成<ul><li><code>high_concurrent</code>：高并发，控制这一组的请求全部在同一时间投放</li><li><code>high_concentration</code>：高集中：所有请求来自同一楼层或者前往同一楼层</li><li><code>high_burst</code>：将5部电梯全部临时调度，然后生成其他请求</li><li><code>random</code>：随机选择请求</li><li><code>multi_update</code>：进行3次改造，然后生成其他请求</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">SCENARIO_PROBS = &#123;<br>    <span class="hljs-string">&#x27;high_concurrent&#x27;</span>: <span class="hljs-number">0.3</span>,<br>    <span class="hljs-string">&#x27;high_concentration&#x27;</span>: <span class="hljs-number">0.2</span>,<br>    <span class="hljs-string">&#x27;high_burst&#x27;</span>: <span class="hljs-number">0.15</span>,<br>    <span class="hljs-string">&#x27;random&#x27;</span>: <span class="hljs-number">0.2</span>,<br>    <span class="hljs-string">&#x27;multi_update&#x27;</span>: <span class="hljs-number">0.15</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="极端测试"><a href="#极端测试" class="headerlink" title="极端测试"></a>极端测试</h3><ul><li><strong>针对高并发场景</strong>，我还设计了另外一个数据生成器，全部请求在同一时间或者间隔不超过2s内给出，以模拟真实场景下的高并发挑战</li><li>后来发现，强测数据中也有类似的专门针对高并发的测试点</li></ul><h3 id="并行测试"><a href="#并行测试" class="headerlink" title="并行测试"></a>并行测试</h3><ul><li>因为一次测试的时间往往在50s以上，甚至高达90s，所以单个case串行测试的效率极低，我们需要用多线程来测试我们的多线程程序</li><li>因为程序运行严格依赖<code>datainput_student_win64</code>，我是采用自动化流程建立十个子文件夹，然后启动十个线程分别在子文件夹下进行测试</li></ul><h2 id="Unit3-1"><a href="#Unit3-1" class="headerlink" title="Unit3"></a>Unit3</h2><ul><li>除了常规的有限制的随机数据外，我重点针对需要优化的地方设计了几种测试模式</li><li><code>special_qtvs</code>：专门针对<code>qtvs</code>的生成模式。首先创建用户，接着建立所有其他人与1号人物的关系，并随机添加一些关系，然后全部加入1号人物的标签中，然后分2阶段测试，第一阶段在网络中随机修改关系，添加关系，小概率查询qtvs；第二阶段，添加了指令<code>dft</code>，以及增加了查询<code>qtvs</code> 的概率。</li><li><code>special_qba</code>：专门针对<code>qba</code>的测试。首先建立一个总人数20%的核心用户群，然后在核心用户中每个用户随机添加2-3个关系，然后开始第二阶段，混合测试：40%概率执行qba查询，并且80%概率查询核心用户，20%概率查询其他用户；30%概率新增或修改核心用户与其他用户之间的关系；30%概率新增用户和关系值，</li><li><code>special_account</code>：第一阶段：添加用户和关系；第二阶段：添加公众号，并添加一些关注和文章；第三阶段：海量的数据来随机进行以下几种操作：贡献文章、删除文章、添加关注、删除公众号、查询接受文章、查询最佳贡献者</li></ul><h2 id="Unit4-1"><a href="#Unit4-1" class="headerlink" title="Unit4"></a>Unit4</h2><ul><li>摆了，用的别人的评测机</li></ul><h1 id="辅助工具"><a href="#辅助工具" class="headerlink" title="辅助工具"></a>辅助工具</h1><h2 id="大模型"><a href="#大模型" class="headerlink" title="大模型"></a>大模型</h2><ul><li>在学习OO的过程中，可以<strong>结合大模型来辅助自己</strong>，比如我在Unit3中会将大段复杂晦涩的<code>jml</code>描述投喂给大模型，让其进行分析与解释，也会通过大模型来学习一些知识：比如并查集、双向BFS等</li><li>大模型对于明确的给定任务完成地相当出色，比如说：请你告诉我什么是<code>Dijkstra</code>算法并用java语言给出一个示例，这种任务大模型能够非常迅速地给出准确答案</li><li>大模型理解<strong>复杂业务需求</strong>的能力有限，比如你直接将作业指导书给它，那么它生成的程序会有很大的漏洞，但如果你是将一个类的jml描述给它，那么它生成的代码的正确性则会有较大的提升</li><li>当然，现在有很强的编程辅助智能体：cursor + claude4，编程能力疑似超越绝大部分程序猿，不过很多公司也不允许使用，其他的大模型也是不允许用的，原因之一在于保护业务的安全，防止数据泄露；另一方面，对于一些大型的代码，AI出手可能会适得其反</li></ul><h2 id="github"><a href="#github" class="headerlink" title="github"></a>github</h2><ul><li>汇聚了全世界开发者的开源社区，里面能找到一些感兴趣的项目，以及学长的代码，甚至有评测机</li><li>维护好自己的github账号，可以将自己的代码和评测机也上传上去，实现开源与传承</li></ul><h1 id="课程收获"><a href="#课程收获" class="headerlink" title="课程收获"></a>课程收获</h1><ul><li>从当初OOpre课程对J面向对象<strong>一无所知</strong>到现在学习了解了许多新知识，比如：封装继承多态<strong>三大特性</strong>、递归下降、多线程编程、规格化设计、契约式编程等，一路走来，我既收获了知识，也享受这份<strong>学习的乐趣</strong>😁</li><li>抗压能力大幅提升，第二单元作业每周周五熬夜debug，第二次作业占据了我整个清明假期，第三次作业debug到了第二天早上5点，这些场景我<strong>终生难忘</strong>😀</li><li>学习OO的过程，我也结识了许多朋友与优秀的学长学姐，这种收获是<strong>无价之宝</strong></li><li>一路走来，我也收获了对编程对开发的热爱，面向对象不仅仅是一种开发方式，更是一种思维方式，它将指导在编程的道路上继续奋勇前进，勇往直前！</li></ul><h1 id="鸣谢"><a href="#鸣谢" class="headerlink" title="鸣谢"></a>鸣谢</h1><ul><li>感谢课程组的老师与助教们的辛勤付出❤️</li></ul>]]></content>
    
    
    <categories>
      
      <category>OO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>OO</tag>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UNIX系统复习-4</title>
    <link href="/2025/05/31/UNIX%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0-4/"/>
    <url>/2025/05/31/UNIX%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0-4/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我复习《UNIX系统》这门课程时记录的一些笔记 ，希望能对你有所帮助😊</p>          </div><h1 id="Shell编程"><a href="#Shell编程" class="headerlink" title="Shell编程"></a>Shell编程</h1><h2 id="一、Shell编程的意义"><a href="#一、Shell编程的意义" class="headerlink" title="一、Shell编程的意义"></a>一、Shell编程的意义</h2><ol><li><p><strong>将命令组合成实用工具</strong></p><ul><li>示例：创建显示目录的脚本<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># lsdir脚本内容</span><br><span class="hljs-built_in">ls</span> -l | sed -n <span class="hljs-string">&#x27;/^d/p&#x27;</span><br> <br><span class="hljs-comment"># 使用方式</span><br><span class="hljs-built_in">chmod</span> +x lsdir<br>./lsdir<br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>快速编写实用软件</strong></p><ul><li>示例：带行号的文件查看器<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># mycat脚本内容</span><br>awk <span class="hljs-string">&#x27;&#123;print NR, &quot;: &quot;, $0&#125;&#x27;</span> <span class="hljs-variable">$1</span><br> <br><span class="hljs-comment"># 使用方式</span><br><span class="hljs-built_in">chmod</span> +x mycat<br>./mycat filename<br></code></pre></td></tr></table></figure></li></ul></li></ol><h2 id="二、位置参数"><a href="#二、位置参数" class="headerlink" title="二、位置参数"></a>二、位置参数</h2><ol><li><p><strong>基本位置参数</strong></p><ul><li><code>$0</code>：脚本名称</li><li><code>$1</code>, <code>$2</code>, …：第1、2个参数等</li><li><code>$#</code>：参数个数</li><li><code>$*</code>：所有参数集合</li></ul></li><li><p><strong>参数重置与移动</strong></p><ul><li>使用<code>set</code>重置参数：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">set</span> new1 new2  <span class="hljs-comment"># 重置$1为new1，$2为new2</span><br></code></pre></td></tr></table></figure></li><li>使用<code>shift</code>移动参数：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">shift</span>    <span class="hljs-comment"># 左移1位，丢弃$1</span><br><span class="hljs-built_in">shift</span> 2  <span class="hljs-comment"># 左移2位</span><br></code></pre></td></tr></table></figure></li></ul></li></ol><h2 id="三、Shell命令行结构"><a href="#三、Shell命令行结构" class="headerlink" title="三、Shell命令行结构"></a>三、Shell命令行结构</h2><ol><li><p><strong>命令组合</strong></p><ul><li>顺序执行：<code>cmd1; cmd2</code></li><li>管道：<code>cmd1 | cmd2</code></li><li>后台执行：<code>cmd &amp;</code></li></ul></li><li><p><strong>命令分组</strong></p><ul><li>子shell执行：<code>(cmd1; cmd2)</code></li><li>当前shell执行：<code>&#123; cmd1; cmd2; &#125;</code></li></ul></li></ol><h2 id="四、命令行模式与元字符"><a href="#四、命令行模式与元字符" class="headerlink" title="四、命令行模式与元字符"></a>四、命令行模式与元字符</h2><table><thead><tr><th>元字符</th><th>功能描述</th></tr></thead><tbody><tr><td><code>&gt;</code></td><td>输出重定向</td></tr><tr><td><code>&gt;&gt;</code></td><td>追加输出</td></tr><tr><td><code>&lt;</code></td><td>输入重定向</td></tr><tr><td><code>|</code></td><td>管道</td></tr><tr><td><code>*</code></td><td>通配符</td></tr><tr><td><code>?</code></td><td>单字符匹配</td></tr><tr><td><code>[a-z]</code></td><td>字符范围匹配</td></tr><tr><td><code>;</code></td><td>命令分隔符</td></tr><tr><td><code>&amp;</code></td><td>后台执行</td></tr><tr><td><code>`</code></td><td>命令替换</td></tr><tr><td><code>$()</code></td><td>命令替换(推荐)</td></tr><tr><td><code>#</code></td><td>注释</td></tr></tbody></table><h2 id="五、引号使用"><a href="#五、引号使用" class="headerlink" title="五、引号使用"></a>五、引号使用</h2><ol><li><p><strong>单引号</strong>：完全原样输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;$USER `date`&#x27;</span>  <span class="hljs-comment"># 输出: $USER `date`</span><br></code></pre></td></tr></table></figure></li><li><p><strong>双引号</strong>：允许变量和命令替换</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$USER</span> `date`&quot;</span>  <span class="hljs-comment"># 输出当前用户和日期</span><br></code></pre></td></tr></table></figure></li><li><p><strong>反引号</strong>：命令替换(已过时，推荐使用$())</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Today is `date`&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Today is <span class="hljs-subst">$(date)</span>&quot;</span>  <span class="hljs-comment"># 推荐写法</span><br></code></pre></td></tr></table></figure></li></ol><h2 id="六、重定向"><a href="#六、重定向" class="headerlink" title="六、重定向"></a>六、重定向</h2><ol><li><p><strong>标准重定向</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">cmd &gt; file    <span class="hljs-comment"># 输出重定向</span><br>cmd &gt;&gt; file   <span class="hljs-comment"># 追加输出</span><br>cmd &lt; file    <span class="hljs-comment"># 输入重定向</span><br></code></pre></td></tr></table></figure></li><li><p><strong>错误重定向</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">cmd 2&gt; error.log          <span class="hljs-comment"># 错误输出到文件</span><br>cmd &gt; output.log 2&gt;&amp;1     <span class="hljs-comment"># 标准和错误都输出到文件</span><br>cmd &amp;&gt; all_output.log     <span class="hljs-comment"># 同上(更简洁)</span><br></code></pre></td></tr></table></figure></li><li><p><strong>Here Document</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> &lt;&lt;<span class="hljs-string">EOF</span><br><span class="hljs-string">多行文本</span><br><span class="hljs-string">可以直接输入</span><br><span class="hljs-string">EOF</span><br></code></pre></td></tr></table></figure></li></ol><h2 id="七、Shell程序结构"><a href="#七、Shell程序结构" class="headerlink" title="七、Shell程序结构"></a>七、Shell程序结构</h2><ol><li><strong>基本结构</strong>：命令序列</li><li><strong>主要命令类型</strong>：<ul><li>普通命令</li><li>赋值命令</li><li>运算命令</li><li>流程控制(if&#x2F;case&#x2F;for&#x2F;while)</li></ul></li></ol><h2 id="八、流程控制"><a href="#八、流程控制" class="headerlink" title="八、流程控制"></a>八、流程控制</h2><ol><li><p><strong>if语句</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">if</span> [ condition ]; <span class="hljs-keyword">then</span><br>  commands<br><span class="hljs-keyword">elif</span> [ condition ]; <span class="hljs-keyword">then</span><br>  commands<br><span class="hljs-keyword">else</span><br>  commands<br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure></li><li><p><strong>case语句</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">case</span> <span class="hljs-variable">$var</span> <span class="hljs-keyword">in</span><br>  pattern1) commands ;;<br>  pattern2) commands ;;<br>  *) default commands ;;<br><span class="hljs-keyword">esac</span><br></code></pre></td></tr></table></figure></li><li><p><strong>for循环</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 形式1</span><br><span class="hljs-keyword">for</span> var <span class="hljs-keyword">in</span> list; <span class="hljs-keyword">do</span><br>  commands<br><span class="hljs-keyword">done</span><br> <br><span class="hljs-comment"># 形式2(C风格)</span><br><span class="hljs-keyword">for</span> ((i=<span class="hljs-number">0</span>; i&lt;<span class="hljs-number">10</span>; i++)); <span class="hljs-keyword">do</span><br>  commands<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure></li><li><p><strong>while循环</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">while</span> [ condition ]; <span class="hljs-keyword">do</span><br>  commands<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure></li><li><p><strong>until循环</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">until</span> [ condition ]; <span class="hljs-keyword">do</span><br>  commands<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure></li></ol><h2 id="九、test命令与条件判断"><a href="#九、test命令与条件判断" class="headerlink" title="九、test命令与条件判断"></a>九、test命令与条件判断</h2><p>test </p><ol><li><p><strong>文件测试</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[ -e file ]    <span class="hljs-comment"># 文件存在</span><br>[ -f file ]    <span class="hljs-comment"># 是普通文件</span><br>[ -d file ]    <span class="hljs-comment"># 是目录</span><br>[ -s file ]    <span class="hljs-comment"># 文件非空</span><br></code></pre></td></tr></table></figure></li><li><p><strong>字符串测试</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[ -z <span class="hljs-string">&quot;<span class="hljs-variable">$str</span>&quot;</span> ]  <span class="hljs-comment"># 字符串为空</span><br>[ -n <span class="hljs-string">&quot;<span class="hljs-variable">$str</span>&quot;</span> ]  <span class="hljs-comment"># 字符串非空</span><br>[ <span class="hljs-string">&quot;<span class="hljs-variable">$a</span>&quot;</span> = <span class="hljs-string">&quot;<span class="hljs-variable">$b</span>&quot;</span> ] <span class="hljs-comment"># 字符串相等</span><br></code></pre></td></tr></table></figure></li><li><p><strong>数值比较</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[ <span class="hljs-variable">$a</span> -eq <span class="hljs-variable">$b</span> ]  <span class="hljs-comment"># 等于</span><br>[ <span class="hljs-variable">$a</span> -ne <span class="hljs-variable">$b</span> ]  <span class="hljs-comment"># 不等于</span><br>[ <span class="hljs-variable">$a</span> -gt <span class="hljs-variable">$b</span> ]  <span class="hljs-comment"># 大于</span><br>[ <span class="hljs-variable">$a</span> -lt <span class="hljs-variable">$b</span> ]  <span class="hljs-comment"># 小于</span><br></code></pre></td></tr></table></figure></li></ol><h2 id="十、Shell内部变量"><a href="#十、Shell内部变量" class="headerlink" title="十、Shell内部变量"></a>十、Shell内部变量</h2><table><thead><tr><th>变量</th><th>描述</th></tr></thead><tbody><tr><td><code>$?</code></td><td>上条命令退出状态</td></tr><tr><td><code>$$</code></td><td>当前shell的PID</td></tr><tr><td><code>$!</code></td><td>最后后台进程PID</td></tr><tr><td><code>$#</code></td><td>参数个数</td></tr><tr><td><code>$@</code></td><td>所有参数(保留引号)</td></tr><tr><td><code>$*</code></td><td>所有参数(不保留引号)</td></tr><tr><td><code>$_</code></td><td>上条命令的最后一个参数</td></tr></tbody></table><h2 id="十一、实用示例"><a href="#十一、实用示例" class="headerlink" title="十一、实用示例"></a>十一、实用示例</h2><ol><li><p><strong>查找命令路径(mywhich)</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><span class="hljs-keyword">case</span> <span class="hljs-variable">$#</span> <span class="hljs-keyword">in</span><br>  0) <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Usage: <span class="hljs-variable">$0</span> cmd&quot;</span>; <span class="hljs-built_in">exit</span> 1 ;;<br><span class="hljs-keyword">esac</span><br> <br>IFS=:<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">dir</span> <span class="hljs-keyword">in</span> <span class="hljs-variable">$PATH</span>; <span class="hljs-keyword">do</span><br>  <span class="hljs-keyword">if</span> [ -x <span class="hljs-string">&quot;<span class="hljs-variable">$dir</span>/<span class="hljs-variable">$1</span>&quot;</span> ]; <span class="hljs-keyword">then</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$dir</span>/<span class="hljs-variable">$1</span>&quot;</span><br>    <span class="hljs-built_in">exit</span> 0<br>  <span class="hljs-keyword">fi</span><br><span class="hljs-keyword">done</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>: not found&quot;</span><br><span class="hljs-built_in">exit</span> 1<br></code></pre></td></tr></table></figure></li><li><p><strong>倒序显示参数</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><span class="hljs-keyword">for</span> ((i=<span class="hljs-variable">$#</span>; i&gt;<span class="hljs-number">0</span>; i--)); <span class="hljs-keyword">do</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;!i&#125;</span>  <span class="hljs-comment"># 间接引用</span><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure></li></ol><h2 id="十二、Shell编程优缺点"><a href="#十二、Shell编程优缺点" class="headerlink" title="十二、Shell编程优缺点"></a>十二、Shell编程优缺点</h2><p><strong>优点</strong>：</p><ul><li>快速组合现有命令</li><li>无需编译，修改方便</li><li>适合系统管理任务</li></ul><p><strong>缺点</strong>：</p><ul><li>执行效率较低</li><li>复杂逻辑可读性差</li><li>可移植性受限</li></ul><h2 id="最佳实践建议"><a href="#最佳实践建议" class="headerlink" title="最佳实践建议"></a>最佳实践建议</h2><ol><li>总在第一行指定解释器：<code>#!/bin/bash</code></li><li>使用<code>$()</code>代替反引号进行命令替换</li><li>变量引用加双引号：<code>&quot;$var&quot;</code></li><li>使用<code>[[ ]]</code>代替<code>[ ]</code>进行条件测试(更强大)</li><li>为脚本添加帮助信息和错误处理</li><li>复杂脚本考虑使用Python等更强大的语言</li></ol><p>通过掌握这些核心知识，您可以编写出高效、可靠的Shell脚本，完成各种系统管理和自动化任务。</p><h1 id="C语言开发环境"><a href="#C语言开发环境" class="headerlink" title="C语言开发环境"></a>C语言开发环境</h1><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><ol><li><p><strong>C语言与UNIX的关系</strong></p><ul><li>1973年由Dennis Ritchie发明，用于重写UNIX系统</li><li>成为UNIX系统的”自然”语言</li><li>1988年IEEE推出POSIX标准，统一了C语言头文件</li></ul></li><li><p><strong>C语言优势</strong></p><ul><li>高效性：直接操作硬件</li><li>可移植性：POSIX标准确保跨平台兼容性</li><li>广泛应用：系统编程、嵌入式开发首选语言</li></ul></li></ol><h2 id="二、C程序实例分析"><a href="#二、C程序实例分析" class="headerlink" title="二、C程序实例分析"></a>二、C程序实例分析</h2><h3 id="1-基本结构"><a href="#1-基本结构" class="headerlink" title="1. 基本结构"></a>1. 基本结构</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">/* 注释：传统第一个C程序 */</span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span>  <span class="hljs-comment">// 预编译指令：包含标准I/O头文件</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">(<span class="hljs-type">int</span> argc, <span class="hljs-type">char</span> *argv[])</span> &#123;  <span class="hljs-comment">// 主函数，程序入口</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Hello World!\n&quot;</span>);      <span class="hljs-comment">// 输出语句</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;                      <span class="hljs-comment">// 返回值(0表示成功)</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="2-关键语法解析"><a href="#2-关键语法解析" class="headerlink" title="2. 关键语法解析"></a>2. 关键语法解析</h3><ol><li><p><strong>注释</strong></p><ul><li><code>/* ... */</code>：多行注释</li><li><code>//</code>：单行注释(C99标准)</li></ul></li><li><p><strong>预编译指令</strong></p><ul><li><code>#include &lt;头文件&gt;</code>：从系统路径(&#x2F;usr&#x2F;include)加载</li><li><code>#include &quot;头文件&quot;</code>：从当前目录加载</li></ul></li><li><p><strong>主函数</strong></p><ul><li>必须的入口函数</li><li>参数：<ul><li><code>argc</code>：参数个数</li><li><code>argv[]</code>：参数数组(argv[0]为程序名)</li></ul></li><li>返回值：<ul><li>0表示成功</li><li>非0表示错误(便于Shell脚本判断)</li></ul></li></ul></li></ol><h2 id="三、C编译过程"><a href="#三、C编译过程" class="headerlink" title="三、C编译过程"></a>三、C编译过程</h2><h3 id="1-四个阶段"><a href="#1-四个阶段" class="headerlink" title="1. 四个阶段"></a>1. 四个阶段</h3><ol><li><p><strong>预处理</strong></p><ul><li>处理<code>#include</code>、<code>#define</code>等指令</li><li>生成<code>.i</code>文件</li></ul></li><li><p><strong>编译</strong></p><ul><li>词法&#x2F;语法分析</li><li>生成汇编代码(.s文件)</li></ul></li><li><p><strong>汇编</strong></p><ul><li>将汇编代码转为机器码</li><li>生成目标文件(.o)</li></ul></li><li><p><strong>链接</strong></p><ul><li>合并多个目标文件</li><li>生成可执行程序</li></ul></li></ol><h3 id="2-常用GCC命令"><a href="#2-常用GCC命令" class="headerlink" title="2. 常用GCC命令"></a>2. 常用GCC命令</h3><table><thead><tr><th>命令</th><th>功能</th></tr></thead><tbody><tr><td><code>gcc hello.c</code></td><td>生成a.out</td></tr><tr><td><code>gcc -o hello hello.c</code></td><td>指定输出文件名</td></tr><tr><td><code>gcc -c file.c</code></td><td>仅编译生成.o文件</td></tr><tr><td><code>gcc -g program.c</code></td><td>加入调试信息</td></tr></tbody></table><h2 id="四、调试工具GDB"><a href="#四、调试工具GDB" class="headerlink" title="四、调试工具GDB"></a>四、调试工具GDB</h2><h3 id="1-核心功能"><a href="#1-核心功能" class="headerlink" title="1. 核心功能"></a>1. 核心功能</h3><ul><li>设置断点</li><li>单步执行</li><li>查看变量值</li><li>修改程序状态</li></ul><h3 id="2-常用命令"><a href="#2-常用命令" class="headerlink" title="2. 常用命令"></a>2. 常用命令</h3><table><thead><tr><th>命令</th><th>功能</th></tr></thead><tbody><tr><td><code>gdb ./a.out</code></td><td>启动调试</td></tr><tr><td><code>break main</code></td><td>在main函数设断点</td></tr><tr><td><code>run</code></td><td>运行程序</td></tr><tr><td><code>print x</code></td><td>查看变量x的值</td></tr><tr><td><code>next</code></td><td>单步执行(不进入函数)</td></tr><tr><td><code>quit</code></td><td>退出GDB</td></tr></tbody></table><h2 id="五、工程管理工具Make"><a href="#五、工程管理工具Make" class="headerlink" title="五、工程管理工具Make"></a>五、工程管理工具Make</h2><h3 id="1-Makefile规则"><a href="#1-Makefile规则" class="headerlink" title="1. Makefile规则"></a>1. Makefile规则</h3><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">目标文件: 依赖文件1 依赖文件2 ...</span><br>    &lt;tab&gt;命令1<br>    &lt;tab&gt;命令2<br></code></pre></td></tr></table></figure><h3 id="2-示例"><a href="#2-示例" class="headerlink" title="2. 示例"></a>2. 示例</h3><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">app: main.o utils.o</span><br>    gcc -o app main.o utils.o<br><br><span class="hljs-section">main.o: main.c</span><br>    gcc -c main.c<br><br><span class="hljs-section">utils.o: utils.c</span><br>    gcc -c utils.c<br><br><span class="hljs-section">clean:</span><br>    rm -f *.o app<br><span class="hljs-meta"><span class="hljs-keyword">.PHONY</span>: clean</span><br></code></pre></td></tr></table></figure><h3 id="3-重要概念"><a href="#3-重要概念" class="headerlink" title="3. 重要概念"></a>3. 重要概念</h3><ol><li><p><strong>变量</strong></p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs makefile">OBJS = main.o utils.o<br><span class="hljs-section">app: <span class="hljs-variable">$(OBJS)</span></span><br>    gcc -o app <span class="hljs-variable">$(OBJS)</span><br></code></pre></td></tr></table></figure></li><li><p><strong>隐含规则</strong></p><ul><li>自动推导依赖关系</li><li>如<code>.o</code>文件自动从<code>.c</code>文件生成</li></ul></li><li><p><strong>伪目标</strong></p><ul><li>用于非编译任务(如clean)</li><li>需用<code>.PHONY</code>声明</li></ul></li></ol><h2 id="六、标准C函数"><a href="#六、标准C函数" class="headerlink" title="六、标准C函数"></a>六、标准C函数</h2><h3 id="1-系统调用-vs-库函数"><a href="#1-系统调用-vs-库函数" class="headerlink" title="1. 系统调用 vs 库函数"></a>1. 系统调用 vs 库函数</h3><table><thead><tr><th>特性</th><th>系统调用</th><th>库函数</th></tr></thead><tbody><tr><td>提供者</td><td>操作系统内核</td><td>编程语言</td></tr><tr><td>功能</td><td>基础接口</td><td>高级封装</td></tr><tr><td>示例</td><td>write()</td><td>printf()</td></tr><tr><td>可替换性</td><td>不可替换</td><td>可替换</td></tr></tbody></table><h3 id="2-UNIX体系结构"><a href="#2-UNIX体系结构" class="headerlink" title="2. UNIX体系结构"></a>2. UNIX体系结构</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">应用程序 → 库函数 → 系统调用 → 内核<br></code></pre></td></tr></table></figure><h3 id="3-标准化"><a href="#3-标准化" class="headerlink" title="3. 标准化"></a>3. 标准化</h3><ol><li><p><strong>ISO C</strong></p><ul><li>C90(1989)、C99(1999)</li><li>定义24个标准头文件</li></ul></li><li><p><strong>POSIX</strong></p><ul><li>增强UNIX系统可移植性</li><li>定义26个必需头文件</li></ul></li><li><p><strong>SUS</strong></p><ul><li>单一UNIX规范</li><li>扩展POSIX功能</li></ul></li></ol><h1 id="系统调用"><a href="#系统调用" class="headerlink" title="系统调用"></a>系统调用</h1><h2 id="一、文件I-O（文件操作）"><a href="#一、文件I-O（文件操作）" class="headerlink" title="一、文件I&#x2F;O（文件操作）"></a><strong>一、文件I&#x2F;O（文件操作）</strong></h2><h3 id="1-文件描述符（File-Descriptor-fd）"><a href="#1-文件描述符（File-Descriptor-fd）" class="headerlink" title="1. 文件描述符（File Descriptor, fd）"></a><strong>1. 文件描述符（File Descriptor, fd）</strong></h3><ul><li><strong>定义</strong>：非负整数，用于标识进程打开的文件。</li><li><strong>默认文件描述符</strong>：<ul><li><code>0</code>：标准输入（<code>STdin_FILENO</code>，键盘输入）。</li><li><code>1</code>：标准输出（<code>STDOUT_FILENO</code>，屏幕输出）。</li><li><code>2</code>：标准错误（<code>STDERR_FILENO</code>，屏幕错误输出）。</li></ul></li></ul><h4 id="示例：读取标准输入并写入标准输出"><a href="#示例：读取标准输入并写入标准输出" class="headerlink" title="示例：读取标准输入并写入标准输出"></a><strong>示例：读取标准输入并写入标准输出</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-type">char</span> buf[<span class="hljs-number">1024</span>];<br>    <span class="hljs-type">ssize_t</span> n = read(STDIN_FILENO, buf, <span class="hljs-keyword">sizeof</span>(buf));  <span class="hljs-comment">// 从键盘读取</span><br>    write(STDOUT_FILENO, buf, n);                      <span class="hljs-comment">// 输出到屏幕</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>解释</strong>：</p><ul><li><code>read(fd, buf, size)</code>：从 <code>fd</code> 读取最多 <code>size</code> 字节到 <code>buf</code>，返回实际读取字节数。</li><li><code>write(fd, buf, size)</code>：将 <code>buf</code> 的 <code>size</code> 字节写入 <code>fd</code>。</li></ul><hr><h3 id="2-open-和-creat"><a href="#2-open-和-creat" class="headerlink" title="2. open() 和 creat()"></a><strong>2. <code>open()</code> 和 <code>creat()</code></strong></h3><h4 id="open-函数"><a href="#open-函数" class="headerlink" title="open() 函数"></a><strong><code>open()</code> 函数</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;fcntl.h&gt;</span></span><br><span class="hljs-type">int</span> <span class="hljs-title function_">open</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">char</span> *pathname, <span class="hljs-type">int</span> flags, <span class="hljs-type">mode_t</span> mode)</span>;<br></code></pre></td></tr></table></figure><ul><li>**<code>flags</code>**：<ul><li><code>O_RDONLY</code>：只读。</li><li><code>O_WRONLY</code>：只写。</li><li><code>O_RDWR</code>：读写。</li><li><code>O_CREAT</code>：文件不存在时创建。</li><li><code>O_TRUNC</code>：若文件存在，清空内容。</li><li><code>O_APPEND</code>：追加写入（避免并发问题）。</li></ul></li></ul><h4 id="示例：打开-创建文件"><a href="#示例：打开-创建文件" class="headerlink" title="示例：打开&#x2F;创建文件"></a><strong>示例：打开&#x2F;创建文件</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;fcntl.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-type">int</span> fd = open(<span class="hljs-string">&quot;test.txt&quot;</span>, O_WRONLY | O_CREAT | O_TRUNC, <span class="hljs-number">0644</span>);<br>    write(fd, <span class="hljs-string">&quot;Hello, World!\n&quot;</span>, <span class="hljs-number">14</span>);<br>    close(fd);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>解释</strong>：</p><ul><li><code>O_CREAT | O_TRUNC</code>：若文件不存在则创建，若存在则清空。</li><li><code>0644</code>：文件权限（<code>rw-r--r--</code>）。</li></ul><hr><h3 id="3-lseek-：移动文件指针"><a href="#3-lseek-：移动文件指针" class="headerlink" title="3. lseek()：移动文件指针"></a><strong>3. <code>lseek()</code>：移动文件指针</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-type">off_t</span> <span class="hljs-title function_">lseek</span><span class="hljs-params">(<span class="hljs-type">int</span> fd, <span class="hljs-type">off_t</span> offset, <span class="hljs-type">int</span> whence)</span>;<br></code></pre></td></tr></table></figure><ul><li>**<code>whence</code>**：<ul><li><code>SEEK_SET</code>：从文件开头计算偏移。</li><li><code>SEEK_CUR</code>：从当前位置计算偏移。</li><li><code>SEEK_END</code>：从文件末尾计算偏移。</li></ul></li></ul><h4 id="示例：读取文件第-10-字节"><a href="#示例：读取文件第-10-字节" class="headerlink" title="示例：读取文件第 10 字节"></a><strong>示例：读取文件第 10 字节</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;fcntl.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-type">int</span> fd = open(<span class="hljs-string">&quot;test.txt&quot;</span>, O_RDONLY);<br>    lseek(fd, <span class="hljs-number">10</span>, SEEK_SET);  <span class="hljs-comment">// 移动到第 10 字节</span><br>    <span class="hljs-type">char</span> buf[<span class="hljs-number">1</span>];<br>    read(fd, buf, <span class="hljs-number">1</span>);         <span class="hljs-comment">// 读取 1 字节</span><br>    close(fd);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><hr><h3 id="4-dup2-：文件描述符重定向"><a href="#4-dup2-：文件描述符重定向" class="headerlink" title="4. dup2()：文件描述符重定向"></a><strong>4. <code>dup2()</code>：文件描述符重定向</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-type">int</span> <span class="hljs-title function_">dup2</span><span class="hljs-params">(<span class="hljs-type">int</span> oldfd, <span class="hljs-type">int</span> newfd)</span>;<br></code></pre></td></tr></table></figure><ul><li><strong>作用</strong>：让 <code>newfd</code> 指向 <code>oldfd</code> 的文件。</li></ul><h4 id="示例：将标准输出重定向到文件"><a href="#示例：将标准输出重定向到文件" class="headerlink" title="示例：将标准输出重定向到文件"></a><strong>示例：将标准输出重定向到文件</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;fcntl.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-type">int</span> fd = open(<span class="hljs-string">&quot;output.txt&quot;</span>, O_WRONLY | O_CREAT | O_TRUNC, <span class="hljs-number">0644</span>);<br>    dup2(fd, STDOUT_FILENO);  <span class="hljs-comment">// 标准输出指向文件</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;This goes to output.txt!\n&quot;</span>);  <span class="hljs-comment">// 写入文件而非屏幕</span><br>    close(fd);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><hr><h2 id="文件属性及目录"><a href="#文件属性及目录" class="headerlink" title="文件属性及目录"></a>文件属性及目录</h2><h3 id="一、文件属性（struct-stat）"><a href="#一、文件属性（struct-stat）" class="headerlink" title="一、文件属性（struct stat）"></a><strong>一、文件属性（<code>struct stat</code>）</strong></h3><h3 id="1-属性结构体"><a href="#1-属性结构体" class="headerlink" title="1. 属性结构体"></a><strong>1. 属性结构体</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sys/stat.h&gt;</span></span><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">stat</span> &#123;</span><br>    <span class="hljs-type">mode_t</span>  st_mode;   <span class="hljs-comment">// 文件类型和权限（核心字段）</span><br>    <span class="hljs-type">ino_t</span>   st_ino;    <span class="hljs-comment">// inode 号（唯一标识文件）</span><br>    <span class="hljs-type">dev_t</span>   st_dev;    <span class="hljs-comment">// 文件所在设备的 ID</span><br>    <span class="hljs-type">dev_t</span>   st_rdev;   <span class="hljs-comment">// 设备文件的设备号（仅特殊文件有效）</span><br>    <span class="hljs-type">nlink_t</span> st_nlink;  <span class="hljs-comment">// 硬链接计数</span><br>    <span class="hljs-type">uid_t</span>   st_uid;    <span class="hljs-comment">// 所有者用户 ID</span><br>    <span class="hljs-type">gid_t</span>   st_gid;    <span class="hljs-comment">// 所有者组 ID</span><br>    <span class="hljs-type">off_t</span>   st_size;   <span class="hljs-comment">// 文件大小（字节）</span><br>    <span class="hljs-type">time_t</span>  st_atime;  <span class="hljs-comment">// 最后访问时间</span><br>    <span class="hljs-type">time_t</span>  st_mtime;  <span class="hljs-comment">// 最后修改内容时间</span><br>    <span class="hljs-type">time_t</span>  st_ctime;  <span class="hljs-comment">// 最后修改属性时间（如权限）</span><br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="2-获取文件属性"><a href="#2-获取文件属性" class="headerlink" title="2. 获取文件属性"></a><strong>2. 获取文件属性</strong></h3><ul><li>**<code>stat()</code>**：通过路径获取属性。<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">stat</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">char</span> *pathname, <span class="hljs-keyword">struct</span> stat *buf)</span>;<br></code></pre></td></tr></table></figure></li><li>**<code>fstat()</code>**：通过文件描述符获取属性。<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">fstat</span><span class="hljs-params">(<span class="hljs-type">int</span> fd, <span class="hljs-keyword">struct</span> stat *buf)</span>;<br></code></pre></td></tr></table></figure></li></ul><h4 id="示例：打印文件大小"><a href="#示例：打印文件大小" class="headerlink" title="示例：打印文件大小"></a><strong>示例：打印文件大小</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sys/stat.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">stat</span> <span class="hljs-title">buf</span>;</span><br>    stat(<span class="hljs-string">&quot;test.txt&quot;</span>, &amp;buf);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;File size: %ld bytes\n&quot;</span>, buf.st_size);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><hr><h3 id="二、文件类型与权限"><a href="#二、文件类型与权限" class="headerlink" title="二、文件类型与权限"></a><strong>二、文件类型与权限</strong></h3><h3 id="1-文件类型判断宏"><a href="#1-文件类型判断宏" class="headerlink" title="1. 文件类型判断宏"></a><strong>1. 文件类型判断宏</strong></h3><table><thead><tr><th><strong>宏</strong></th><th><strong>文件类型</strong></th></tr></thead><tbody><tr><td><code>S_ISREG(mode)</code></td><td>普通文件（如 <code>.txt</code>）</td></tr><tr><td><code>S_ISDIR(mode)</code></td><td>目录</td></tr><tr><td><code>S_ISCHR(mode)</code></td><td>字符设备（如 <code>/dev/tty</code>）</td></tr><tr><td><code>S_ISBLK(mode)</code></td><td>块设备（如 <code>/dev/sda</code>）</td></tr><tr><td><code>S_ISFIFO(mode)</code></td><td>管道（FIFO）</td></tr><tr><td><code>S_ISLNK(mode)</code></td><td>符号链接</td></tr><tr><td><code>S_ISSOCK(mode)</code></td><td>套接字</td></tr></tbody></table><h4 id="示例：判断文件类型"><a href="#示例：判断文件类型" class="headerlink" title="示例：判断文件类型"></a><strong>示例：判断文件类型</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-keyword">if</span> (S_ISREG(buf.st_mode)) &#123;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;This is a regular file.\n&quot;</span>);<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="2-文件权限"><a href="#2-文件权限" class="headerlink" title="2. 文件权限"></a><strong>2. 文件权限</strong></h3><ul><li><strong>9 个权限位</strong>：<ul><li><strong>用户</strong>：<code>S_IRUSR</code>（读）、<code>S_IWUSR</code>（写）、<code>S_IXUSR</code>（执行）。</li><li><strong>组</strong>：<code>S_IRGRP</code>、<code>S_IWGRP</code>、<code>S_IXGRP</code>。</li><li><strong>其他</strong>：<code>S_IROTH</code>、<code>S_IWOTH</code>、<code>S_IXOTH</code>。</li></ul></li><li><strong>特殊权限位</strong>：<ul><li><code>S_ISUID</code>：执行时设置有效用户 ID（如 <code>passwd</code>）。</li><li><code>S_ISGID</code>：执行时设置有效组 ID。</li><li><code>S_ISVTX</code>：粘滞位（目录下文件仅所有者可删）。</li></ul></li></ul><h4 id="修改权限"><a href="#修改权限" class="headerlink" title="修改权限"></a><strong>修改权限</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">chmod(<span class="hljs-string">&quot;file.txt&quot;</span>, <span class="hljs-number">0644</span>);  <span class="hljs-comment">// rw-r--r--</span><br>fchmod(fd, <span class="hljs-number">0755</span>);         <span class="hljs-comment">// rwxr-xr-x</span><br></code></pre></td></tr></table></figure><hr><h3 id="三、文件操作"><a href="#三、文件操作" class="headerlink" title="三、文件操作"></a><strong>三、文件操作</strong></h3><h3 id="1-创建新文件"><a href="#1-创建新文件" class="headerlink" title="1. 创建新文件"></a><strong>1. 创建新文件</strong></h3><ul><li><strong>权限计算</strong>：<code>实际权限 = mode &amp; ~umask</code><br>（<code>umask</code> 默认为 <code>022</code>，屏蔽组和其他的写权限）</li><li><strong>示例</strong>：<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">umask(<span class="hljs-number">002</span>);                    <span class="hljs-comment">// 设置 umask</span><br><span class="hljs-type">int</span> fd = open(<span class="hljs-string">&quot;new.txt&quot;</span>, O_CREAT | O_WRONLY, <span class="hljs-number">0666</span>); <span class="hljs-comment">// 实际权限：664</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="2-硬链接与删除"><a href="#2-硬链接与删除" class="headerlink" title="2. 硬链接与删除"></a><strong>2. 硬链接与删除</strong></h3><ul><li>**<code>link()</code>**：创建硬链接（增加 <code>st_nlink</code>）。<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">link(<span class="hljs-string">&quot;old.txt&quot;</span>, <span class="hljs-string">&quot;new.txt&quot;</span>);  <span class="hljs-comment">// new.txt 指向 old.txt</span><br></code></pre></td></tr></table></figure></li><li>**<code>unlink()</code>**：删除链接（<code>st_nlink--</code>，若为 0 则删除文件）。<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">unlink(<span class="hljs-string">&quot;file.txt&quot;</span>);  <span class="hljs-comment">// 删除文件（需无进程打开）</span><br></code></pre></td></tr></table></figure></li><li>**<code>remove()</code>**：等价于 <code>unlink()</code>（文件）或 <code>rmdir()</code>（目录）。</li></ul><h3 id="3-重命名"><a href="#3-重命名" class="headerlink" title="3. 重命名"></a><strong>3. 重命名</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">rename(<span class="hljs-string">&quot;old.txt&quot;</span>, <span class="hljs-string">&quot;new.txt&quot;</span>);  <span class="hljs-comment">// 原子操作</span><br></code></pre></td></tr></table></figure><hr><h3 id="四、目录操作"><a href="#四、目录操作" class="headerlink" title="四、目录操作"></a><strong>四、目录操作</strong></h3><h3 id="1-创建与删除目录"><a href="#1-创建与删除目录" class="headerlink" title="1. 创建与删除目录"></a><strong>1. 创建与删除目录</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">mkdir(<span class="hljs-string">&quot;mydir&quot;</span>, <span class="hljs-number">0755</span>);  <span class="hljs-comment">// 创建目录</span><br>rmdir(<span class="hljs-string">&quot;mydir&quot;</span>);        <span class="hljs-comment">// 删除空目录</span><br></code></pre></td></tr></table></figure><h3 id="2-读取目录内容"><a href="#2-读取目录内容" class="headerlink" title="2. 读取目录内容"></a><strong>2. 读取目录内容</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;dirent.h&gt;</span></span><br>DIR *dp = opendir(<span class="hljs-string">&quot;.&quot;</span>);                <span class="hljs-comment">// 打开目录</span><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">dirent</span> *<span class="hljs-title">dir</span> =</span> readdir(dp);      <span class="hljs-comment">// 读取条目</span><br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;File: %s\n&quot;</span>, dir-&gt;d_name);     <span class="hljs-comment">// 打印文件名</span><br>closedir(dp);                          <span class="hljs-comment">// 关闭目录</span><br></code></pre></td></tr></table></figure><h4 id="struct-dirent-关键字段"><a href="#struct-dirent-关键字段" class="headerlink" title="struct dirent 关键字段"></a><strong><code>struct dirent</code> 关键字段</strong></h4><ul><li><code>d_name</code>：文件名。</li><li><code>d_type</code>：文件类型（如 <code>DT_REG</code> 普通文件）。</li></ul><hr><h3 id="五、工作目录管理"><a href="#五、工作目录管理" class="headerlink" title="五、工作目录管理"></a><strong>五、工作目录管理</strong></h3><h4 id="1-改变工作目录"><a href="#1-改变工作目录" class="headerlink" title="1. 改变工作目录"></a><strong>1. 改变工作目录</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">chdir(<span class="hljs-string">&quot;/tmp&quot;</span>);       <span class="hljs-comment">// 切换到 /tmp</span><br>fchdir(dir_fd);      <span class="hljs-comment">// 通过文件描述符切换</span><br></code></pre></td></tr></table></figure><h4 id="2-获取当前目录"><a href="#2-获取当前目录" class="headerlink" title="2. 获取当前目录"></a><strong>2. 获取当前目录</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">char</span> cwd[<span class="hljs-number">1024</span>];<br>getcwd(cwd, <span class="hljs-keyword">sizeof</span>(cwd));  <span class="hljs-comment">// 类似 pwd 命令</span><br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;CWD: %s\n&quot;</span>, cwd);<br></code></pre></td></tr></table></figure><hr><h3 id="六、总结表"><a href="#六、总结表" class="headerlink" title="六、总结表"></a><strong>六、总结表</strong></h3><table><thead><tr><th><strong>功能</strong></th><th><strong>函数&#x2F;宏</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td><strong>获取属性</strong></td><td><code>stat()</code>, <code>fstat()</code></td><td>填充 <code>struct stat</code></td></tr><tr><td><strong>文件类型判断</strong></td><td><code>S_ISREG()</code>, <code>S_ISDIR()</code></td><td>检查 <code>st_mode</code></td></tr><tr><td><strong>修改权限</strong></td><td><code>chmod()</code>, <code>fchmod()</code></td><td>设置 <code>rwx</code> 权限</td></tr><tr><td><strong>创建文件</strong></td><td><code>open()</code> with <code>O_CREAT</code></td><td>受 <code>umask</code> 影响</td></tr><tr><td><strong>硬链接操作</strong></td><td><code>link()</code>, <code>unlink()</code></td><td>修改 <code>st_nlink</code></td></tr><tr><td><strong>目录操作</strong></td><td><code>mkdir()</code>, <code>opendir()</code></td><td>创建&#x2F;遍历目录</td></tr><tr><td><strong>工作目录</strong></td><td><code>chdir()</code>, <code>getcwd()</code></td><td>类似 <code>cd</code> 和 <code>pwd</code></td></tr></tbody></table><hr><h3 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a><strong>关键点</strong></h3><ol><li><strong><code>st_mode</code> 是核心字段</strong>：包含文件类型和权限。</li><li><strong>权限计算</strong>：<code>实际权限 = mode &amp; ~umask</code>。</li><li><strong>硬链接 vs 符号链接</strong>：<ul><li>硬链接：同一文件，共享 inode。</li><li>符号链接：独立文件，存储目标路径。</li></ul></li><li><strong>目录操作</strong>：只有内核可写目录，用户通过 <code>opendir/readdir</code> 读取。</li></ol><p><strong>建议</strong>：结合 <code>ls -l</code> 命令和 <code>stat</code> 命令验证文件属性。</p><h2 id="二、进程控制"><a href="#二、进程控制" class="headerlink" title="二、进程控制"></a><strong>二、进程控制</strong></h2><h3 id="1-fork-：创建子进程"><a href="#1-fork-：创建子进程" class="headerlink" title="1. fork()：创建子进程"></a><strong>1. <code>fork()</code>：创建子进程</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-type">pid_t</span> <span class="hljs-title function_">fork</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span>;<br></code></pre></td></tr></table></figure><ul><li><strong>返回值</strong>：<ul><li><code>&gt;0</code>：父进程，返回子进程 PID。</li><li><code>=0</code>：子进程。</li><li><code>-1</code>：出错。</li></ul></li></ul><h4 id="示例：父子进程执行不同任务"><a href="#示例：父子进程执行不同任务" class="headerlink" title="示例：父子进程执行不同任务"></a><strong>示例：父子进程执行不同任务</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-type">pid_t</span> pid = fork();<br>    <span class="hljs-keyword">if</span> (pid == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Child process (PID=%d)\n&quot;</span>, getpid());<br>    &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (pid &gt; <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Parent process (PID=%d), child PID=%d\n&quot;</span>, getpid(), pid);<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        perror(<span class="hljs-string">&quot;fork failed&quot;</span>);<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>输出</strong>：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">Parent process (<span class="hljs-attribute">PID</span>=1234), child <span class="hljs-attribute">PID</span>=1235<br>Child process (<span class="hljs-attribute">PID</span>=1235)<br></code></pre></td></tr></table></figure><hr><h3 id="2-exec-：替换进程映像"><a href="#2-exec-：替换进程映像" class="headerlink" title="2. exec()：替换进程映像"></a><strong>2. <code>exec()</code>：替换进程映像</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-type">int</span> <span class="hljs-title function_">execl</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">char</span> *path, <span class="hljs-type">const</span> <span class="hljs-type">char</span> *arg0, ..., (<span class="hljs-type">char</span> *)<span class="hljs-number">0</span>)</span>;<br></code></pre></td></tr></table></figure><ul><li><strong>作用</strong>：用新程序替换当前进程（如 <code>ls</code>、<code>gcc</code>）。</li></ul><h4 id="示例：子进程执行-ls-l"><a href="#示例：子进程执行-ls-l" class="headerlink" title="示例：子进程执行 ls -l"></a><strong>示例：子进程执行 <code>ls -l</code></strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-type">pid_t</span> pid = fork();<br>    <span class="hljs-keyword">if</span> (pid == <span class="hljs-number">0</span>) &#123;<br>        execl(<span class="hljs-string">&quot;/bin/ls&quot;</span>, <span class="hljs-string">&quot;ls&quot;</span>, <span class="hljs-string">&quot;-l&quot;</span>, <span class="hljs-literal">NULL</span>);  <span class="hljs-comment">// 替换为 ls -l</span><br>        perror(<span class="hljs-string">&quot;exec failed&quot;</span>);  <span class="hljs-comment">// 若 exec 失败才会执行</span><br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        wait(<span class="hljs-literal">NULL</span>);  <span class="hljs-comment">// 等待子进程结束</span><br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="exec-函数族对比表"><a href="#exec-函数族对比表" class="headerlink" title="exec 函数族对比表"></a><strong><code>exec</code> 函数族对比表</strong></h3><p>以下是 UNIX&#x2F;Linux 中 <code>exec</code> 系列函数的详细对比，包括参数、使用场景和区别：</p><table><thead><tr><th><strong>函数</strong></th><th><strong>参数形式</strong></th><th><strong>环境变量</strong></th><th><strong>查找路径</strong></th><th><strong>典型用途</strong></th></tr></thead><tbody><tr><td><strong><code>execl</code></strong></td><td><code>execl(&quot;路径&quot;, &quot;arg0&quot;, &quot;arg1&quot;, ..., (char *)0)</code></td><td>继承当前</td><td>需完整路径</td><td>已知可执行文件路径，参数列表固定</td></tr><tr><td><strong><code>execv</code></strong></td><td><code>execv(&quot;路径&quot;, char *argv[])</code> （<code>argv</code> 需以 <code>NULL</code> 结尾）</td><td>继承当前</td><td>需完整路径</td><td>参数列表动态（如数组）</td></tr><tr><td><strong><code>execle</code></strong></td><td><code>execle(&quot;路径&quot;, &quot;arg0&quot;, ..., (char *)0, char *envp[])</code></td><td>自定义</td><td>需完整路径</td><td>需指定环境变量</td></tr><tr><td><strong><code>execve</code></strong></td><td><code>execve(&quot;路径&quot;, char *argv[], char *envp[])</code> （<code>argv</code> 和 <code>envp</code> 需以 <code>NULL</code> 结尾）</td><td>自定义</td><td>需完整路径</td><td>系统调用，最底层实现</td></tr><tr><td><strong><code>execlp</code></strong></td><td><code>execlp(&quot;文件名&quot;, &quot;arg0&quot;, ..., (char *)0)</code></td><td>继承当前</td><td>从 <code>PATH</code> 查找</td><td>直接使用文件名（如 <code>ls</code>）</td></tr><tr><td><strong><code>execvp</code></strong></td><td><code>execvp(&quot;文件名&quot;, char *argv[])</code> （<code>argv</code> 需以 <code>NULL</code> 结尾）</td><td>继承当前</td><td>从 <code>PATH</code> 查找</td><td>动态参数 + <code>PATH</code> 查找</td></tr></tbody></table><hr><h3 id="关键区别"><a href="#关键区别" class="headerlink" title="关键区别"></a><strong>关键区别</strong></h3><ol><li><p><strong>参数传递方式</strong>：</p><ul><li><strong><code>l</code> 后缀</strong>（如 <code>execl</code>）：参数以<strong>可变参数列表</strong>（<code>arg0, arg1, ..., NULL</code>）传递。</li><li><strong><code>v</code> 后缀</strong>（如 <code>execv</code>）：参数以<strong>指针数组</strong>（<code>argv[]</code>）传递，数组必须以 <code>NULL</code> 结尾。</li></ul></li><li><p><strong>环境变量</strong>：</p><ul><li><strong><code>e</code> 后缀</strong>（如 <code>execle</code>）：可<strong>自定义环境变量</strong>（通过 <code>envp[]</code> 数组）。</li><li>无 <code>e</code> 后缀：继承当前进程的环境变量。</li></ul></li><li><p><strong>路径查找</strong>：</p><ul><li><strong><code>p</code> 后缀</strong>（如 <code>execlp</code>）：自动从 <code>PATH</code> 环境变量查找可执行文件。</li><li>无 <code>p</code> 后缀：必须提供完整路径（如 <code>/bin/ls</code>）。</li></ul></li><li><p><strong>底层实现</strong>：</p><ul><li><strong><code>execve</code></strong> 是唯一的<strong>系统调用</strong>，其他函数均为库函数（最终调用 <code>execve</code>）。</li></ul></li></ol><hr><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a><strong>示例代码</strong></h3><h4 id="1-execl（固定参数-完整路径）"><a href="#1-execl（固定参数-完整路径）" class="headerlink" title="1. execl（固定参数 + 完整路径）"></a>1. <strong><code>execl</code>（固定参数 + 完整路径）</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">execl(<span class="hljs-string">&quot;/bin/ls&quot;</span>, <span class="hljs-string">&quot;ls&quot;</span>, <span class="hljs-string">&quot;-l&quot;</span>, <span class="hljs-literal">NULL</span>);  <span class="hljs-comment">// 执行 `ls -l`</span><br></code></pre></td></tr></table></figure><h4 id="2-execv（动态参数-完整路径）"><a href="#2-execv（动态参数-完整路径）" class="headerlink" title="2. execv（动态参数 + 完整路径）"></a>2. <strong><code>execv</code>（动态参数 + 完整路径）</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">char</span> *args[] = &#123;<span class="hljs-string">&quot;ls&quot;</span>, <span class="hljs-string">&quot;-l&quot;</span>, <span class="hljs-literal">NULL</span>&#125;;<br>execv(<span class="hljs-string">&quot;/bin/ls&quot;</span>, args);<br></code></pre></td></tr></table></figure><h4 id="3-execle（自定义环境变量）"><a href="#3-execle（自定义环境变量）" class="headerlink" title="3. execle（自定义环境变量）"></a>3. <strong><code>execle</code>（自定义环境变量）</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">char</span> *env[] = &#123;<span class="hljs-string">&quot;PATH=/usr/bin&quot;</span>, <span class="hljs-literal">NULL</span>&#125;;<br>execle(<span class="hljs-string">&quot;/bin/ls&quot;</span>, <span class="hljs-string">&quot;ls&quot;</span>, <span class="hljs-string">&quot;-l&quot;</span>, <span class="hljs-literal">NULL</span>, env);<br></code></pre></td></tr></table></figure><h4 id="4-execlp（自动查找-PATH）"><a href="#4-execlp（自动查找-PATH）" class="headerlink" title="4. execlp（自动查找 PATH）"></a>4. <strong><code>execlp</code>（自动查找 <code>PATH</code>）</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">execlp(<span class="hljs-string">&quot;ls&quot;</span>, <span class="hljs-string">&quot;ls&quot;</span>, <span class="hljs-string">&quot;-l&quot;</span>, <span class="hljs-literal">NULL</span>);  <span class="hljs-comment">// 无需写 `/bin/ls`</span><br></code></pre></td></tr></table></figure><h4 id="5-execvp（动态参数-自动查找）"><a href="#5-execvp（动态参数-自动查找）" class="headerlink" title="5. execvp（动态参数 + 自动查找）"></a>5. <strong><code>execvp</code>（动态参数 + 自动查找）</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">char</span> *args[] = &#123;<span class="hljs-string">&quot;ls&quot;</span>, <span class="hljs-string">&quot;-l&quot;</span>, <span class="hljs-literal">NULL</span>&#125;;<br>execvp(<span class="hljs-string">&quot;ls&quot;</span>, args);<br></code></pre></td></tr></table></figure><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul><li><strong>需要完整路径</strong> → 用 <code>execl</code>、<code>execv</code>、<code>execle</code>、<code>execve</code>。</li><li><strong>需要 <code>PATH</code> 查找</strong> → 用 <code>execlp</code>、<code>execvp</code>。</li><li><strong>动态参数</strong> → 用 <code>execv</code>、<code>execvp</code>、<code>execve</code>。</li><li><strong>自定义环境变量</strong> → 用 <code>execle</code>、<code>execve</code>。</li></ul><p><strong>注意</strong>：所有 <code>exec</code> 函数成功时<strong>不返回</strong>，失败时返回 <code>-1</code> 并设置 <code>errno</code>。</p><hr><h3 id="3-wait-：回收子进程"><a href="#3-wait-：回收子进程" class="headerlink" title="3. wait()：回收子进程"></a><strong>3. <code>wait()</code>：回收子进程</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sys/wait.h&gt;</span></span><br><span class="hljs-type">pid_t</span> <span class="hljs-title function_">wait</span><span class="hljs-params">(<span class="hljs-type">int</span> *status)</span>;<br></code></pre></td></tr></table></figure><ul><li><strong>作用</strong>：父进程等待子进程结束，避免僵尸进程。</li></ul><h4 id="示例：父进程等待子进程"><a href="#示例：父进程等待子进程" class="headerlink" title="示例：父进程等待子进程"></a><strong>示例：父进程等待子进程</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sys/wait.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-type">pid_t</span> pid = fork();<br>    <span class="hljs-keyword">if</span> (pid == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Child process\n&quot;</span>);<br>        sleep(<span class="hljs-number">2</span>);<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        wait(<span class="hljs-literal">NULL</span>);  <span class="hljs-comment">// 等待子进程结束</span><br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Parent process: child finished\n&quot;</span>);<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><hr><h2 id="三、进程间通信（IPC）"><a href="#三、进程间通信（IPC）" class="headerlink" title="三、进程间通信（IPC）"></a><strong>三、进程间通信（IPC）</strong></h2><h3 id="1-管道（pipe）"><a href="#1-管道（pipe）" class="headerlink" title="1. 管道（pipe）"></a><strong>1. 管道（<code>pipe</code>）</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-type">int</span> <span class="hljs-title function_">pipe</span><span class="hljs-params">(<span class="hljs-type">int</span> fd[<span class="hljs-number">2</span>])</span>;<br></code></pre></td></tr></table></figure><ul><li>**<code>fd[0]</code>**：读端。</li><li>**<code>fd[1]</code>**：写端。</li></ul><h4 id="示例：父子进程通信"><a href="#示例：父子进程通信" class="headerlink" title="示例：父子进程通信"></a><strong>示例：父子进程通信</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-type">int</span> fd[<span class="hljs-number">2</span>];<br>    pipe(fd);  <span class="hljs-comment">// 创建管道</span><br><br>    <span class="hljs-type">pid_t</span> pid = fork();<br>    <span class="hljs-keyword">if</span> (pid == <span class="hljs-number">0</span>) &#123;<br>        close(fd[<span class="hljs-number">0</span>]);  <span class="hljs-comment">// 子进程关闭读端</span><br>        write(fd[<span class="hljs-number">1</span>], <span class="hljs-string">&quot;Hello, parent!&quot;</span>, <span class="hljs-number">14</span>);<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        close(fd[<span class="hljs-number">1</span>]);  <span class="hljs-comment">// 父进程关闭写端</span><br>        <span class="hljs-type">char</span> buf[<span class="hljs-number">20</span>];<br>        read(fd[<span class="hljs-number">0</span>], buf, <span class="hljs-keyword">sizeof</span>(buf));<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Parent received: %s\n&quot;</span>, buf);<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>输出</strong>：</p><figure class="highlight erlang-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs erlang-repl">Parent received: Hello, parent!<br></code></pre></td></tr></table></figure><ul><li><strong>再给一个例子，好好理解</strong></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><br><span class="hljs-type">int</span> fd[<span class="hljs-number">2</span>];<br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    pipe(fd);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d  %d\n&quot;</span>, fd[<span class="hljs-number">0</span>], fd[<span class="hljs-number">1</span>]);<br>    <span class="hljs-type">pid_t</span> pid = fork();<br>    <span class="hljs-keyword">if</span> (pid == <span class="hljs-number">0</span>) &#123;<br>        close(fd[<span class="hljs-number">1</span>]);<br>        dup2(fd[<span class="hljs-number">0</span>], <span class="hljs-number">0</span>);<br>        execlp(<span class="hljs-string">&quot;wc&quot;</span>, <span class="hljs-string">&quot;wc&quot;</span>, <span class="hljs-string">&quot;-l&quot;</span>, (<span class="hljs-type">char</span> *)<span class="hljs-number">0</span>);<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;hello world!\n&quot;</span>);<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        close(fd[<span class="hljs-number">0</span>]);<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;before dup2\n&quot;</span>);<br>        dup2(fd[<span class="hljs-number">1</span>], <span class="hljs-number">1</span>);<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;after dup2\n&quot;</span>);<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;lalalala this line shouldn&#x27;t be print\n&quot;</span>);<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">3</span>  <span class="hljs-number">4</span><br><span class="hljs-attribute">before</span> dup2<br><span class="hljs-attribute">2</span><br></code></pre></td></tr></table></figure><hr><h2 id="四、信号（Signal）"><a href="#四、信号（Signal）" class="headerlink" title="四、信号（Signal）"></a><strong>四、信号（Signal）</strong></h2><h3 id="1-signal-：注册信号处理函数"><a href="#1-signal-：注册信号处理函数" class="headerlink" title="1. signal()：注册信号处理函数"></a><strong>1. <code>signal()</code>：注册信号处理函数</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;signal.h&gt;</span></span><br><span class="hljs-type">void</span> (*signal(<span class="hljs-type">int</span> sig, <span class="hljs-type">void</span> (*handler)(<span class="hljs-type">int</span>)))(<span class="hljs-type">int</span>);<br></code></pre></td></tr></table></figure><ul><li><strong>常见信号</strong>：<ul><li><code>SIGINT</code>（<code>Ctrl+C</code>）。</li><li><code>SIGKILL</code>（强制终止）。</li><li><code>SIGALRM</code>（定时器）。</li></ul></li></ul><h4 id="示例：捕获-Ctrl-C"><a href="#示例：捕获-Ctrl-C" class="headerlink" title="示例：捕获 Ctrl+C"></a><strong>示例：捕获 <code>Ctrl+C</code></strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;signal.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><br><span class="hljs-type">void</span> <span class="hljs-title function_">handler</span><span class="hljs-params">(<span class="hljs-type">int</span> sig)</span> &#123;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;\nReceived SIGINT (Ctrl+C)\n&quot;</span>);<br>    _exit(<span class="hljs-number">1</span>);  <span class="hljs-comment">// 退出程序</span><br>&#125;<br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    signal(SIGINT, handler);  <span class="hljs-comment">// 注册信号处理函数</span><br>    <span class="hljs-keyword">while</span> (<span class="hljs-number">1</span>) &#123;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Running...\n&quot;</span>);<br>        sleep(<span class="hljs-number">1</span>);<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>运行</strong>：</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-function"><span class="hljs-title">Running</span></span>...<br><span class="hljs-function"><span class="hljs-title">Running</span></span>...<br>^C<br>Received SIGINT (Ctrl+C)<br></code></pre></td></tr></table></figure><hr><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h2><table><thead><tr><th><strong>主题</strong></th><th><strong>关键函数</strong></th><th><strong>用途</strong></th></tr></thead><tbody><tr><td><strong>文件I&#x2F;O</strong></td><td><code>open</code>, <code>read</code>, <code>write</code>, <code>lseek</code>, <code>dup2</code></td><td>文件操作、重定向</td></tr><tr><td><strong>进程控制</strong></td><td><code>fork</code>, <code>exec</code>, <code>wait</code></td><td>创建进程、执行程序</td></tr><tr><td><strong>IPC</strong></td><td><code>pipe</code>, <code>mkfifo</code></td><td>进程间通信</td></tr><tr><td><strong>信号</strong></td><td><code>signal</code>, <code>kill</code>, <code>alarm</code></td><td>异步事件处理</td></tr></tbody></table><p><strong>建议</strong>：</p><ul><li>结合代码示例动手实验，加深理解。</li><li>使用 <code>strace</code> 观察系统调用执行过程（如 <code>strace ./a.out</code>）。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>UNIX</tag>
      
      <tag>复习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UNIX系统复习-3</title>
    <link href="/2025/05/31/UNIX%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0-3/"/>
    <url>/2025/05/31/UNIX%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0-3/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我复习《UNIX系统》这门课程时记录的一些笔记 ，希望能对你有所帮助😊</p>          </div><h1 id="模式匹配与处理语言awk"><a href="#模式匹配与处理语言awk" class="headerlink" title="模式匹配与处理语言awk"></a>模式匹配与处理语言awk</h1><h3 id="1-AWK-基本概念"><a href="#1-AWK-基本概念" class="headerlink" title="1. AWK 基本概念"></a><strong>1. AWK 基本概念</strong></h3><ul><li><strong>AWK</strong> 是一种<strong>模式匹配与处理语言</strong>，主要用于<strong>文本处理</strong>和<strong>数据提取</strong>。</li><li><strong>基本使用形式</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;program&#x27;</span> filename          <span class="hljs-comment"># 处理文件</span><br><span class="hljs-built_in">command</span> | awk <span class="hljs-string">&#x27;program&#x27;</span>         <span class="hljs-comment"># 处理管道输入</span><br></code></pre></td></tr></table></figure></li><li><strong>program 结构</strong>：<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">pattern1 &#123; action1 &#125;<br>pattern2 &#123; action2 &#125;<br>...<br></code></pre></td></tr></table></figure><ul><li><code>pattern</code>：匹配条件（正则表达式、逻辑表达式）</li><li><code>action</code>：匹配后执行的操作（如 <code>print</code>、<code>printf</code>）</li></ul></li></ul><hr><h3 id="2-默认行为"><a href="#2-默认行为" class="headerlink" title="2. 默认行为"></a><strong>2. 默认行为</strong></h3><ul><li><strong>默认模式</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;/pattern/&#x27;</span> file    <span class="hljs-comment"># 类似 `grep`，输出匹配的行</span><br>awk <span class="hljs-string">&#x27;&#123;print&#125;&#x27;</span> file      <span class="hljs-comment"># 类似 `cat`，输出所有行</span><br></code></pre></td></tr></table></figure></li><li><strong>示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;//&#x27;</span> file           <span class="hljs-comment"># 输出所有行</span><br>awk <span class="hljs-string">&#x27;/main/&#123;print &quot;-----&quot;&#125;&#x27;</span> file  <span class="hljs-comment"># 匹配 `main` 并打印分隔线</span><br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="3-记录与字段"><a href="#3-记录与字段" class="headerlink" title="3. 记录与字段"></a><strong>3. 记录与字段</strong></h3><ul><li><strong>记录（Record）</strong>：默认以<strong>行</strong>为单位，<code>NR</code> 表示当前行号。</li><li><strong>字段（Field）</strong>：默认以<strong>空格&#x2F;Tab</strong>分隔，<code>$1</code>、<code>$2</code>… 表示第 1、2 个字段，<code>$0</code> 表示整行，<code>NF</code> 表示字段数。</li><li><strong>示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;&#123;print NR, $0&#125;&#x27;</span> file       <span class="hljs-comment"># 显示行号 + 整行</span><br><span class="hljs-built_in">ls</span> -l | awk <span class="hljs-string">&#x27;/^d/&#123;print $8&#125;&#x27;</span>    <span class="hljs-comment"># 仅输出目录名</span><br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="4-输出控制"><a href="#4-输出控制" class="headerlink" title="4. 输出控制"></a><strong>4. 输出控制</strong></h3><ul><li>**<code>print</code>**：默认换行<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;&#123;print NR &quot;: &quot; $1, $8&#125;&#x27;</span> file<br></code></pre></td></tr></table></figure></li><li>**<code>printf</code>**：格式化输出<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;&#123;printf(&quot;%d: %s\n&quot;, NR, $0)&#125;&#x27;</span> file<br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="5-模式匹配（Pattern）"><a href="#5-模式匹配（Pattern）" class="headerlink" title="5. 模式匹配（Pattern）"></a><strong>5. 模式匹配（Pattern）</strong></h3><ul><li><strong>字段匹配</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;$5 ~ /.....*/&#x27;</span>        <span class="hljs-comment"># 第 5 字段长度 ≥ 5</span><br>awk <span class="hljs-string">&#x27;$5 !~ /.....*/&#x27;</span>       <span class="hljs-comment"># 第 5 字段长度 &lt; 5</span><br>awk <span class="hljs-string">&#x27;$5 == &quot;4096&quot;&#x27;</span>         <span class="hljs-comment"># 第 5 字段等于 &quot;4096&quot;</span><br></code></pre></td></tr></table></figure></li><li><strong>逻辑运算</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;length($5) &gt; 3&#x27;</span>       <span class="hljs-comment"># 第 5 字段长度 &gt; 3</span><br>awk <span class="hljs-string">&#x27;!(length($5) &gt; 3)&#x27;</span>    <span class="hljs-comment"># 第 5 字段长度 ≤ 3</span><br></code></pre></td></tr></table></figure></li><li><strong>特殊模式</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;BEGIN &#123;初始化动作&#125;&#x27;</span>    <span class="hljs-comment"># 程序开始前执行</span><br>awk <span class="hljs-string">&#x27;END &#123;结束动作&#125;&#x27;</span>        <span class="hljs-comment"># 程序结束后执行</span><br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="6-变量与运算"><a href="#6-变量与运算" class="headerlink" title="6. 变量与运算"></a><strong>6. 变量与运算</strong></h3><ul><li><strong>变量定义</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;&#123;nw += NF; nc += length($0)+1&#125; END &#123;print NR, nw, nc&#125;&#x27;</span> file<br></code></pre></td></tr></table></figure><ul><li>类似 <code>wc</code> 命令，计算<strong>行数、单词数、字符数</strong>。</li></ul></li><li><strong>算术运算</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;&#123;sum += $1&#125; END &#123;print sum&#125;&#x27;</span>  <span class="hljs-comment"># 计算第 1 列总和</span><br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="7-流程控制"><a href="#7-流程控制" class="headerlink" title="7. 流程控制"></a><strong>7. 流程控制</strong></h3><ul><li>**<code>if-else</code>**：<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">awk <span class="hljs-string">&#x27;&#123;</span><br><span class="hljs-string">  if ($1 == lastword) </span><br><span class="hljs-string">    printf(&quot;重复单词 %s, 行号 %d:\n%s\n&quot;, $1, NR, $0);</span><br><span class="hljs-string">  lastword = $NF;</span><br><span class="hljs-string">&#125;&#x27;</span> file<br></code></pre></td></tr></table></figure></li><li><strong><code>while</code> 循环</strong>：<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk">awk <span class="hljs-string">&#x27;&#123;</span><br><span class="hljs-string">  i = 1;</span><br><span class="hljs-string">  while (i &lt;= NF) &#123;</span><br><span class="hljs-string">    print $i;</span><br><span class="hljs-string">    i++;</span><br><span class="hljs-string">  &#125;</span><br><span class="hljs-string">&#125;&#x27;</span> file<br></code></pre></td></tr></table></figure></li><li><strong>控制语句</strong>：<ul><li><code>break</code>：退出循环</li><li><code>continue</code>：跳过当前循环</li><li><code>next</code>：处理下一条记录</li><li><code>exit</code>：结束程序</li></ul></li></ul><hr><h3 id="8-内部变量"><a href="#8-内部变量" class="headerlink" title="8. 内部变量"></a><strong>8. 内部变量</strong></h3><table><thead><tr><th>变量</th><th>描述</th></tr></thead><tbody><tr><td><code>FILENAME</code></td><td>当前输入文件名</td></tr><tr><td><code>RS</code></td><td>输入记录分隔符（默认 <code>\n</code>）</td></tr><tr><td><code>FS</code></td><td>输入字段分隔符（默认 <code>空格/Tab</code>）</td></tr><tr><td><code>NF</code></td><td>当前记录的字段数</td></tr><tr><td><code>NR</code></td><td>当前记录数（行号）</td></tr><tr><td><code>OFS</code></td><td>输出字段分隔符（默认 <code>空格</code>）</td></tr><tr><td><code>ORS</code></td><td>输出记录分隔符（默认 <code>\n</code>）</td></tr></tbody></table><hr><h3 id="9-常用函数"><a href="#9-常用函数" class="headerlink" title="9. 常用函数"></a><strong>9. 常用函数</strong></h3><table><thead><tr><th>函数</th><th>描述</th></tr></thead><tbody><tr><td><code>length(s)</code></td><td>返回字符串长度</td></tr><tr><td><code>substr(s, m, n)</code></td><td>提取子串（从 <code>m</code> 开始，取 <code>n</code> 个字符）</td></tr><tr><td><code>index(s1, s2)</code></td><td>返回 <code>s2</code> 在 <code>s1</code> 中的位置</td></tr><tr><td><code>split(s, a, sep)</code></td><td>按 <code>sep</code> 分割 <code>s</code> 存入数组 <code>a</code></td></tr><tr><td><code>tolower(s)</code> &#x2F; <code>toupper(s)</code></td><td>转换大小写</td></tr><tr><td><code>sin(x)</code> &#x2F; <code>cos(x)</code> &#x2F; <code>log(x)</code></td><td>数学函数</td></tr></tbody></table><hr><h3 id="10-数组"><a href="#10-数组" class="headerlink" title="10. 数组"></a><strong>10. 数组</strong></h3><ul><li><strong>定义与遍历</strong>：<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk">awk <span class="hljs-string">&#x27;&#123;</span><br><span class="hljs-string">  line[NR] = $0  # 存储每行到数组</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string">END &#123;</span><br><span class="hljs-string">  for (i=1; i&lt;=NR; i++) </span><br><span class="hljs-string">    print line[i]  # 按顺序输出</span><br><span class="hljs-string">&#125;&#x27;</span> file<br></code></pre></td></tr></table></figure></li><li><strong>关联数组</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;&#123;count[$1]++&#125; END &#123;for (word in count) print word, count[word]&#125;&#x27;</span> file<br></code></pre></td></tr></table></figure><ul><li>统计第 1 列单词出现次数（类似 <code>uniq -c</code>）。</li></ul></li></ul><hr><h3 id="11-输入输出约定"><a href="#11-输入输出约定" class="headerlink" title="11. 输入输出约定"></a><strong>11. 输入输出约定</strong></h3><ul><li><strong>输入来源</strong>：<ul><li>指定文件名：<code>awk &#39;program&#39; file</code></li><li>未指定：从 <code>stdin</code> 读取（支持管道）</li></ul></li><li><strong>示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> file | awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>   <span class="hljs-comment"># 从管道读取</span><br>awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span> file         <span class="hljs-comment"># 从文件读取</span><br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="12-典型应用场景"><a href="#12-典型应用场景" class="headerlink" title="12. 典型应用场景"></a><strong>12. 典型应用场景</strong></h3><ol><li><strong>数据提取</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;&#123;print $1, $3&#125;&#x27;</span> data.txt  <span class="hljs-comment"># 提取第 1、3 列</span><br></code></pre></td></tr></table></figure></li><li><strong>统计计算</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;&#123;sum += $1&#125; END &#123;print sum&#125;&#x27;</span> numbers.txt<br></code></pre></td></tr></table></figure></li><li><strong>文本过滤</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;/error/&#123;print&#125;&#x27;</span> logfile  <span class="hljs-comment"># 过滤含 &quot;error&quot; 的行</span><br></code></pre></td></tr></table></figure></li><li><strong>格式化输出</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;&#123;printf &quot;%-10s %5d\n&quot;, $1, $2&#125;&#x27;</span> data.txt<br></code></pre></td></tr></table></figure></li></ol><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul><li><strong>AWK 优势</strong>：<ul><li>强大的<strong>文本处理</strong>能力</li><li>支持<strong>正则匹配</strong>、<strong>字段操作</strong></li><li>内置<strong>变量</strong>和<strong>函数</strong>简化编程</li></ul></li><li><strong>适用场景</strong>：<ul><li>日志分析</li><li>数据清洗</li><li>报表生成</li></ul></li><li><strong>学习建议</strong>：<ul><li>掌握<strong>模式匹配</strong>（<code>/pattern/</code>）</li><li>熟练使用<strong>字段操作</strong>（<code>$1</code>, <code>$NF</code>）</li><li>理解<strong>流程控制</strong>（<code>if</code>, <code>while</code>, <code>for</code>）</li></ul></li></ul><p>通过掌握这些核心知识，可以高效使用 AWK 处理各种文本数据任务。 🚀</p><h1 id="图形界面-X"><a href="#图形界面-X" class="headerlink" title="图形界面-X"></a>图形界面-X</h1><h2 id="X-Window"><a href="#X-Window" class="headerlink" title="**X Window **"></a>**X Window **</h2><h3 id="1-历史发展"><a href="#1-历史发展" class="headerlink" title="1. 历史发展"></a><strong>1. 历史发展</strong></h3><ul><li><strong>起源</strong>（1984年）：<ul><li>由MIT基于斯坦福大学的<strong>W视窗系统</strong>开发，首个版本命名为<strong>X1</strong>（X是W的下一个字母），正式名称为<strong>X Window</strong>。</li><li>1985年陆续发布X6、X9、X10版本，并开始对外授权。</li></ul></li><li><strong>X11时代</strong>：<ul><li>1987年发布<strong>X11</strong>（最终稳定版本），MIT退出开发。</li><li>1994年发布<strong>X11R6</strong>（第六次修订版），成为主流标准。</li></ul></li></ul><h3 id="2-体系结构"><a href="#2-体系结构" class="headerlink" title="2. 体系结构"></a><strong>2. 体系结构</strong></h3><ul><li><strong>客户-服务器模型</strong>：<ul><li><strong>X Client</strong>：应用程序（如浏览器、编辑器），负责生成显示请求。</li><li><strong>X Server</strong>：接收请求并驱动硬件显示，同时处理输入设备（键盘、鼠标）事件。</li><li><strong>X协议</strong>：Client与Server之间的通信协议，独立于硬件和操作系统。</li></ul></li></ul><h3 id="3-核心组件"><a href="#3-核心组件" class="headerlink" title="3. 核心组件"></a><strong>3. 核心组件</strong></h3><ul><li><strong>X Server</strong>：<ul><li>功能：处理图形输出（如窗口渲染）和输入事件（如鼠标点击）。</li><li>特点：屏蔽硬件差异，支持跨平台（如远程桌面场景）。</li></ul></li><li><strong>窗口管理器（Window Manager）</strong>：<ul><li>特殊X Client，管理窗口布局、大小、叠放顺序等（如GNOME的Mutter）。</li></ul></li><li><strong>桌面系统（Desktop Environment）</strong>：<ul><li>依赖窗口管理器，提供完整用户界面（如KDE、XFCE）。</li></ul></li></ul><h3 id="4-X协议的优势"><a href="#4-X协议的优势" class="headerlink" title="4. X协议的优势"></a><strong>4. X协议的优势</strong></h3><ul><li><strong>可移植性</strong>：协议抽象化，Client无需关心底层硬件。</li><li><strong>网络透明性</strong>：<ul><li>Client和Server可运行在不同机器（如本地显示远程程序）。</li><li><strong>注意</strong>：本地运行X Server，远程运行X Client（与常规网络模型相反）。</li></ul></li></ul><h3 id="5-开发接口"><a href="#5-开发接口" class="headerlink" title="5. 开发接口"></a><strong>5. 开发接口</strong></h3><ul><li><strong>X Library</strong>（底层）：<ul><li>直接操作X协议，例如<strong>Xlib</strong>、<strong>Motif</strong>、<strong>Qt</strong>（早期版本）。</li></ul></li><li><strong>X Toolkit</strong>（高层）：<ul><li>提供封装组件，如<strong>Xt</strong>、<strong>GTK</strong>（基于Xlib）、<strong>OpenLook</strong>。</li></ul></li></ul><h3 id="6-应用场景"><a href="#6-应用场景" class="headerlink" title="6. 应用场景"></a><strong>6. 应用场景</strong></h3><ul><li><strong>远程图形化访问</strong>：通过X11转发（如SSH -X）运行远程图形程序。</li><li><strong>现代衍生</strong>：Wayland（X11的替代者）仍兼容X协议。</li></ul><h1 id="系统管理"><a href="#系统管理" class="headerlink" title="系统管理"></a>系统管理</h1><h2 id="Ubuntu的管理"><a href="#Ubuntu的管理" class="headerlink" title="Ubuntu的管理"></a>Ubuntu的管理</h2><h4 id="1-激活超级用户（root）（非必须，默认禁用）"><a href="#1-激活超级用户（root）（非必须，默认禁用）" class="headerlink" title="1. 激活超级用户（root）（非必须，默认禁用）"></a><strong>1. 激活超级用户（root）</strong>（非必须，默认禁用）</h4><ul><li><strong>方法 1：修改配置文件</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> vi /etc/gdm/gdm.conf  <span class="hljs-comment"># 将 &quot;AllowRoot=false&quot; 改为 &quot;AllowRoot=true&quot;</span><br></code></pre></td></tr></table></figure>或通过 GUI：<br><code>系统</code> → <code>系统管理</code> → <code>登录窗口</code> → <code>安全</code> → 勾选 <code>允许本地系统管理员登录</code>。</li><li><strong>方法 2：设置 root 密码</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> passwd root  <span class="hljs-comment"># 设置 root 密码</span><br></code></pre></td></tr></table></figure></li><li><strong>生效方式</strong>：重启系统。</li></ul><h4 id="2-更改软件源（优化下载速度）"><a href="#2-更改软件源（优化下载速度）" class="headerlink" title="2. 更改软件源（优化下载速度）"></a><strong>2. 更改软件源</strong>（优化下载速度）</h4><ul><li><strong>配置文件路径</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/apt/sources.list  <span class="hljs-comment"># 存储软件源地址</span><br></code></pre></td></tr></table></figure></li><li><strong>修改方式</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> vi /etc/apt/sources.list  <span class="hljs-comment"># 替换为更快的镜像源（如教育网源）</span><br></code></pre></td></tr></table></figure></li><li><strong>更新软件列表</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt-get update  <span class="hljs-comment"># 使新软件源生效</span><br></code></pre></td></tr></table></figure></li><li><strong>查找合适源</strong>：<br>可搜索 <code>Ubuntu 教育网 软件源</code> 获取优化地址</li></ul><h2 id="网络管理"><a href="#网络管理" class="headerlink" title="网络管理"></a>网络管理</h2><h3 id="1-网络基础配置"><a href="#1-网络基础配置" class="headerlink" title="1. 网络基础配置"></a><strong>1. 网络基础配置</strong></h3><p><strong>（1）IP地址、子网掩码、默认网关</strong></p><ul><li><strong>配置文件</strong>：<code>/etc/network/interfaces</code></li><li><strong>静态IP配置示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">auto eth0<br>iface eth0 inet static<br>    address 192.168.1.100       <span class="hljs-comment"># 设置IP地址</span><br>    netmask 255.255.255.0       <span class="hljs-comment"># 子网掩码</span><br>    gateway 192.168.1.1         <span class="hljs-comment"># 默认网关</span><br></code></pre></td></tr></table></figure></li><li><strong>动态IP（DHCP）配置</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">iface eth0 inet dhcp<br></code></pre></td></tr></table></figure></li><li><strong>生效方式</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> /etc/init.d/networking restart  <span class="hljs-comment"># 重启网络服务</span><br>ifconfig                              <span class="hljs-comment"># 查看IP配置</span><br>route -n                              <span class="hljs-comment"># 查看路由表</span><br></code></pre></td></tr></table></figure></li></ul><p><strong>（2）DNS服务器配置</strong></p><ul><li><strong>配置文件</strong>：<code>/etc/resolv.conf</code></li><li><strong>修改方式</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">nameserver 8.8.8.8          <span class="hljs-comment"># Google DNS</span><br>nameserver 8.8.4.4          <span class="hljs-comment"># 备用DNS</span><br></code></pre></td></tr></table></figure></li><li><strong>注意</strong>：<ul><li>该文件可能被 <code>NetworkManager</code> 管理，手动修改可能被覆盖。</li><li>长期生效可修改 <code>/etc/resolvconf/resolv.conf.d/base</code>。</li></ul></li></ul><hr><h3 id="2-网络应用实例"><a href="#2-网络应用实例" class="headerlink" title="2. 网络应用实例"></a><strong>2. 网络应用实例</strong></h3><p><strong>（1）搭建 Web 服务器（Apache）</strong></p><ul><li><strong>安装 Apache</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt-get install apache2<br></code></pre></td></tr></table></figure></li><li><strong>访问方式</strong>：<ul><li>浏览器输入 <code>http://&lt;Ubuntu_IP&gt;</code>（如 <code>http://192.168.1.100</code>）。</li></ul></li></ul><p><strong>（2）文件共享（Samba 网络邻居）</strong></p><ul><li><strong>安装 Samba</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt-get install samba<br></code></pre></td></tr></table></figure></li><li><strong>共享文件夹</strong>：<ol><li>创建共享目录并设置权限：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> /shared<br><span class="hljs-built_in">chmod</span> 777 /shared<br></code></pre></td></tr></table></figure></li><li>右键文件夹 → <strong>共享选项</strong>，勾选 <strong>共享此目录</strong>。</li><li>配置 Samba 访问权限（可选编辑 <code>/etc/samba/smb.conf</code>）。</li></ol></li></ul><hr><h3 id="3-关键命令总结"><a href="#3-关键命令总结" class="headerlink" title="3. 关键命令总结"></a><strong>3. 关键命令总结</strong></h3><table><thead><tr><th><strong>用途</strong></th><th><strong>命令</strong></th></tr></thead><tbody><tr><td>重启网络服务</td><td><code>sudo /etc/init.d/networking restart</code></td></tr><tr><td>查看IP配置</td><td><code>ifconfig</code> 或 <code>ip a</code></td></tr><tr><td>查看路由表</td><td><code>route -n</code></td></tr><tr><td>测试网络连通性</td><td><code>ping &lt;IP&gt;</code></td></tr><tr><td>安装Apache</td><td><code>sudo apt-get install apache2</code></td></tr><tr><td>安装Samba</td><td><code>sudo apt-get install samba</code></td></tr></tbody></table><hr><p><strong>注</strong>：</p><ul><li>修改网络配置前建议备份原文件（如 <code>sudo cp /etc/network/interfaces ~/interfaces.bak</code>）。</li><li>使用 <code>systemd</code> 的较新系统可能需改用 <code>sudo systemctl restart networking</code>。</li></ul><h3 id="4-术语解释"><a href="#4-术语解释" class="headerlink" title="4.术语解释"></a>4.术语解释</h3><h4 id="1-核心概念解释"><a href="#1-核心概念解释" class="headerlink" title="1. 核心概念解释"></a><strong>1. 核心概念解释</strong></h4><table><thead><tr><th><strong>术语</strong></th><th><strong>定义</strong></th><th><strong>示例</strong></th></tr></thead><tbody><tr><td><strong>IP地址</strong></td><td>设备的网络唯一标识符，用于在互联网或局域网中通信。分为 <strong>IPv4</strong>（如 <code>192.168.1.100</code>）和 <strong>IPv6</strong>。</td><td><code>192.168.1.100</code></td></tr><tr><td><strong>子网掩码</strong></td><td>定义IP地址中哪部分属于<strong>网络号</strong>，哪部分属于<strong>主机号</strong>，用于划分同一子网内的设备。</td><td><code>255.255.255.0</code>（表示前24位是网络号）</td></tr><tr><td><strong>默认网关</strong></td><td>本地网络与外部网络（如互联网）通信的“出口”，通常是路由器的IP地址。</td><td><code>192.168.1.1</code></td></tr><tr><td><strong>动态IP</strong></td><td>由DHCP服务器自动分配的IP地址，设备重启后可能变化。适合家庭网络或临时设备。</td><td>通过 <code>iface eth0 inet dhcp</code> 配置</td></tr><tr><td><strong>DNS服务器</strong></td><td>将域名（如 <code>google.com</code>）转换为IP地址的服务，使人类可读的网址能被计算机理解。</td><td><code>8.8.8.8</code>（Google DNS）</td></tr></tbody></table><hr><h4 id="2-详细说明"><a href="#2-详细说明" class="headerlink" title="2. 详细说明"></a><strong>2. 详细说明</strong></h4><h5 id="（1）IP地址"><a href="#（1）IP地址" class="headerlink" title="（1）IP地址"></a><strong>（1）IP地址</strong></h5><ul><li><strong>作用</strong>：唯一标识网络中的设备（如电脑、手机、服务器）。</li><li><strong>分类</strong>：<ul><li><strong>IPv4</strong>：32位地址，格式如 <code>192.168.1.100</code>（已濒临枯竭）。</li><li><strong>IPv6</strong>：128位地址，格式如 <code>2001:0db8:85a3::8a2e:0370:7334</code>（解决IPv4不足问题）。</li></ul></li><li><strong>类型</strong>：<ul><li><strong>公有IP</strong>：全球唯一，由ISP分配（如 <code>203.0.113.1</code>）。</li><li><strong>私有IP</strong>：局域网内使用（如 <code>192.168.x.x</code>、<code>10.x.x.x</code>）。</li></ul></li></ul><h5 id="（2）子网掩码"><a href="#（2）子网掩码" class="headerlink" title="（2）子网掩码"></a><strong>（2）子网掩码</strong></h5><ul><li><strong>作用</strong>：划分IP地址的<strong>网络部分</strong>和<strong>主机部分</strong>。<ul><li>示例：IP <code>192.168.1.100</code>，子网掩码 <code>255.255.255.0</code> → 网络号 <code>192.168.1.0</code>，主机号 <code>100</code>。</li></ul></li><li><strong>常见子网掩码</strong>：<ul><li><code>255.255.255.0</code>（&#x2F;24）：支持254台主机（如家庭网络）。</li><li><code>255.255.0.0</code>（&#x2F;16）：支持6.5万台主机（如企业网络）。</li></ul></li></ul><h5 id="（3）默认网关"><a href="#（3）默认网关" class="headerlink" title="（3）默认网关"></a><strong>（3）默认网关</strong></h5><ul><li><strong>作用</strong>：当设备需要访问其他网络（如互联网）时，数据包会发送到网关（通常是路由器）。<ul><li>示例：家庭路由器的IP通常是 <code>192.168.1.1</code>。</li></ul></li></ul><h5 id="（4）动态IP（DHCP）"><a href="#（4）动态IP（DHCP）" class="headerlink" title="（4）动态IP（DHCP）"></a><strong>（4）动态IP（DHCP）</strong></h5><ul><li><strong>工作原理</strong>：设备启动时向DHCP服务器（如路由器）请求IP地址，自动分配并设置子网掩码、网关等。</li><li><strong>优点</strong>：无需手动配置，适合移动设备或临时网络。</li></ul><h5 id="（5）DNS服务器"><a href="#（5）DNS服务器" class="headerlink" title="（5）DNS服务器"></a><strong>（5）DNS服务器</strong></h5><ul><li><strong>作用</strong>：将域名（如 <code>baidu.com</code>）解析为IP地址（如 <code>39.156.66.10</code>）。</li><li><strong>常见DNS</strong>：<ul><li><code>8.8.8.8</code>（Google Public DNS）</li><li><code>114.114.114.114</code>（中国电信DNS）</li></ul></li></ul><hr><h4 id="3-配置对比表（静态IP-vs-动态IP）"><a href="#3-配置对比表（静态IP-vs-动态IP）" class="headerlink" title="3. 配置对比表（静态IP vs 动态IP）"></a><strong>3. 配置对比表（静态IP vs 动态IP）</strong></h4><table><thead><tr><th><strong>配置项</strong></th><th><strong>静态IP</strong></th><th><strong>动态IP（DHCP）</strong></th></tr></thead><tbody><tr><td><strong>IP地址</strong></td><td>手动指定（如 <code>192.168.1.100</code>）</td><td>由DHCP服务器自动分配</td></tr><tr><td><strong>适用场景</strong></td><td>服务器、打印机等需固定IP的设备</td><td>手机、笔记本电脑等临时设备</td></tr><tr><td><strong>配置复杂度</strong></td><td>需手动设置IP、子网掩码、网关</td><td>自动获取，无需配置</td></tr><tr><td><strong>稳定性</strong></td><td>IP固定，不易冲突</td><td>IP可能变化，依赖DHCP服务器</td></tr></tbody></table><hr><h4 id="4-示例命令（Ubuntu）"><a href="#4-示例命令（Ubuntu）" class="headerlink" title="4. 示例命令（Ubuntu）"></a><strong>4. 示例命令（Ubuntu）</strong></h4><ul><li><p><strong>查看当前IP和网关</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">ifconfig      <span class="hljs-comment"># 查看IP和子网掩码</span><br>route -n      <span class="hljs-comment"># 查看默认网关</span><br><span class="hljs-built_in">cat</span> /etc/resolv.conf  <span class="hljs-comment"># 查看DNS服务器</span><br></code></pre></td></tr></table></figure></li><li><p><strong>测试DNS解析</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ping google.com  <span class="hljs-comment"># 测试DNS是否正常工作</span><br></code></pre></td></tr></table></figure></li></ul><hr><p><strong>总结</strong>：</p><ul><li>IP地址是设备的“门牌号”，子网掩码决定“小区范围”，网关是“小区出口”，DNS是“电话簿”。</li><li>静态IP适合固定设备，动态IP适合移动设备。</li></ul><h2 id="用户管理"><a href="#用户管理" class="headerlink" title="用户管理"></a>用户管理</h2><ul><li>略</li></ul><h2 id="磁盘管理"><a href="#磁盘管理" class="headerlink" title="磁盘管理"></a>磁盘管理</h2><h3 id="1-磁盘概述"><a href="#1-磁盘概述" class="headerlink" title="1. 磁盘概述"></a><strong>1. 磁盘概述</strong></h3><ul><li><strong>磁盘类型</strong>：<ul><li><strong>软盘</strong>：<code>/dev/fd[01]</code>（已淘汰）</li><li><strong>光盘</strong>：<code>/dev/cdrom</code>（通常符号链接到 <code>/dev/sr0</code>）</li><li><strong>硬盘</strong>：<ul><li><strong>IDE硬盘</strong>：<code>/dev/hd[a-d]</code>（旧式接口，已少见）</li><li><strong>SCSI&#x2F;SATA硬盘</strong>：<code>/dev/sd[a-z]</code>（现代主流设备）</li></ul></li></ul></li><li><strong>分区命名规则</strong>：<ul><li>主分区：<code>/dev/sda1</code>、<code>/dev/sda2</code>（最多4个）</li><li>扩展分区 + 逻辑分区：<code>/dev/sda5</code>、<code>/dev/sda6</code>（从5开始编号）</li></ul></li></ul><hr><h3 id="2-硬盘分区（fdisk）"><a href="#2-硬盘分区（fdisk）" class="headerlink" title="2. 硬盘分区（fdisk）"></a><strong>2. 硬盘分区（<code>fdisk</code>）</strong></h3><ul><li><strong>分区规则</strong>：<ul><li>一块硬盘最多分4个<strong>主分区</strong>，或 <strong>3主分区 + 1扩展分区</strong>（扩展分区可再分多个逻辑分区）。</li></ul></li><li><strong>操作步骤</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> fdisk /dev/sdb   <span class="hljs-comment"># 对第二块硬盘分区</span><br></code></pre></td></tr></table></figure><ul><li>交互命令：<ul><li><code>n</code>：新建分区</li><li><code>d</code>：删除分区</li><li><code>p</code>：打印分区表</li><li><code>w</code>：保存并退出</li></ul></li></ul></li></ul><hr><h3 id="3-格式化分区（mkfs）"><a href="#3-格式化分区（mkfs）" class="headerlink" title="3. 格式化分区（mkfs）"></a><strong>3. 格式化分区（<code>mkfs</code>）</strong></h3><ul><li><strong>作用</strong>：创建文件系统（如ext4、NTFS）。</li><li><strong>常用命令</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> mkfs -t ext4 /dev/sdb1  <span class="hljs-comment"># 格式化为ext4文件系统</span><br><span class="hljs-built_in">sudo</span> mkfs -t ntfs /dev/sdb2  <span class="hljs-comment"># 格式化为NTFS（Windows兼容）</span><br></code></pre></td></tr></table></figure></li><li><strong>常见文件系统类型</strong>：<ul><li><code>ext4</code>（Linux默认）</li><li><code>xfs</code>（高性能）</li><li><code>ntfs</code>&#x2F;<code>fat32</code>（Windows兼容）</li></ul></li></ul><hr><h3 id="4-挂载与卸载（mount-umount）"><a href="#4-挂载与卸载（mount-umount）" class="headerlink" title="4. 挂载与卸载（mount&#x2F;umount）"></a><strong>4. 挂载与卸载（<code>mount</code>&#x2F;<code>umount</code>）</strong></h3><ul><li><strong>挂载分区</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> mount /dev/sdb1 /mnt/data  <span class="hljs-comment"># 将分区挂载到/mnt/data</span><br></code></pre></td></tr></table></figure><ul><li><strong>要求</strong>：挂载点目录需为空。</li></ul></li><li><strong>卸载分区</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> umount /dev/sdb1   <span class="hljs-comment"># 通过设备名卸载</span><br><span class="hljs-built_in">sudo</span> umount /mnt/data   <span class="hljs-comment"># 通过挂载点卸载</span><br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="5-查看磁盘信息（df-lsblk）"><a href="#5-查看磁盘信息（df-lsblk）" class="headerlink" title="5. 查看磁盘信息（df&#x2F;lsblk）"></a><strong>5. 查看磁盘信息（<code>df</code>&#x2F;<code>lsblk</code>）</strong></h3><ul><li><strong>查看挂载情况</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">df</span> -h                  <span class="hljs-comment"># 显示磁盘使用情况（人类可读）</span><br>lsblk                  <span class="hljs-comment"># 查看所有块设备及挂载点</span><br></code></pre></td></tr></table></figure></li><li><strong>示例输出</strong>：<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/dev/</span>sdb1     <span class="hljs-number">20</span>G  <span class="hljs-number">5.0</span>G   <span class="hljs-number">15</span>G  <span class="hljs-number">25</span>% <span class="hljs-regexp">/mnt/</span>data<br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="6-关键命令速查表"><a href="#6-关键命令速查表" class="headerlink" title="6. 关键命令速查表"></a><strong>6. 关键命令速查表</strong></h3><table><thead><tr><th><strong>操作</strong></th><th><strong>命令</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>分区管理</td><td><code>sudo fdisk /dev/sdb</code></td><td>交互式分区工具</td></tr><tr><td>格式化分区</td><td><code>sudo mkfs -t ext4 /dev/sdb1</code></td><td>创建ext4文件系统</td></tr><tr><td>挂载分区</td><td><code>sudo mount /dev/sdb1 /mnt/data</code></td><td>关联分区与目录</td></tr><tr><td>卸载分区</td><td><code>sudo umount /mnt/data</code></td><td>解除挂载</td></tr><tr><td>查看磁盘空间</td><td><code>df -h</code></td><td>显示已挂载分区的使用情况</td></tr><tr><td>查看块设备</td><td><code>lsblk</code></td><td>列出所有磁盘及分区信息</td></tr></tbody></table><hr><h3 id="7-注意事项"><a href="#7-注意事项" class="headerlink" title="7. 注意事项"></a><strong>7. 注意事项</strong></h3><ol><li><strong>数据安全</strong>：操作分区前备份重要数据，<code>fdisk</code> 和 <code>mkfs</code> 会清除数据！</li><li><strong>自动挂载</strong>：编辑 <code>/etc/fstab</code> 可实现开机自动挂载。</li><li><strong>权限问题</strong>：挂载后需确保用户对目录有读写权限（如 <code>chmod 777 /mnt/data</code>）。</li></ol><p><strong>总结</strong>：</p><ul><li><strong>分区</strong> → <strong>格式化</strong> → <strong>挂载</strong> 是磁盘使用的三大步骤。</li><li><code>fdisk</code> 用于分区，<code>mkfs</code> 用于格式化，<code>mount</code>&#x2F;<code>umount</code> 用于挂载和卸载。</li></ul><h2 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h2><h4 id="1-进程概述"><a href="#1-进程概述" class="headerlink" title="1. 进程概述"></a><strong>1. 进程概述</strong></h4><ul><li><strong>进程（Process）</strong>：<ul><li>简单来说，就是一个<strong>正在运行的程序</strong>（但严格来说，进程是程序的执行实例，包含代码、数据和系统资源）。</li><li>包括：<ul><li><strong>系统核心进程</strong>（如 <code>systemd</code>、<code>kthreadd</code>）</li><li><strong>系统服务进程</strong>（如 <code>sshd</code>、<code>nginx</code>）</li><li><strong>普通用户进程</strong>（如 <code>bash</code>、<code>vim</code>）</li></ul></li><li><strong>可强制结束</strong>（通过 <code>kill</code> 命令发送信号）。</li></ul></li></ul><hr><h4 id="2-查看进程（ps-命令）"><a href="#2-查看进程（ps-命令）" class="headerlink" title="2. 查看进程（ps 命令）"></a><strong>2. 查看进程（<code>ps</code> 命令）</strong></h4><table><thead><tr><th><strong>命令</strong></th><th><strong>作用</strong></th></tr></thead><tbody><tr><td><code>ps</code></td><td>查看当前终端启动的进程（仅当前会话）。</td></tr><tr><td><code>ps -t 终端名</code></td><td>显示某个终端（如 <code>pts/0</code>）启动的进程。</td></tr><tr><td><code>ps -u 用户名</code></td><td>显示某个用户（如 <code>root</code>）的所有进程。</td></tr><tr><td><code>ps -e</code></td><td>显示<strong>所有进程</strong>（包括系统进程）。</td></tr><tr><td><code>ps -f</code></td><td>显示进程的<strong>完整信息</strong>（如启动命令）。</td></tr><tr><td><code>ps –ef | grep &#39;xxx&#39;</code></td><td>过滤特定进程（如 <code>ps -ef | grep nginx</code>）。</td></tr></tbody></table><p><strong>示例</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ps -ef | grep sshd  <span class="hljs-comment"># 查找所有SSH服务进程</span><br></code></pre></td></tr></table></figure><hr><h4 id="3-结束进程（kill-命令）"><a href="#3-结束进程（kill-命令）" class="headerlink" title="3. 结束进程（kill 命令）"></a><strong>3. 结束进程（<code>kill</code> 命令）</strong></h4><ul><li><p><strong>语法</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">kill</span> [-signal] PID<br></code></pre></td></tr></table></figure><ul><li><code>PID</code>：进程ID（通过 <code>ps</code> 或 <code>top</code> 获取）。</li><li><code>-signal</code>：发送的信号类型（默认 <code>-15</code>，即 <code>SIGTERM</code>）。</li></ul></li><li><p><strong>常用信号</strong>：</p><table><thead><tr><th><strong>信号</strong></th><th><strong>数值</strong></th><th><strong>作用</strong></th></tr></thead><tbody><tr><td><code>SIGINT</code></td><td><code>2</code></td><td>中断进程（类似 <code>Ctrl+C</code>）。</td></tr><tr><td><code>SIGTERM</code></td><td><code>15</code></td><td>优雅终止（默认信号）。</td></tr><tr><td><code>SIGKILL</code></td><td><code>9</code></td><td>强制杀死（无法被忽略）。</td></tr></tbody></table></li><li><p><strong>查看所有信号</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">kill</span> -l<br></code></pre></td></tr></table></figure></li><li><p><strong>示例</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">kill</span> -9 1234   <span class="hljs-comment"># 强制结束PID为1234的进程</span><br></code></pre></td></tr></table></figure></li></ul><hr><h4 id="4-关键点总结"><a href="#4-关键点总结" class="headerlink" title="4. 关键点总结"></a><strong>4. 关键点总结</strong></h4><ol><li><strong>进程是程序的运行实例</strong>，包含系统资源。</li><li><strong><code>ps</code> 用于查看进程</strong>，常用组合 <code>ps -ef</code> + <code>grep</code>。</li><li><strong><code>kill</code> 用于终止进程</strong>：<ul><li>优先用 <code>-15</code>（<code>SIGTERM</code>）尝试正常退出。</li><li>若无效，再用 <code>-9</code>（<code>SIGKILL</code>）强制结束。</li></ul></li><li>**慎用 <code>kill -9</code>**：可能导致资源未释放（如临时文件未删除）。</li></ol><hr><h4 id="5-扩展命令"><a href="#5-扩展命令" class="headerlink" title="5. 扩展命令"></a><strong>5. 扩展命令</strong></h4><ul><li>**<code>top</code>&#x2F;<code>htop</code>**：动态查看进程及资源占用。</li><li>**<code>pkill</code>**：按进程名结束（如 <code>pkill -9 nginx</code>）。</li><li>**<code>killall</code>**：结束所有同名进程（如 <code>killall -u git</code>）。</li></ul><hr><p><strong>附：进程管理流程图</strong></p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs inform7">查看进程 → ps -ef | grep <span class="hljs-comment">[进程名]</span> → 获取PID → kill -15 <span class="hljs-comment">[PID]</span> → 检查是否结束 → 若未结束 → kill -9 <span class="hljs-comment">[PID]</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>UNIX</tag>
      
      <tag>复习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UNIX系统复习-2</title>
    <link href="/2025/05/30/UNIX%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0-2/"/>
    <url>/2025/05/30/UNIX%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0-2/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我复习《UNIX系统》这门课程时记录的一些笔记 ，希望能对你有所帮助😊</p>          </div><h1 id="i-节点"><a href="#i-节点" class="headerlink" title="i 节点"></a>i 节点</h1><blockquote><p>理解 i 节点能帮助你更深入掌握 Linux 文件系统的工作原理！</p></blockquote><p>在 Linux&#x2F;Unix 文件系统中，<strong>i 节点（inode）</strong> 是文件系统的核心数据结构，它存储了文件的<strong>元数据</strong>（metadata），但&#x3D;&#x3D;不包含文件名或文件内容本身&#x3D;&#x3D;</p><hr><h2 id="1-i-节点的基本概念"><a href="#1-i-节点的基本概念" class="headerlink" title="1. i 节点的基本概念"></a><strong>1. i 节点的基本概念</strong></h2><h3 id="（1）文件的组成"><a href="#（1）文件的组成" class="headerlink" title="（1）文件的组成"></a><strong>（1）文件的组成</strong></h3><p>每个文件在文件系统中由 <strong>3 部分</strong>组成：</p><ol><li><strong>文件名</strong>（File Name）：用户可见的名称（如 <code>file1</code>）。</li><li><strong>i 节点</strong>（inode）：存储文件的元数据（权限、所有者、时间戳、数据块位置等）。</li><li><strong>文件内容</strong>（Data Blocks）：实际存储文件数据的磁盘块。</li></ol><h3 id="（2）查看文件的-i-节点"><a href="#（2）查看文件的-i-节点" class="headerlink" title="（2）查看文件的 i 节点"></a><strong>（2）查看文件的 i 节点</strong></h3><p>使用 <code>ls -il</code> 可以查看文件的 i 节点编号：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">ls</span> -il<br>1234567 -rw-r--r-- 1 user group 1024 Jan 1 10:00 file1<br></code></pre></td></tr></table></figure><ul><li><strong><code>1234567</code></strong> 是 <code>file1</code> 的 i 节点编号。</li><li>其他信息（权限、所有者、大小、时间等）均存储在 i 节点中。</li></ul><hr><h2 id="2-i-节点中存储的信息"><a href="#2-i-节点中存储的信息" class="headerlink" title="2. i 节点中存储的信息"></a><strong>2. i 节点中存储的信息</strong></h2><p>每个 i 节点包含以下关键信息（通过 <code>stat</code> 命令可查看）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">stat</span> file1<br>  File: file1<br>  Size: 1024        Blocks: 8          IO Block: 4096   regular file<br>Device: 801h/2049d  Inode: 1234567     Links: 1<br>Access: 2024-01-01 10:00:00.000000000 +0800<br>Modify: 2024-01-01 10:00:00.000000000 +0800<br>Change: 2024-01-01 10:00:00.000000000 +0800<br> Birth: -<br></code></pre></td></tr></table></figure><table><thead><tr><th>字段</th><th>说明</th></tr></thead><tbody><tr><td><strong>Inode 编号</strong></td><td>文件的唯一标识（如 <code>1234567</code>）。</td></tr><tr><td><strong>文件类型</strong></td><td>普通文件、目录、符号链接等。</td></tr><tr><td><strong>权限（Mode）</strong></td><td>如 <code>-rw-r--r--</code>（对应数字 <code>644</code>）。</td></tr><tr><td><strong>所有者（UID）</strong></td><td>文件属主的用户 ID。</td></tr><tr><td><strong>所属组（GID）</strong></td><td>文件属组的组 ID。</td></tr><tr><td><strong>硬链接数（Links）</strong></td><td>指向该 i 节点的文件名数量（见下文）。</td></tr><tr><td><strong>文件大小</strong></td><td>字节数（如 <code>1024</code>）。</td></tr><tr><td><strong>时间戳</strong></td><td>访问时间（<code>atime</code>）、修改时间（<code>mtime</code>）、状态变更时间（<code>ctime</code>）。</td></tr><tr><td><strong>数据块位置</strong></td><td>文件内容存储在磁盘的哪些块中。</td></tr></tbody></table><hr><h2 id="3-硬链接（Hard-Link）与-i-节点的关系"><a href="#3-硬链接（Hard-Link）与-i-节点的关系" class="headerlink" title="3. 硬链接（Hard Link）与 i 节点的关系"></a><strong>3. 硬链接（Hard Link）与 i 节点的关系</strong></h2><h3 id="（1）创建硬链接"><a href="#（1）创建硬链接" class="headerlink" title="（1）创建硬链接"></a><strong>（1）创建硬链接</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">ln</span> file1 file2  <span class="hljs-comment"># 为 file1 的 i 节点增加一个新文件名 file2</span><br></code></pre></td></tr></table></figure><ul><li><strong>本质</strong>：<code>file1</code> 和 <code>file2</code> 指向同一个 i 节点（如 <code>1234567</code>）。</li><li><strong>效果</strong>：<ul><li>i 节点的 <strong>硬链接计数（Links）</strong> 从 <code>1</code> 变为 <code>2</code>。</li><li><strong>修改 <code>file1</code> 或 <code>file2</code> 会同步更改文件内容（因为指向同一数据块）</strong>！！！</li></ul></li></ul><h3 id="（2）删除硬链接"><a href="#（2）删除硬链接" class="headerlink" title="（2）删除硬链接"></a><strong>（2）删除硬链接</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">rm</span> file2  <span class="hljs-comment"># 删除文件名 file2</span><br></code></pre></td></tr></table></figure><ul><li><strong>本质</strong>：仅删除 <code>file2</code> 这个文件名，i 节点（<code>1234567</code>）和数据块仍存在。</li><li><strong>效果</strong>：<ul><li>i 节点的硬链接计数从 <code>2</code> 变回 <code>1</code>（<code>file1</code> 仍指向它）。</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">rm</span> file1  <span class="hljs-comment"># 删除最后一个文件名 file1</span><br></code></pre></td></tr></table></figure><ul><li><strong>本质</strong>：i 节点的硬链接计数降为 <code>0</code>，系统自动回收该 i 节点及其数据块。</li></ul><hr><h2 id="4-i-节点-vs-文件名"><a href="#4-i-节点-vs-文件名" class="headerlink" title="4. i 节点 vs 文件名"></a><strong>4. i 节点 vs 文件名</strong></h2><table><thead><tr><th>行为</th><th>对 i 节点的影响</th></tr></thead><tbody><tr><td><strong>创建文件</strong></td><td>分配一个空闲 i 节点，初始化元数据，硬链接计数设为 <code>1</code>。</td></tr><tr><td><strong>创建硬链接</strong></td><td>i 节点的硬链接计数 <code>+1</code>（多个文件名共享同一 i 节点）。</td></tr><tr><td><strong>删除文件</strong></td><td>i 节点的硬链接计数 <code>-1</code>；若计数为 <code>0</code>，则释放 i 节点和数据块。</td></tr><tr><td><strong>修改文件内容</strong></td><td>更新 i 节点中的 <code>mtime</code> 和 <code>ctime</code>，但 i 节点编号不变。</td></tr><tr><td><strong>重命名文件</strong></td><td>仅修改文件名记录，不影响 i 节点。</td></tr></tbody></table><hr><h2 id="5-关键结论"><a href="#5-关键结论" class="headerlink" title="5. 关键结论"></a><strong>5. 关键结论</strong></h2><ol><li><strong>i 节点是文件的唯一标识</strong><ul><li><strong>文件名只是 i 节点的“别名”，同一个文件可以有多个文件名（硬链接）</strong></li></ul></li><li><strong>删除文件的实际过程</strong><ul><li><code>rm</code> 只是减少 i 节点的硬链接计数，计数为 <code>0</code> 时才会真正删除文件。</li></ul></li><li><strong>硬链接的限制</strong><ul><li>不能跨文件系统（因为 i 节点编号仅在同一个文件系统内唯一）。</li><li>不能对目录创建硬链接（防止循环引用导致文件系统混乱）。</li></ul></li></ol><hr><h2 id="6-扩展命令"><a href="#6-扩展命令" class="headerlink" title="6. 扩展命令"></a><strong>6. 扩展命令</strong></h2><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td><code>ls -i</code></td><td>显示文件的 i 节点编号。</td></tr><tr><td><code>df -i</code></td><td>查看文件系统的 i 节点使用情况。</td></tr><tr><td><code>find / -inum 1234</code></td><td>通过 i 节点编号查找文件。</td></tr></tbody></table><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul><li><strong>i 节点是文件的“身份证”</strong>，存储元数据但不含文件名或内容。</li><li><strong>硬链接</strong>是多个文件名共享同一 i 节点，<strong>软链接（符号链接）</strong>则是&#x3D;&#x3D;独立的文件（有自己的 i 节点）,存储目标文件的路径（字符串）&#x3D;&#x3D;</li><li>文件删除的本质是 <strong>减少 i 节点的硬链接计数</strong>，计数归零后系统才会回收资源。</li></ul><h1 id="文件编辑与流处理"><a href="#文件编辑与流处理" class="headerlink" title="文件编辑与流处理"></a>文件编辑与流处理</h1><h2 id="ed编辑器"><a href="#ed编辑器" class="headerlink" title="ed编辑器"></a>ed编辑器</h2><p><code>ed</code> 是 Unix 历史上的里程碑式行编辑器，由 Ken Thompson 在 1970 年开发，其设计哲学深刻影响了后续工具（如 <code>sed</code>、<code>vim</code>）。以下是结合您提供的资料的系统性总结：</p><hr><h3 id="一、ed-的核心优势"><a href="#一、ed-的核心优势" class="headerlink" title="一、ed 的核心优势"></a><strong>一、ed 的核心优势</strong></h3><ol><li><p><strong>极简主义</strong></p><ul><li>仅需 <strong>几 KB 内存</strong> 即可运行，适合救援模式或嵌入式系统。</li><li>无图形界面，所有操作通过<strong>命令驱动</strong>，可通过管道脚本化编辑。</li></ul></li><li><p><strong>正则表达式先驱</strong></p><ul><li>其正则语法（如 <code>s/old/new/</code>）被 <code>sed</code>、<code>grep</code>、<code>perl</code> 等工具继承，成为业界标准。</li></ul></li><li><p><strong>高效处理</strong></p><ul><li>直接按行操作，避免全文件加载，处理大文件时速度显著优于现代编辑器。</li></ul></li></ol><hr><h3 id="二、核心命令详解（按功能分类）"><a href="#二、核心命令详解（按功能分类）" class="headerlink" title="二、核心命令详解（按功能分类）"></a><strong>二、核心命令详解（按功能分类）</strong></h3><h4 id="1-导航与显示"><a href="#1-导航与显示" class="headerlink" title="1. 导航与显示"></a><strong>1. 导航与显示</strong></h4><table><thead><tr><th>命令</th><th>作用</th><th>示例</th></tr></thead><tbody><tr><td><code>n</code></td><td>跳转到第 n 行</td><td><code>5</code> → 跳转到第 5 行</td></tr><tr><td><code>.+3</code></td><td>相对行号（当前行+3）</td><td><code>.,.+5p</code> → 显示后续 5 行</td></tr><tr><td><code>$</code></td><td>最后一行</td><td><code>$p</code> → 显示末行</td></tr><tr><td><code>,p</code></td><td>显示全部内容</td><td><code>1,$p</code> 等效</td></tr></tbody></table><h4 id="2-编辑操作"><a href="#2-编辑操作" class="headerlink" title="2. 编辑操作"></a><strong>2. 编辑操作</strong></h4><table><thead><tr><th>命令</th><th>作用</th><th>结束符</th></tr></thead><tbody><tr><td><code>a</code></td><td>当前行后追加</td><td>独占一行的 <code>.</code></td></tr><tr><td><code>i</code></td><td>当前行前插入</td><td>同上</td></tr><tr><td><code>m,nd</code></td><td>删除 m 到 n 行</td><td>无</td></tr><tr><td><code>u</code></td><td>撤销最后一次编辑</td><td>仅支持一级撤销</td></tr></tbody></table><h4 id="3-搜索与替换"><a href="#3-搜索与替换" class="headerlink" title="3. 搜索与替换"></a><strong>3. 搜索与替换</strong></h4><table><thead><tr><th>命令</th><th>关键细节</th><th>示例</th></tr></thead><tbody><tr><td><code>/pattern/</code></td><td>向后搜索，循环至文件尾</td><td><code>/main/</code> → 跳转到下一个 “main”</td></tr><tr><td><code>?pattern?</code></td><td>向前搜索，循环至文件头</td><td><code>?int?</code> → 向上找 “int”</td></tr><tr><td><code>s/old/new/[g]</code></td><td><code>g</code> 表示全局替换</td><td><code>s/cat/dog/g</code> → 替换所有 “cat”</td></tr><tr><td><code>&amp;</code> 占位符</td><td>引用匹配的文本</td><td><code>s/big/very &amp;/</code> → “big” → “very big”</td></tr></tbody></table><h4 id="4-文件操作"><a href="#4-文件操作" class="headerlink" title="4. 文件操作"></a><strong>4. 文件操作</strong></h4><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td><code>w [file]</code></td><td>保存（可另存为）</td></tr><tr><td><code>q</code></td><td>安全退出（未保存时提示）</td></tr><tr><td><code>Q</code></td><td>强制退出（不保存）</td></tr><tr><td><code>!cmd</code></td><td>临时执行 shell 命令（如 <code>!ls</code>）</td></tr></tbody></table><hr><h3 id="三、经典使用场景"><a href="#三、经典使用场景" class="headerlink" title="三、经典使用场景"></a><strong>三、经典使用场景</strong></h3><h4 id="1-脚本化编辑"><a href="#1-脚本化编辑" class="headerlink" title="1. 脚本化编辑"></a><strong>1. 脚本化编辑</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 通过管道批量替换文件内容</span><br><span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;1,\$s/foo/bar/g\nwq&quot;</span> | ed file.txt<br></code></pre></td></tr></table></figure><h4 id="2-精准行操作"><a href="#2-精准行操作" class="headerlink" title="2. 精准行操作"></a><strong>2. 精准行操作</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 删除包含 &quot;debug&quot; 的行及其后 2 行</span><br><span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;/debug/,+2d\nwq&quot;</span> | ed log.txt<br></code></pre></td></tr></table></figure><h4 id="3-正则表达式测试"><a href="#3-正则表达式测试" class="headerlink" title="3. 正则表达式测试"></a><strong>3. 正则表达式测试</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 测试正则匹配范围（显示首次 &quot;error&quot; 到文件末尾）</span><br><span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;/error/,\$p\nq&quot;</span> | ed app.log<br></code></pre></td></tr></table></figure><hr><h3 id="四、与现代工具对比"><a href="#四、与现代工具对比" class="headerlink" title="四、与现代工具对比"></a><strong>四、与现代工具对比</strong></h3><table><thead><tr><th>特性</th><th><code>ed</code></th><th><code>sed</code>&#x2F;<code>vim</code></th></tr></thead><tbody><tr><td><strong>交互性</strong></td><td>极低（纯命令）</td><td>中&#x2F;高</td></tr><tr><td><strong>学习曲线</strong></td><td>陡峭</td><td>中等</td></tr><tr><td><strong>适用场景</strong></td><td>脚本化、救援模式</td><td>日常编辑</td></tr><tr><td><strong>正则支持</strong></td><td>基础</td><td>扩展（如 <code>\d</code>）</td></tr></tbody></table><hr><h3 id="五、操作流程图解"><a href="#五、操作流程图解" class="headerlink" title="五、操作流程图解"></a><strong>五、操作流程图解</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">启动 ed → [命令模式] <br>  ├─ 导航 (n, $, .) <br>  ├─ 编辑 (a/i, d, s) <br>  ├─ 搜索 (/pattern/, ?pattern?) <br>  └─ 保存退出 (wq)<br></code></pre></td></tr></table></figure><hr><h3 id="六、为什么今天仍值得了解？"><a href="#六、为什么今天仍值得了解？" class="headerlink" title="六、为什么今天仍值得了解？"></a><strong>六、为什么今天仍值得了解？</strong></h3><ol><li><strong>理解 Unix 哲学</strong>：<code>ed</code> 是“工具链组合”思想的典范（与 <code>grep</code>、<code>awk</code> 协同）。</li><li><strong>应急修复</strong>：当系统仅剩 <code>/bin/ed</code> 可用时，它是最后的编辑手段。</li><li><strong>历史教育</strong>：通过 <code>ed</code> 可直观看到现代编辑器的演进路径。</li></ol><p>若需快速上手实用编辑，推荐 <code>nano</code> 或 <code>vim</code>；但掌握 <code>ed</code> 如同学习汇编语言，能加深对计算本质的理解。</p><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><h3 id="ed-正则表达式核心知识点"><a href="#ed-正则表达式核心知识点" class="headerlink" title="ed 正则表达式核心知识点"></a><strong>ed 正则表达式核心知识点</strong></h3><hr><h4 id="1-元字符与转义"><a href="#1-元字符与转义" class="headerlink" title="1. 元字符与转义"></a><strong>1. 元字符与转义</strong></h4><table><thead><tr><th>语法</th><th>作用</th><th>示例</th></tr></thead><tbody><tr><td><code>普通字符</code></td><td>匹配自身（如 <code>a</code>、<code>1</code>）</td><td><code>cat</code> → 匹配 “cat”</td></tr><tr><td><code>\c</code></td><td>取消字符 <code>c</code> 的特殊意义</td><td><code>\*</code> → 匹配字面 <code>*</code></td></tr><tr><td><code>^</code></td><td><strong>行首</strong>（仅模式起始处有效）</td><td><code>^hello</code> → 行首的 “hello”</td></tr><tr><td><code>$</code></td><td><strong>行尾</strong>（仅模式结束处有效）</td><td><code>end$</code> → 行尾的 “end”</td></tr><tr><td><code>.</code></td><td>匹配<strong>任意单个字符</strong></td><td><code>a.c</code> → “abc”, “a c”</td></tr></tbody></table><hr><h4 id="2-字符类"><a href="#2-字符类" class="headerlink" title="2. 字符类"></a><strong>2. 字符类</strong></h4><table><thead><tr><th>语法</th><th>作用</th><th>示例</th></tr></thead><tbody><tr><td><code>[...]</code></td><td>匹配括号内<strong>任一字符</strong></td><td><code>[aeiou]</code> → 任意元音</td></tr><tr><td><code>[^...]</code></td><td>匹配<strong>不在</strong>括号内的字符</td><td><code>[^0-9]</code> → 非数字</td></tr><tr><td><code>[a-z]</code></td><td>字符范围（支持连字符 <code>-</code>）</td><td><code>[A-Z]</code> → 大写字母</td></tr></tbody></table><hr><h4 id="3-重复与分组"><a href="#3-重复与分组" class="headerlink" title="3. 重复与分组"></a><strong>3. 重复与分组</strong></h4><table><thead><tr><th>语法</th><th>作用</th><th>示例</th></tr></thead><tbody><tr><td><code>r*</code></td><td><strong>0 次或多次</strong>匹配 <code>r</code></td><td><code>a*</code> → “”, “a”, “aa”</td></tr><tr><td><code>.*</code></td><td>匹配<strong>任意长度字符串</strong></td><td><code>^a.*z$</code> → “a开头到z结尾的整行”</td></tr><tr><td><code>&amp;</code></td><td><strong>替换时</strong>引用匹配的文本</td><td><code>s/cat/&amp;s/</code> → “cat”→”cats”</td></tr></tbody></table><hr><h4 id="4-常用模式示例"><a href="#4-常用模式示例" class="headerlink" title="4. 常用模式示例"></a><strong>4. 常用模式示例</strong></h4><table><thead><tr><th>模式</th><th>匹配目标</th></tr></thead><tbody><tr><td><code>^$</code></td><td><strong>空行</strong>（开头即结尾）</td></tr><tr><td><code>^#</code></td><td>以 <code>#</code> 开头的行（如注释）</td></tr><tr><td><code>\.$</code></td><td>以点号结尾的行</td></tr><tr><td><code>/[tT]est/</code></td><td>“test” 或 “Test”</td></tr><tr><td><code>[0-9][0-9]*</code></td><td><strong>至少一个数字</strong></td></tr><tr><td><code>^[^a-zA-Z]</code></td><td>不以字母开头的行</td></tr><tr><td><code>/thing.$/</code></td><td>以thing加任意符号结尾的行</td></tr></tbody></table><hr><h4 id="5-全局命令-g-v"><a href="#5-全局命令-g-v" class="headerlink" title="5. 全局命令 (g/v)"></a><strong>5. 全局命令 (<code>g/v</code>)</strong></h4><ul><li>命令格式：m,ng&#x2F;re&#x2F;cmd，含义是<strong>从m行到n行中对于匹配re模式的行执行命令cmd</strong>. 如果作用范围是全文件(1,$)，m、n可以省略</li></ul><table><thead><tr><th>命令格式</th><th>作用</th><th>示例</th></tr></thead><tbody><tr><td><code>g/re/p</code></td><td><strong>打印</strong>匹配 <code>re</code> 的行</td><td><code>g/error/p</code> → 显示含 “error” 的行</td></tr><tr><td><code>g/re/d</code></td><td><strong>删除</strong>匹配 <code>re</code> 的行</td><td><code>g/^$/d</code> → 删除所有空行</td></tr><tr><td><code>g/re/s/old/new/</code></td><td>对匹配 <code>re</code> 的行替换 <code>old</code>→<code>new</code></td><td><code>g/foo/s/bar/baz/</code> → 仅替换含 “foo” 的行中的 “bar”</td></tr><tr><td><code>v/re/p</code></td><td><strong>取反</strong>操作（不匹配 <code>re</code> 的行）</td><td><code>v/^$/p</code> → 打印非空行</td></tr></tbody></table><hr><h4 id="6-移动与复制"><a href="#6-移动与复制" class="headerlink" title="6. 移动与复制"></a><strong>6. 移动与复制</strong></h4><table><thead><tr><th>命令格式</th><th>作用</th><th>示例</th></tr></thead><tbody><tr><td><code>m,nmd</code></td><td>将 <code>m</code> 到 <code>n</code> 行<strong>移动</strong>到 <code>d</code> 行后</td><td><code>2,4m5</code> → 2-4 行移到第 5 行后</td></tr><tr><td><code>m,ntd</code></td><td>将 <code>m</code> 到 <code>n</code> 行<strong>复制</strong>到 <code>d</code> 行后</td><td><code>1,3t$</code> → 复制 1-3 行到文件末尾</td></tr><tr><td><code>g/^/m0</code></td><td><strong>反转文件行序</strong>（每行移到第 0 行后）</td><td>结果：最后一行变第一行</td></tr></tbody></table><hr><h3 id="关键注意事项"><a href="#关键注意事项" class="headerlink" title="关键注意事项"></a><strong>关键注意事项</strong></h3><ol><li><strong>贪婪匹配</strong>：<code>r*</code> 是贪婪的（尽可能多匹配）。</li><li><strong>转义特殊字符</strong>：如 <code>\.</code> 匹配字面点号，<code>\\</code> 匹配反斜杠。</li><li><strong>全局命令范围</strong>：省略 <code>m,n</code> 时默认全文件（<code>1,$</code>）。</li><li>**替换中的 <code>&amp;</code>**：仅在 <code>s///</code> 右侧有效，代表匹配的全文。</li></ol><p>ed 的正则为现代工具（如 <code>sed</code>&#x2F;<code>grep</code>）奠定了基础，虽功能有限，但足够处理基础文本操作。</p><h2 id="sed流编辑器"><a href="#sed流编辑器" class="headerlink" title="sed流编辑器"></a>sed流编辑器</h2><p>以下是结合参考文献对sed知识点的系统总结，关键部分标注引用来源：</p><hr><h3 id="sed核心特性"><a href="#sed核心特性" class="headerlink" title="sed核心特性"></a><strong>sed核心特性</strong></h3><ol><li><p><strong>流编辑器本质</strong></p><ul><li>直接从<code>ed</code>发展而来，专为处理<strong>输入流</strong>设计（文件或管道数据）</li><li>单次扫描输入流，高效但无法回溯已处理行（局限性）</li><li>默认输出到stdout且<strong>不修改原文件</strong>（需重定向保存结果）</li></ul></li><li><p><strong>基础语法结构</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sed [选项] <span class="hljs-string">&#x27;编辑命令1;编辑命令2&#x27;</span> 输入文件<br></code></pre></td></tr></table></figure><ul><li>经典示例：<code>sed &#39;s/UNIX/Linux/g&#39; file</code> 全局替换文本</li><li>支持多命令组合：<code>sed &#39;s/foo/bar/; 10q&#39; file</code>（替换+显示前10行）</li></ul></li></ol><hr><h3 id="常用操作场景"><a href="#常用操作场景" class="headerlink" title="常用操作场景"></a><strong>常用操作场景</strong></h3><table><thead><tr><th>功能</th><th>命令示例</th><th>说明</th></tr></thead><tbody><tr><td><strong>文本替换</strong></td><td><code>sed &#39;s/old/new/g&#39; file</code></td><td>全局替换（加<code>g</code>标志）</td></tr><tr><td><strong>行范围操作</strong></td><td><code>sed &#39;10q&#39; file</code></td><td>显示前10行（<code>q</code>退出命令）</td></tr><tr><td><strong>模式过滤</strong></td><td><code>sed -n &#39;/pattern/p&#39; file</code></td><td>仅显示匹配行（<code>-n</code>关闭默认输出，等效<code>grep</code>）</td></tr><tr><td><strong>目录列表过滤</strong></td><td>&#96;ls -l</td><td>sed -n ‘&#x2F;^d&#x2F;p’&#96;</td></tr><tr><td><strong>多模式范围输出</strong></td><td><code>sed -n &#39;/start/,/end/p&#39; file</code></td><td>打印两个模式间的所有行（如<code>/David/,/Emily/p</code>）</td></tr></tbody></table><hr><h3 id="关键选项与限制"><a href="#关键选项与限制" class="headerlink" title="关键选项与限制"></a><strong>关键选项与限制</strong></h3><ul><li><p><strong>选项</strong></p><ul><li><code>-n</code>：抑制默认输出，需显式用<code>p</code>命令打印</li><li><code>-e</code>：指定多个编辑命令（如<code>sed -e &#39;s/a/b/&#39; -e &#39;s/c/d/&#39;</code>）</li></ul></li><li><p><strong>局限性</strong></p><ul><li><strong>无法行运算</strong>：如<code>$-1d</code>非法，因sed是单向流处理器</li><li><strong>无交互模式</strong>：所有操作需通过命令预设</li></ul></li></ul><hr><h3 id="与管道的协同"><a href="#与管道的协同" class="headerlink" title="与管道的协同"></a><strong>与管道的协同</strong></h3><ul><li><p><strong>动态处理流数据</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">who</span> | sed <span class="hljs-string">&#x27;s/ .*//&#x27;</span>  <span class="hljs-comment"># 提取用户名（删除首个空格后的所有内容）</span><br></code></pre></td></tr></table></figure></li><li><p><strong>组合其他命令</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grep <span class="hljs-string">&quot;error&quot;</span> log.txt | sed <span class="hljs-string">&#x27;s/ERROR/CRITICAL/g&#x27;</span>  <span class="hljs-comment"># 二次过滤替换</span><br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="扩展应用"><a href="#扩展应用" class="headerlink" title="扩展应用"></a><strong>扩展应用</strong></h3><ul><li><p><strong>批量编辑文件</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sed -i.bak <span class="hljs-string">&#x27;s/old/new/g&#x27;</span> *.txt  <span class="hljs-comment"># 直接修改文件并备份（`-i`选项）</span><br></code></pre></td></tr></table></figure></li><li><p><strong>条件删除</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sed <span class="hljs-string">&#x27;/pattern/d&#x27;</span> file  <span class="hljs-comment"># 删除匹配行（如空行`/^$/d`）</span><br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="设计哲学"><a href="#设计哲学" class="headerlink" title="设计哲学"></a><strong>设计哲学</strong></h3><ul><li><strong>UNIX工具链思维</strong>：专注单一功能（文本转换），通过管道与其他工具（如<code>grep</code>、<code>awk</code>）协作[][6]</li><li><strong>高效性</strong>：单次流处理比交互式编辑器更适合自动化任务[][6]</li></ul><p>通过掌握这些核心模式，可高效完成90%的文本处理需求。进阶学习可参考GNU sed手册[6]中的正则表达式和保持空间（hold space）技巧。</p><h2 id="grep过滤器"><a href="#grep过滤器" class="headerlink" title="grep过滤器"></a>grep过滤器</h2><p><code>grep</code> 是 Linux&#x2F;Unix 中最强大的<strong>文本过滤工具</strong>之一，常与管道符 <code>|</code> 和其他命令（如 <code>ls</code>、<code>sed</code>、<code>sort</code>）结合使用</p><hr><h3 id="1-grep-基础语法"><a href="#1-grep-基础语法" class="headerlink" title="1. grep 基础语法"></a><strong>1. <code>grep</code> 基础语法</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grep [选项] <span class="hljs-string">&#x27;搜索模式&#x27;</span> 文件名<br></code></pre></td></tr></table></figure><ul><li><strong>核心功能</strong>：从文件或输入流中筛选<strong>包含指定模式的行</strong>。</li><li><strong>常用选项</strong>：<ul><li><code>-i</code>：忽略大小写（如 <code>grep -i &quot;error&quot; log.txt</code>）。</li><li><code>-v</code>：反向匹配（显示<strong>不包含</strong>模式的行）。</li><li><code>-n</code>：显示匹配行的行号。</li><li><code>-c</code>：统计匹配行的数量。</li><li><code>-r</code>：递归搜索目录（如 <code>grep -r &quot;main&quot; /src</code>）。</li></ul></li></ul><hr><h3 id="示例-1：搜索文件内容"><a href="#示例-1：搜索文件内容" class="headerlink" title="示例 1：搜索文件内容"></a><strong>示例 1：搜索文件内容</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grep <span class="hljs-string">&#x27;^void&#x27;</span> sig.c<br></code></pre></td></tr></table></figure><ul><li><strong>作用</strong>：在 <code>sig.c</code> 文件中查找<strong>以 <code>void</code> 开头的行</strong>。</li></ul><h3 id="示例-2：过滤目录"><a href="#示例-2：过滤目录" class="headerlink" title="示例 2：过滤目录"></a><strong>示例 2：过滤目录</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> -l | grep <span class="hljs-string">&#x27;^d&#x27;</span><br></code></pre></td></tr></table></figure><ul><li><strong>作用</strong>：列出当前目录下的<strong>子目录</strong>（不包括文件）。</li><li><strong>原理</strong>：<ul><li><code>ls -l</code> 的输出中，目录行以 <code>d</code> 开头（如 <code>drwxr-xr-x</code>）。</li><li><code>^d</code> 匹配行首的 <code>d</code> 字符。</li></ul></li></ul><h3 id="示例-3：提取目录名"><a href="#示例-3：提取目录名" class="headerlink" title="示例 3：提取目录名"></a><strong>示例 3：提取目录名</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> -l | grep <span class="hljs-string">&#x27;^d&#x27;</span> | sed <span class="hljs-string">&#x27;s/.* //&#x27;</span><br></code></pre></td></tr></table></figure><ul><li><strong>作用</strong>：仅输出子目录名（去掉权限、时间等冗余信息）。</li><li><strong>分步解析</strong>：<ol><li><code>ls -l | grep &#39;^d&#39;</code>：筛选目录行。</li><li><code>sed &#39;s/.* //&#39;</code>：<strong>删除行中最后一个空格前的所有内容</strong>（保留目录名）。</li></ol></li></ul><hr><h3 id="3-grep-家族对比"><a href="#3-grep-家族对比" class="headerlink" title="3. grep 家族对比"></a><strong>3. <code>grep</code> 家族对比</strong></h3><table><thead><tr><th>命令</th><th>全称</th><th>特点</th></tr></thead><tbody><tr><td><code>grep</code></td><td>Global Regular Expression Print</td><td>基础版，支持<strong>基本正则表达式</strong>（BRE）。</td></tr><tr><td><code>egrep</code></td><td>Extended GREP</td><td>支持<strong>扩展正则表达式</strong>（ERE），如 <code>+</code>、<code>?</code>、&#96;</td></tr><tr><td><code>fgrep</code></td><td>Fixed-string GREP</td><td>仅匹配固定字符串（<strong>不支持正则</strong>），但处理大文件时速度最快<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="`fgrep` 相当于 `grep -F`，适合精确匹配固定字符串（如日志中的错误码）。">[2]</span></a></sup>。</td></tr></tbody></table><h3 id="典型场景"><a href="#典型场景" class="headerlink" title="典型场景"></a><strong>典型场景</strong></h3><ol><li><strong><code>egrep</code> 的扩展功能</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">egrep <span class="hljs-string">&#x27;error|warn&#x27;</span> log.txt  <span class="hljs-comment"># 同时匹配 &quot;error&quot; 或 &quot;warn&quot;</span><br></code></pre></td></tr></table></figure></li><li><strong><code>fgrep</code> 的高效性</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fgrep -c <span class="hljs-string">&quot;404&quot;</span> access.log  <span class="hljs-comment"># 快速统计 404 错误出现的次数</span><br></code></pre></td></tr></table></figure></li></ol><hr><h3 id="4-结合其他过滤器"><a href="#4-结合其他过滤器" class="headerlink" title="4. 结合其他过滤器"></a><strong>4. 结合其他过滤器</strong></h3><h3 id="4-1-sort（排序）"><a href="#4-1-sort（排序）" class="headerlink" title="4.1 sort（排序）"></a><strong>4.1 <code>sort</code>（排序）</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> | <span class="hljs-built_in">sort</span>          <span class="hljs-comment"># 按字母排序文件名</span><br>grep <span class="hljs-string">&quot;error&quot;</span> log.txt | <span class="hljs-built_in">sort</span> -u  <span class="hljs-comment"># 去重后排序</span><br></code></pre></td></tr></table></figure><h3 id="4-2-head-tail（首尾行）"><a href="#4-2-head-tail（首尾行）" class="headerlink" title="4.2 head &#x2F; tail（首尾行）"></a><strong>4.2 <code>head</code> &#x2F; <code>tail</code>（首尾行）</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">grep <span class="hljs-string">&quot;Exception&quot;</span> server.log | <span class="hljs-built_in">tail</span> -5  <span class="hljs-comment"># 显示最后5个异常</span><br>grep <span class="hljs-string">&quot;login&quot;</span> auth.log | <span class="hljs-built_in">head</span> -3       <span class="hljs-comment"># 显示前3次登录记录</span><br></code></pre></td></tr></table></figure><h3 id="4-3-组合实战"><a href="#4-3-组合实战" class="headerlink" title="4.3 组合实战"></a><strong>4.3 组合实战</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 统计日志中每个错误类型的出现次数（排序后取前5）</span><br>grep -o <span class="hljs-string">&quot;ERROR: .*&quot;</span> app.log | <span class="hljs-built_in">sort</span> | <span class="hljs-built_in">uniq</span> -c | <span class="hljs-built_in">sort</span> -nr | <span class="hljs-built_in">head</span> -5<br></code></pre></td></tr></table></figure><ul><li><strong>步骤分解</strong>：<ol><li><code>grep -o</code>：提取所有 <code>ERROR: xxx</code> 内容。</li><li><code>sort</code>：排序以便 <code>uniq</code> 统计。</li><li><code>uniq -c</code>：计数。</li><li><code>sort -nr</code>：按计数降序。</li><li><code>head -5</code>：显示前5条。</li></ol></li></ul><hr><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h2><ul><li><strong><code>grep</code><strong>：文本过滤的瑞士军刀，核心是</strong>模式匹配</strong>。</li><li><strong>管道组合</strong>：与 <code>sed</code>、<code>sort</code>、<code>head</code>&#x2F;<code>tail</code> 联动可解决复杂问题。</li><li><strong>选择工具</strong>：<ul><li>简单搜索 → <code>grep</code> 或 <code>fgrep</code>。</li><li>复杂正则 → <code>egrep</code>。</li><li>需排序&#x2F;统计 → 结合 <code>sort | uniq -c</code>。</li></ul></li></ul><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><code>egrep</code> 相当于 <code>grep -E</code>，支持 <code>|</code>（或）、<code>+</code>（1次或多次）等扩展语法。<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><code>fgrep</code> 相当于 <code>grep -F</code>，适合精确匹配固定字符串（如日志中的错误码）。<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>UNIX</tag>
      
      <tag>复习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UNIX系统复习-1</title>
    <link href="/2025/05/27/UNIX%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0-1/"/>
    <url>/2025/05/27/UNIX%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0-1/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是我复习《UNIX系统》这门课程时记录的一些笔记 ，希望能对你有所帮助😊</p>          </div><h1 id="ed-编辑器"><a href="#ed-编辑器" class="headerlink" title="ed 编辑器"></a>ed 编辑器</h1><p>ed 是 UNIX 系统中最古老的行编辑器之一，虽然现在使用较少，但了解它的用法对于理解 UNIX 历史和许多现代编辑器的工作原理很有帮助。以下是 ed 编辑器的详细用法和实例。</p><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="启动和退出-ed"><a href="#启动和退出-ed" class="headerlink" title="启动和退出 ed"></a>启动和退出 ed</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cmake">$ ed                 <span class="hljs-comment"># 启动 ed 编辑器</span><br>a                    <span class="hljs-comment"># 进入输入模式</span><br>这是第一行文本<br>这是第二行文本<br>.                    <span class="hljs-comment"># 退出输入模式</span><br>w <span class="hljs-keyword">test</span>.txt           <span class="hljs-comment"># 保存到文件</span><br>q                    <span class="hljs-comment"># 退出 ed</span><br></code></pre></td></tr></table></figure><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><ol><li><p><strong><code>a</code> - 追加文本</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">a</span><br>这是追加的文本<br>.<br></code></pre></td></tr></table></figure></li><li><p><strong><code>i</code> - 插入文本</strong></p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-number">1</span>i                 <span class="hljs-meta"># 在第一行前插入</span><br>这是插入的文本<br>.<br></code></pre></td></tr></table></figure></li><li><p><strong><code>p</code> - 打印行</strong></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>,<span class="hljs-number">3</span>p              # 打印<span class="hljs-number">1</span>-<span class="hljs-number">3</span>行<br></code></pre></td></tr></table></figure></li><li><p><strong><code>d</code> - 删除行</strong></p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-number">2</span>d                <span class="hljs-meta"># 删除第2行</span><br></code></pre></td></tr></table></figure></li><li><p><strong><code>w</code> - 写入文件</strong></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">w</span> filename        <span class="hljs-comment"># 保存到文件</span><br></code></pre></td></tr></table></figure></li><li><p><strong><code>q</code> - 退出</strong></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">q</span>                 <span class="hljs-comment"># 退出编辑器</span><br></code></pre></td></tr></table></figure></li></ol><h2 id="实用示例"><a href="#实用示例" class="headerlink" title="实用示例"></a>实用示例</h2><h3 id="示例1：创建和编辑文件"><a href="#示例1：创建和编辑文件" class="headerlink" title="示例1：创建和编辑文件"></a>示例1：创建和编辑文件</h3><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs gauss">$ <span class="hljs-keyword">ed</span><br>a                   <span class="hljs-meta"># 开始输入</span><br>Hello, world!<br>This is <span class="hljs-keyword">line</span> <span class="hljs-number">2.</span><br><span class="hljs-keyword">Line</span> <span class="hljs-number">3</span> here.<br>.                   <span class="hljs-meta"># 结束输入</span><br><span class="hljs-number">1</span>,<span class="hljs-number">3</span>p                <span class="hljs-meta"># 查看所有内容</span><br>w hello.txt         <span class="hljs-meta"># 保存</span><br>q                   <span class="hljs-meta"># 退出</span><br></code></pre></td></tr></table></figure><h3 id="示例2：编辑现有文件"><a href="#示例2：编辑现有文件" class="headerlink" title="示例2：编辑现有文件"></a>示例2：编辑现有文件</h3><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs gauss">$ <span class="hljs-keyword">ed</span> existing.txt<br><span class="hljs-number">1</span>,<span class="hljs-number">3</span>p                <span class="hljs-meta"># 查看前3行</span><br><span class="hljs-number">2</span>d                  <span class="hljs-meta"># 删除第2行</span><br>a                   <span class="hljs-meta"># 追加新内容</span><br><span class="hljs-keyword">New</span> <span class="hljs-keyword">line</span> added<br>.<br>w                   <span class="hljs-meta"># 保存更改</span><br>q                   <span class="hljs-meta"># 退出</span><br></code></pre></td></tr></table></figure><h3 id="示例3：搜索和替换"><a href="#示例3：搜索和替换" class="headerlink" title="示例3：搜索和替换"></a>示例3：搜索和替换</h3><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs dts">$ ed document.txt<br><span class="hljs-number">1</span>,$p                <span class="hljs-meta"># 查看所有内容</span><br><span class="hljs-keyword">/pattern/</span>           <span class="hljs-meta"># 搜索<span class="hljs-string">&quot;pattern&quot;</span></span><br>s<span class="hljs-keyword">/old/</span>new/          <span class="hljs-meta"># 替换当前行的第一个<span class="hljs-string">&quot;old&quot;</span>为<span class="hljs-string">&quot;new&quot;</span></span><br>s<span class="hljs-keyword">/old/</span>new/g         <span class="hljs-meta"># 替换当前行所有<span class="hljs-string">&quot;old&quot;</span>为<span class="hljs-string">&quot;new&quot;</span></span><br><span class="hljs-number">1</span>,$s<span class="hljs-keyword">/old/</span>new/g      <span class="hljs-meta"># 全局替换</span><br>w                   <span class="hljs-meta"># 保存</span><br>q                   <span class="hljs-meta"># 退出</span><br></code></pre></td></tr></table></figure><h3 id="示例4：使用行号编辑"><a href="#示例4：使用行号编辑" class="headerlink" title="示例4：使用行号编辑"></a>示例4：使用行号编辑</h3><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs gauss">$ <span class="hljs-keyword">ed</span> data.txt<br><span class="hljs-number">3</span>                   <span class="hljs-meta"># 移动到第3行</span><br>p                   <span class="hljs-meta"># 打印当前行</span><br>a                   <span class="hljs-meta"># 在第3行后追加</span><br>Additional info<br>.<br><span class="hljs-number">2</span>,<span class="hljs-number">4</span>p                <span class="hljs-meta"># 查看2-4行</span><br><span class="hljs-number">5</span>i                  <span class="hljs-meta"># 在第5行前插入</span><br>Inserted <span class="hljs-keyword">line</span><br>.<br>w                   <span class="hljs-meta"># 保存</span><br>q                   <span class="hljs-meta"># 退出</span><br></code></pre></td></tr></table></figure><h2 id="高级功能"><a href="#高级功能" class="headerlink" title="高级功能"></a>高级功能</h2><h3 id="正则表达式搜索"><a href="#正则表达式搜索" class="headerlink" title="正则表达式搜索"></a>正则表达式搜索</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/^[A-Z]/</span>            <span class="hljs-comment"># 搜索以大写字母开头的行</span><br><span class="hljs-regexp">/[0-9]$/</span>            <span class="hljs-comment"># 搜索以数字结尾的行</span><br></code></pre></td></tr></table></figure><h3 id="全局命令"><a href="#全局命令" class="headerlink" title="全局命令"></a>全局命令</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">g</span>/<span class="hljs-selector-tag">pattern</span>/<span class="hljs-selector-tag">p</span>         # 打印所有包含<span class="hljs-selector-tag">pattern</span>的行<br><span class="hljs-selector-tag">g</span>/<span class="hljs-selector-tag">pattern</span>/d         # 删除所有包含<span class="hljs-selector-tag">pattern</span>的行<br>v/<span class="hljs-selector-tag">pattern</span>/<span class="hljs-selector-tag">p</span>         # 打印所有不包含<span class="hljs-selector-tag">pattern</span>的行<br></code></pre></td></tr></table></figure><h3 id="标记位置"><a href="#标记位置" class="headerlink" title="标记位置"></a>标记位置</h3><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs llvm">kx                  # 用字母<span class="hljs-keyword">x</span>标记当前位置<br>&#x27;<span class="hljs-keyword">x</span>                  # 返回标记<span class="hljs-keyword">x</span>的位置<br></code></pre></td></tr></table></figure><h3 id="合并文件"><a href="#合并文件" class="headerlink" title="合并文件"></a>合并文件</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs powershell"><span class="hljs-variable">$</span> ed main.txt<br><span class="hljs-built_in">r</span> other.txt         <span class="hljs-comment"># 将other.txt内容读入当前文件</span><br></code></pre></td></tr></table></figure><h2 id="实际应用场景"><a href="#实际应用场景" class="headerlink" title="实际应用场景"></a>实际应用场景</h2><h3 id="场景1：批量替换文本"><a href="#场景1：批量替换文本" class="headerlink" title="场景1：批量替换文本"></a>场景1：批量替换文本</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">$ ed config<span class="hljs-selector-class">.cfg</span><br><span class="hljs-number">1</span>,<span class="hljs-variable">$s</span>/<span class="hljs-number">192.168</span>.<span class="hljs-number">1.1</span>/<span class="hljs-number">10.0</span>.<span class="hljs-number">0.1</span>/<span class="hljs-selector-tag">g</span><br>w<br>q<br></code></pre></td></tr></table></figure><h3 id="场景2：提取特定行"><a href="#场景2：提取特定行" class="headerlink" title="场景2：提取特定行"></a>场景2：提取特定行</h3><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs 1c">$ ed logfile.<span class="hljs-built_in">log</span><br><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-number">10</span>p               <span class="hljs-meta"># 查看前10行</span><br><span class="hljs-number">5</span><span class="hljs-punctuation">,</span><span class="hljs-number">7</span>w extract.txt    <span class="hljs-meta"># 将5-7行保存到extract.txt</span><br>q<br></code></pre></td></tr></table></figure><h3 id="场景3：快速编辑脚本"><a href="#场景3：快速编辑脚本" class="headerlink" title="场景3：快速编辑脚本"></a>场景3：快速编辑脚本</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">$ ed script.sh<br><span class="hljs-regexp">/function_name/</span>     <span class="hljs-comment"># 找到函数定义</span><br>.,+<span class="hljs-number">5</span>p               <span class="hljs-comment"># 查看函数及其后5行</span><br>.,+<span class="hljs-number">5</span>s<span class="hljs-regexp">/old/</span>new/g     <span class="hljs-comment"># 替换函数内的文本</span><br>w<br>q<br></code></pre></td></tr></table></figure><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol><li>ed 是行编辑器，没有全屏界面，所有操作基于命令</li><li>命令区分大小写</li><li>点(.)单独一行表示结束输入模式</li><li>如果不保存就退出，会丢失所有更改</li><li>错误操作可能导致数据丢失，建议经常保存</li></ol><p>虽然 ed 编辑器在现代环境中使用较少，但了解它的基本操作对于理解 UNIX 哲学和许多现代文本处理工具的历史发展很有帮助。</p><h1 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h1><h2 id="文件操作命令"><a href="#文件操作命令" class="headerlink" title="文件操作命令"></a>文件操作命令</h2><h3 id="1-mv-移动或重命名文件"><a href="#1-mv-移动或重命名文件" class="headerlink" title="1. mv - 移动或重命名文件"></a>1. <code>mv</code> - 移动或重命名文件</h3><p><strong>基本语法</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mv</span> [选项] 源文件 目标文件<br><span class="hljs-built_in">mv</span> [选项] 源文件... 目标目录<br></code></pre></td></tr></table></figure><p><strong>常用选项</strong>:</p><ul><li><code>-i</code>: 交互式操作，覆盖前提示</li><li><code>-f</code>: 强制覆盖，不提示</li><li><code>-n</code>: 不覆盖已存在文件</li><li><code>-v</code>: 显示详细操作信息</li></ul><p><strong>示例</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mv</span> file.txt newname.txt          <span class="hljs-comment"># 重命名文件</span><br><span class="hljs-built_in">mv</span> file.txt ~/Documents/        <span class="hljs-comment"># 移动文件到目录</span><br><span class="hljs-built_in">mv</span> -i old.txt new.txt           <span class="hljs-comment"># 交互式重命名</span><br><span class="hljs-built_in">mv</span> *.txt text_files/            <span class="hljs-comment"># 移动所有txt文件到目录</span><br><span class="hljs-built_in">mv</span> -v file1 file2 <span class="hljs-built_in">dir</span>/          <span class="hljs-comment"># 显示详细移动信息</span><br></code></pre></td></tr></table></figure><h3 id="2-cp-复制文件"><a href="#2-cp-复制文件" class="headerlink" title="2. cp - 复制文件"></a>2. <code>cp</code> - 复制文件</h3><p><strong>基本语法</strong>:</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-keyword">cp</span> [选项] 源文件 目标文件<br><span class="hljs-keyword">cp</span> [选项] 源文件... 目标目录<br></code></pre></td></tr></table></figure><p><strong>常用选项</strong>:</p><ul><li><code>-i</code>: 交互式复制</li><li><code>-r</code>&#x2F;<code>-R</code>: 递归复制目录</li><li><code>-p</code>: 保留文件属性(权限、时间戳等)</li><li><code>-a</code>: 归档模式，相当于<code>-dpR</code></li><li><code>-v</code>: 显示详细操作信息</li></ul><p><strong>示例</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> file.txt file_backup.txt      <span class="hljs-comment"># 复制文件</span><br><span class="hljs-built_in">cp</span> -r dir1/ dir2/               <span class="hljs-comment"># 递归复制目录</span><br><span class="hljs-built_in">cp</span> -p file.txt backup/          <span class="hljs-comment"># 保留属性复制</span><br><span class="hljs-built_in">cp</span> -a project/ backup/          <span class="hljs-comment"># 完全复制目录结构</span><br><span class="hljs-built_in">cp</span> *.txt text_backup/           <span class="hljs-comment"># 复制所有txt文件</span><br></code></pre></td></tr></table></figure><h3 id="3-rm-删除文件"><a href="#3-rm-删除文件" class="headerlink" title="3. rm - 删除文件"></a>3. <code>rm</code> - 删除文件</h3><p><strong>基本语法</strong>:</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs prolog">rm [选项] 文件...<br></code></pre></td></tr></table></figure><p><strong>常用选项</strong>:</p><ul><li><code>-i</code>: 交互式删除</li><li><code>-f</code>: 强制删除，不提示</li><li><code>-r</code>&#x2F;<code>-R</code>: 递归删除目录</li><li><code>-v</code>: 显示详细操作信息</li></ul><p><strong>示例</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">rm</span> file.txt                     <span class="hljs-comment"># 删除文件</span><br><span class="hljs-built_in">rm</span> -i *.tmp                     <span class="hljs-comment"># 交互式删除tmp文件</span><br><span class="hljs-built_in">rm</span> -rf old_dir/                 <span class="hljs-comment"># 强制递归删除目录(慎用)</span><br><span class="hljs-built_in">rm</span> -v *.<span class="hljs-built_in">log</span>                     <span class="hljs-comment"># 显示删除的日志文件</span><br></code></pre></td></tr></table></figure><h2 id="文本处理命令"><a href="#文本处理命令" class="headerlink" title="文本处理命令"></a>文本处理命令</h2><h3 id="4-wc-统计文本信息"><a href="#4-wc-统计文本信息" class="headerlink" title="4. wc - 统计文本信息"></a>4. <code>wc</code> - 统计文本信息</h3><p><strong>基本语法</strong>:</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs prolog">wc [选项] 文件...<br></code></pre></td></tr></table></figure><p><strong>常用选项</strong>:</p><ul><li><code>-l</code>: 只统计行数</li><li><code>-w</code>: 只统计单词数</li><li><code>-c</code>: 只统计字节数</li><li><code>-m</code>: 只统计字符数</li><li><code>-L</code>: 显示最长行的长度</li></ul><p><strong>示例</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">wc</span> file.txt                     <span class="hljs-comment"># 显示行数、单词数、字节数</span><br><span class="hljs-built_in">wc</span> -l *.<span class="hljs-built_in">log</span>                     <span class="hljs-comment"># 统计所有日志文件行数</span><br><span class="hljs-built_in">wc</span> -w report.txt                <span class="hljs-comment"># 统计文件单词数</span><br><span class="hljs-built_in">wc</span> -L longfile.txt              <span class="hljs-comment"># 找出最长行的长度</span><br><span class="hljs-built_in">cat</span> file.txt | <span class="hljs-built_in">wc</span> -c            <span class="hljs-comment"># 统计字节数(管道用法)</span><br></code></pre></td></tr></table></figure><h3 id="5-grep-文本搜索"><a href="#5-grep-文本搜索" class="headerlink" title="5. grep - 文本搜索"></a>5. <code>grep</code> - 文本搜索</h3><p><strong>基本语法</strong>:</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs prolog">grep [选项] 模式 文件...<br></code></pre></td></tr></table></figure><p><strong>常用选项</strong>:</p><ul><li><code>-i</code>: 忽略大小写</li><li><code>-v</code>: 反向匹配(显示不匹配的行)</li><li><code>-n</code>: 显示行号</li><li><code>-c</code>: 统计匹配行数</li><li><code>-r</code>&#x2F;<code>-R</code>: 递归搜索目录</li><li><code>-l</code>: 只显示匹配的文件名</li><li><code>-E</code>: 使用扩展正则表达式(等同于egrep)</li><li><code>-A n</code>: 显示匹配行及其后n行</li><li><code>-B n</code>: 显示匹配行及其前n行</li><li><code>-C n</code>: 显示匹配行及其前后各n行</li></ul><p><strong>示例</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">grep <span class="hljs-string">&quot;error&quot;</span> logfile.txt        <span class="hljs-comment"># 搜索包含error的行</span><br>grep -i <span class="hljs-string">&quot;warning&quot;</span> *.<span class="hljs-built_in">log</span>         <span class="hljs-comment"># 忽略大小写搜索warning</span><br>grep -rn <span class="hljs-string">&quot;main()&quot;</span> src/          <span class="hljs-comment"># 递归搜索并显示行号</span><br>grep -c <span class="hljs-string">&quot;success&quot;</span> report.txt    <span class="hljs-comment"># 统计匹配行数</span><br>grep -v <span class="hljs-string">&quot;^#&quot;</span> config.cfg         <span class="hljs-comment"># 显示不以#开头的行(过滤注释)</span><br>grep -E <span class="hljs-string">&quot;[0-9]&#123;3&#125;-[0-9]&#123;4&#125;&quot;</span> data.txt  <span class="hljs-comment"># 使用正则表达式搜索电话号码</span><br></code></pre></td></tr></table></figure><h3 id="6-sort-排序文本"><a href="#6-sort-排序文本" class="headerlink" title="6. sort - 排序文本"></a>6. <code>sort</code> - 排序文本</h3><p><strong>基本语法</strong>:</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs prolog">sort [选项] 文件...<br></code></pre></td></tr></table></figure><p><strong>常用选项</strong>:</p><ul><li><code>-n</code>: 按数值大小排序</li><li><code>-r</code>: 逆序排序</li><li><code>-k n</code>: 按第n列排序</li><li><code>-t c</code>: 指定列分隔符为c</li><li><code>-u</code>: 去除重复行</li><li><code>-f</code>: 忽略大小写</li><li><code>-o 文件</code>: 将结果输出到指定文件</li></ul><p><strong>示例</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sort</span> names.txt                  <span class="hljs-comment"># 按字母顺序排序</span><br><span class="hljs-built_in">sort</span> -nr numbers.txt           <span class="hljs-comment"># 按数值逆序排序</span><br><span class="hljs-built_in">sort</span> -t: -k3 -n /etc/passwd     <span class="hljs-comment"># 按:分隔的第3列(UID)数值排序</span><br><span class="hljs-built_in">sort</span> -u data.txt                <span class="hljs-comment"># 去重排序</span><br><span class="hljs-built_in">sort</span> -k2,2 -k1,1r employees.txt <span class="hljs-comment"># 先按第2列升序，再按第1列降序</span><br></code></pre></td></tr></table></figure><h3 id="7-cmp-比较文件差异-二进制"><a href="#7-cmp-比较文件差异-二进制" class="headerlink" title="7. cmp - 比较文件差异(二进制)"></a>7. <code>cmp</code> - 比较文件差异(二进制)</h3><p><strong>基本语法</strong>:</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-keyword">cmp</span> [选项] 文件<span class="hljs-number">1</span> 文件<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p><strong>常用选项</strong>:</p><ul><li><code>-l</code>: 显示所有不同字节的位置和值</li><li><code>-s</code>: 静默模式，只返回退出状态</li><li><code>-i n</code>: 跳过前n个字节</li><li><code>-n count</code>: 最多比较count个字节</li></ul><p><strong>示例</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">cmp file1.bin file2.bin         <span class="hljs-comment"># 基本比较</span><br>cmp -l image1.jpg image2.jpg    <span class="hljs-comment"># 显示所有差异字节</span><br>cmp -s old new &amp;&amp; <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;相同&quot;</span> || <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;不同&quot;</span>  <span class="hljs-comment"># 脚本中检查文件是否相同</span><br>cmp -i 1024 a.out b.out        <span class="hljs-comment"># 跳过前1024字节比较</span><br></code></pre></td></tr></table></figure><h3 id="8-diff-比较文件差异-文本"><a href="#8-diff-比较文件差异-文本" class="headerlink" title="8. diff - 比较文件差异(文本)"></a>8. <code>diff</code> - 比较文件差异(文本)</h3><p><strong>基本语法</strong>:</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs css">diff <span class="hljs-selector-attr">[选项]</span> 文件<span class="hljs-number">1</span> 文件<span class="hljs-number">2</span><br>diff <span class="hljs-selector-attr">[选项]</span> 目录<span class="hljs-number">1</span> 目录<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p><strong>常用选项</strong>:</p><ul><li><code>-c</code>: 上下文格式输出</li><li><code>-u</code>: 统一格式输出</li><li><code>-i</code>: 忽略大小写</li><li><code>-w</code>: 忽略空白差异</li><li><code>-r</code>: 递归比较目录</li><li><code>-q</code>: 只报告文件是否不同</li><li><code>-y</code>: 并排格式输出</li><li><code>-a</code>: 将文件视为文本文件(即使看似二进制)</li></ul><p><strong>示例</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">diff file1.txt file2.txt        <span class="hljs-comment"># 基本比较</span><br>diff -u old.c new.c &gt; patch.diff <span class="hljs-comment"># 生成补丁文件</span><br>diff -r dir1/ dir2/             <span class="hljs-comment"># 递归比较目录</span><br>diff -iw config.cfg config.bak  <span class="hljs-comment"># 忽略大小写和空白比较</span><br>diff -y left.txt right.txt      <span class="hljs-comment"># 并排显示差异</span><br></code></pre></td></tr></table></figure><h2 id="组合使用示例"><a href="#组合使用示例" class="headerlink" title="组合使用示例"></a>组合使用示例</h2><ol><li><p><strong>统计当前目录下所有.py文件的行数</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">wc</span> -l *.py<br></code></pre></td></tr></table></figure></li><li><p><strong>查找包含”TODO”的Python文件并显示行号</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grep -n <span class="hljs-string">&quot;TODO&quot;</span> *.py<br></code></pre></td></tr></table></figure></li><li><p><strong>按文件大小排序当前目录下的文件</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> -l | <span class="hljs-built_in">sort</span> -nk5<br></code></pre></td></tr></table></figure></li><li><p><strong>比较两个目录的差异并生成补丁</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">diff -Nur original/ modified/ &gt; changes.patch<br></code></pre></td></tr></table></figure></li><li><p><strong>查找并删除所有临时文件</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">find . -name <span class="hljs-string">&quot;*.tmp&quot;</span> | xargs <span class="hljs-built_in">rm</span> -v<br></code></pre></td></tr></table></figure></li></ol><p>这些命令是UNIX&#x2F;Linux系统中最常用的文本处理工具，熟练掌握它们可以极大提高在命令行环境下的工作效率。</p><h1 id="环境变量设置"><a href="#环境变量设置" class="headerlink" title="环境变量设置"></a>环境变量设置</h1><p>这些命令涉及 <strong>Linux&#x2F;Unix shell 环境变量</strong> 的设置和使用，我来逐个解释：</p><h3 id="1-PS1-C"><a href="#1-PS1-C" class="headerlink" title="1. PS1=C:\\&gt;"></a><strong>1. <code>PS1=C:\\&gt;</code></strong></h3><ul><li><strong>作用</strong>：修改 shell 的 <strong>命令提示符（Prompt String 1）</strong></li><li><strong>效果</strong>：<ul><li><code>PS1</code> 是控制终端提示符的变量</li><li><code>C:\\&gt;</code> 表示提示符会显示为 <code>C:\&gt;</code>（类似 Windows 的 CMD 风格）</li></ul></li><li><strong>示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ PS1=C:\\&gt;<br>C:\&gt;  <span class="hljs-comment"># 现在提示符变成了这样</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="2-PS1-whoami-pwd"><a href="#2-PS1-whoami-pwd" class="headerlink" title="2. PS1=&#39;whoami:pwd &gt;&#39;"></a><strong>2. <code>PS1=&#39;</code>whoami<code>:</code>pwd<code> &gt;&#39;</code></strong></h3><ul><li><strong>作用</strong>：自定义提示符，显示 <strong>当前用户 + 当前目录</strong></li><li><strong>解释</strong>：<ul><li><code>`whoami`</code>：执行 <code>whoami</code> 命令，显示当前用户名（如 <code>root</code> 或 <code>user</code>）</li><li><code>`pwd`</code>：执行 <code>pwd</code> 命令，显示当前工作目录（如 <code>/home/user</code>）</li><li>最终提示符格式：<code>用户名:当前目录 &gt;</code></li></ul></li><li><strong>示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ PS1=<span class="hljs-string">&#x27;`whoami`:`pwd` &gt;&#x27;</span><br>user:/home/user &gt;  <span class="hljs-comment"># 提示符变成这样</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="3-echo-HOME"><a href="#3-echo-HOME" class="headerlink" title="3. echo $HOME"></a><strong>3. <code>echo $HOME</code></strong></h3><ul><li><strong>作用</strong>：显示当前用户的 <strong>家目录（Home Directory）</strong></li><li><strong>解释</strong>：<ul><li><code>$HOME</code> 是环境变量，存储用户的家目录路径（如 <code>/home/user</code>）</li></ul></li><li><strong>示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">echo</span> <span class="hljs-variable">$HOME</span><br>/home/user<br></code></pre></td></tr></table></figure></li></ul><h3 id="4-echo-PATH"><a href="#4-echo-PATH" class="headerlink" title="4. echo $PATH"></a><strong>4. <code>echo $PATH</code></strong></h3><ul><li><strong>作用</strong>：显示 <strong>可执行程序的搜索路径</strong></li><li><strong>解释</strong>：<ul><li><code>$PATH</code> 是环境变量，包含多个目录路径，用 <code>:</code> 分隔</li><li>当输入命令时，shell 会按 <code>PATH</code> 中的顺序查找可执行文件</li></ul></li><li><strong>示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">echo</span> <span class="hljs-variable">$PATH</span><br>/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin<br></code></pre></td></tr></table></figure></li></ul><h3 id="5-PATH-PATH-HOME-bin"><a href="#5-PATH-PATH-HOME-bin" class="headerlink" title="5. PATH=$PATH:$HOME/bin"></a><strong>5. <code>PATH=$PATH:$HOME/bin</code></strong></h3><ul><li><p><strong>作用</strong>：将 <code>$HOME/bin</code> 添加到 <code>PATH</code> 环境变量</p></li><li><p><strong>解释</strong>：</p><ul><li><code>$PATH:$HOME/bin</code>：在原 <code>PATH</code> 后追加 <code>:$HOME/bin</code></li><li>这样 shell 也会在 <code>~/bin</code> 目录下查找可执行文件</li></ul></li><li><p><strong>示例</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$HOME</span>/bin<br>$ <span class="hljs-built_in">echo</span> <span class="hljs-variable">$PATH</span><br>/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/home/user/bin<br></code></pre></td></tr></table></figure></li></ul><h3 id="6-d-home-lls-c"><a href="#6-d-home-lls-c" class="headerlink" title="6. d=/home/lls/c"></a><strong>6. <code>d=/home/lls/c</code></strong></h3><ul><li><strong>作用</strong>：定义一个 **shell 变量 <code>d</code>**，存储路径 <code>/home/lls/c</code></li><li><strong>解释</strong>：<ul><li><code>d</code> 是自定义变量（不是环境变量，仅在当前 shell 有效）</li><li>可以用于后续命令（如 <code>cd $d</code>）</li></ul></li><li><strong>示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ d=/home/lls/c<br>$ <span class="hljs-built_in">echo</span> <span class="hljs-variable">$d</span><br>/home/lls/c<br></code></pre></td></tr></table></figure></li></ul><h3 id="7-cd-d"><a href="#7-cd-d" class="headerlink" title="7. cd $d"></a><strong>7. <code>cd $d</code></strong></h3><ul><li><strong>作用</strong>：切换到变量 <code>d</code> 指定的目录</li><li><strong>解释</strong>：<ul><li><code>$d</code> 会展开为 <code>/home/lls/c</code></li><li>相当于执行 <code>cd /home/lls/c</code></li></ul></li><li><strong>示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">cd</span> <span class="hljs-variable">$d</span><br>$ <span class="hljs-built_in">pwd</span><br>/home/lls/c<br></code></pre></td></tr></table></figure></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><table><thead><tr><th>命令</th><th>作用</th><th>示例</th></tr></thead><tbody><tr><td><code>PS1=...</code></td><td>设置终端提示符</td><td><code>PS1=&#39;\u:\w $&#39;</code></td></tr><tr><td><code>echo $HOME</code></td><td>显示家目录</td><td><code>/home/user</code></td></tr><tr><td><code>echo $PATH</code></td><td>显示可执行文件搜索路径</td><td><code>/usr/bin:/bin</code></td></tr><tr><td><code>PATH=$PATH:...</code></td><td>追加路径到 <code>PATH</code></td><td><code>PATH=$PATH:~/bin</code></td></tr><tr><td><code>var=value</code></td><td>定义 shell 变量</td><td><code>d=/home/lls/c</code></td></tr><tr><td><code>cd $var</code></td><td>切换到变量指定的目录</td><td><code>cd $d</code></td></tr></tbody></table><p>这些命令常用于 <strong>自定义 shell 环境</strong> 或 <strong>管理文件路径</strong>。</p><h1 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h1><h2 id="文件权限"><a href="#文件权限" class="headerlink" title="文件权限"></a>文件权限</h2><h3 id="命令-ls-l-a-out"><a href="#命令-ls-l-a-out" class="headerlink" title="命令 ls -l a.out"></a><strong>命令 <code>ls -l a.out</code></strong></h3><ul><li><strong>作用</strong>：以长格式（<code>-l</code> 选项）列出文件 <code>a.out</code> 的详细信息</li><li><strong>输出示例</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">-rwxr-xr-x 1 user1 <span class="hljs-built_in">users</span> 9319 2010-02-09 23:56 a.out<br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="输出字段解析"><a href="#输出字段解析" class="headerlink" title="输出字段解析"></a><strong>输出字段解析</strong></h3><table><thead><tr><th>字段</th><th>示例值</th><th>含义</th></tr></thead><tbody><tr><td><strong>文件类型和权限</strong></td><td><code>-rwxr-xr-x</code></td><td>文件类型 + 所有者&#x2F;组&#x2F;其他用户的权限</td></tr><tr><td><strong>硬链接数</strong></td><td><code>1</code></td><td>指向该文件的硬链接数量</td></tr><tr><td><strong>所有者</strong></td><td><code>user1</code></td><td>文件所有者用户名</td></tr><tr><td><strong>所属组</strong></td><td><code>users</code></td><td>文件所属用户组</td></tr><tr><td><strong>文件大小</strong></td><td><code>9319</code></td><td>文件大小（字节）</td></tr><tr><td><strong>最后修改时间</strong></td><td><code>2010-02-09 23:56</code></td><td>文件最后修改的日期和时间</td></tr><tr><td><strong>文件名</strong></td><td><code>a.out</code></td><td>文件名</td></tr></tbody></table><hr><h3 id="详细拆解"><a href="#详细拆解" class="headerlink" title="详细拆解"></a><strong>详细拆解</strong></h3><h4 id="1-文件类型和权限-rwxr-xr-x"><a href="#1-文件类型和权限-rwxr-xr-x" class="headerlink" title="1. 文件类型和权限 (-rwxr-xr-x)"></a>1. 文件类型和权限 (<code>-rwxr-xr-x</code>)</h4><ul><li><p>**第一个字符 <code>-</code>**：文件类型</p><ul><li><code>-</code>：普通文件</li><li><code>d</code>：目录</li><li><code>l</code>：符号链接</li><li>其他如 <code>c</code>（字符设备）、<code>b</code>（块设备）等</li></ul></li><li><p><strong>后续 9 个字符</strong>：&#x3D;&#x3D;权限分 3 组（所有者&#x2F;组&#x2F;其他用户&#x3D;&#x3D;）</p><ul><li><code>rwxr-xr-x</code> 分解：<ul><li><strong>所有者权限</strong> (<code>rwx</code>)：可读（<code>r</code>）、可写（<code>w</code>）、可执行（<code>x</code>）</li><li><strong>组权限</strong> (<code>r-x</code>)：可读（<code>r</code>）、不可写（<code>-</code>）、可执行（<code>x</code>）</li><li><strong>其他用户权限</strong> (<code>r-x</code>)：同组权限</li></ul></li></ul></li></ul><h4 id="2-硬链接数-1"><a href="#2-硬链接数-1" class="headerlink" title="2. 硬链接数 (1)"></a>2. 硬链接数 (<code>1</code>)</h4><ul><li>表示该文件有 <strong>1 个硬链接</strong>（即文件名 <code>a.out</code> 是唯一指向该文件 inode 的硬链接）。</li></ul><h4 id="3-所有者和组-user1-users"><a href="#3-所有者和组-user1-users" class="headerlink" title="3. 所有者和组 (user1 users)"></a>3. 所有者和组 (<code>user1 users</code>)</h4><ul><li><strong>所有者</strong>：用户 <code>user1</code> 对该文件有完全控制权（<code>rwx</code>）。</li><li><strong>所属组</strong>：组 <code>users</code> 的成员对该文件有读和执行权限（<code>r-x</code>）。</li></ul><h4 id="4-文件大小-9319"><a href="#4-文件大小-9319" class="headerlink" title="4. 文件大小 (9319)"></a>4. 文件大小 (<code>9319</code>)</h4><ul><li>文件占 <strong>9319 字节</strong>（约 9.1 KB）。</li></ul><h4 id="5-修改时间-2010-02-09-23-56"><a href="#5-修改时间-2010-02-09-23-56" class="headerlink" title="5. 修改时间 (2010-02-09 23:56)"></a>5. 修改时间 (<code>2010-02-09 23:56</code>)</h4><ul><li>文件最后修改时间为 <strong>2010 年 2 月 9 日 23:56</strong>。</li></ul><h4 id="6-文件名-a-out"><a href="#6-文件名-a-out" class="headerlink" title="6. 文件名 (a.out)"></a>6. 文件名 (<code>a.out</code>)</h4><ul><li>这是一个典型的 <strong>可执行文件</strong>（由 <code>gcc</code> 编译默认生成）。</li></ul><hr><h3 id="权限的字母与数字表示"><a href="#权限的字母与数字表示" class="headerlink" title="权限的字母与数字表示"></a><strong>权限的字母与数字表示</strong></h3><table><thead><tr><th>权限</th><th>字母表示</th><th>数字表示（八进制）</th></tr></thead><tbody><tr><td>可读</td><td><code>r</code></td><td><code>4</code></td></tr><tr><td>可写</td><td><code>w</code></td><td><code>2</code></td></tr><tr><td>可执行</td><td><code>x</code></td><td><code>1</code></td></tr><tr><td>无权限</td><td><code>-</code></td><td><code>0</code></td></tr></tbody></table><ul><li>示例 <code>rwxr-xr-x</code> 的数字表示：<ul><li>所有者：<code>rwx</code> &#x3D; <code>4+2+1</code> &#x3D; <code>7</code></li><li>组：<code>r-x</code> &#x3D; <code>4+0+1</code> &#x3D; <code>5</code></li><li>其他用户：<code>r-x</code> &#x3D; <code>5</code></li><li>合并为 **<code>755</code>**（常用权限）</li></ul></li></ul><hr><h3 id="扩展命令"><a href="#扩展命令" class="headerlink" title="扩展命令"></a><strong>扩展命令</strong></h3><p>如果想修改权限或所有者：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">chmod</span> 755 a.out <span class="hljs-comment"># 修改权限为 755</span><br><br><span class="hljs-built_in">chmod</span> –w a.out <span class="hljs-comment"># 移除文件 a.out 的所有写权限（w），即禁止任何用户修改该文件</span><br><br><span class="hljs-built_in">chmod</span> +x ldir <span class="hljs-comment"># 为文件或目录 ldir 添加 所有用户的执行权限（x）</span><br><br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">chown</span> newuser a.out <span class="hljs-comment"># 修改所有者为 newuser</span><br><br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">chgrp</span> newgroup a.out <span class="hljs-comment"># 修改所属组为 newgroup</span><br></code></pre></td></tr></table></figure><h1 id="设备文件"><a href="#设备文件" class="headerlink" title="设备文件"></a>设备文件</h1><blockquote><p>Everything is a File</p></blockquote><p>UNIX&#x2F;Linux 系统的一个核心设计思想是 <strong>“一切皆文件”</strong>（Everything is a File），包括硬件设备（如键盘、鼠标、磁盘、终端等）也被抽象为文件，存放在 <code>/dev</code> 目录下。这使得用户和程序可以通过标准的文件操作（如 <code>read</code>、<code>write</code>、<code>open</code>、<code>close</code>）来与设备交互，而无需关心底层硬件细节。</p><hr><h2 id="1-设备文件的分类"><a href="#1-设备文件的分类" class="headerlink" title="1. 设备文件的分类"></a><strong>1. 设备文件的分类</strong></h2><p>在 UNIX&#x2F;Linux 中，设备文件主要分为两类：</p><h3 id="1-字符设备（Character-Devices）"><a href="#1-字符设备（Character-Devices）" class="headerlink" title="(1) 字符设备（Character Devices）"></a><strong>(1) 字符设备（Character Devices）</strong></h3><ul><li><p><strong>特点</strong>：以<strong>字符流</strong>方式逐字节传输数据，适用于<strong>非块存储设备</strong>（如键盘、鼠标、串口、终端等）。</p></li><li><p><strong>示例</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">ls</span> -l /dev/ttyS0  <span class="hljs-comment"># 串口设备</span><br>crw-rw---- 1 root dialout 4, 64 Oct 10 10:00 /dev/ttyS0<br></code></pre></td></tr></table></figure><ul><li><code>c</code> 表示字符设备，<code>4</code> 是主设备号，<code>64</code> 是次设备号。</li></ul></li></ul><h3 id="2-块设备（Block-Devices）"><a href="#2-块设备（Block-Devices）" class="headerlink" title="(2) 块设备（Block Devices）"></a><strong>(2) 块设备（Block Devices）</strong></h3><ul><li><p><strong>特点</strong>：以<strong>固定大小的数据块</strong>（如 512B、4KB）传输，适用于<strong>存储设备</strong>（如硬盘、SSD、U盘等）。</p></li><li><p><strong>示例</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">ls</span> -l /dev/sda  <span class="hljs-comment"># 硬盘设备</span><br>brw-rw---- 1 root disk 8, 0 Oct 10 10:00 /dev/sda<br></code></pre></td></tr></table></figure><ul><li><code>b</code> 表示块设备，<code>8</code> 是主设备号，<code>0</code> 是次设备号。</li></ul></li></ul><hr><h2 id="2-设备号（Major-Minor-Numbers）"><a href="#2-设备号（Major-Minor-Numbers）" class="headerlink" title="2. 设备号（Major &amp; Minor Numbers）"></a><strong>2. 设备号（Major &amp; Minor Numbers）</strong></h2><p>每个设备文件都有两个关键数字：</p><ul><li><strong>主设备号（Major Number）</strong>：标识<strong>设备类型</strong>（如 <code>4</code> 表示终端设备，<code>8</code> 表示 SCSI 磁盘）。</li><li><strong>次设备号（Minor Number）</strong>：标识<strong>具体设备实例</strong>（如 <code>/dev/sda1</code> 和 <code>/dev/sda2</code> 是同一硬盘的不同分区）。</li></ul><p>查看设备号：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">ls</span> -l /dev/sda<br>brw-rw---- 1 root disk 8, 0 Oct 10 10:00 /dev/sda  <span class="hljs-comment"># 主设备号=8，次设备号=0</span><br></code></pre></td></tr></table></figure><hr><h2 id="3-设备文件的创建与管理"><a href="#3-设备文件的创建与管理" class="headerlink" title="3. 设备文件的创建与管理"></a><strong>3. 设备文件的创建与管理</strong></h2><h3 id="1-手动创建设备文件（mknod）"><a href="#1-手动创建设备文件（mknod）" class="headerlink" title="(1) 手动创建设备文件（mknod）"></a><strong>(1) 手动创建设备文件（<code>mknod</code>）</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">sudo</span> <span class="hljs-built_in">mknod</span> /dev/mydevice c 123 0  <span class="hljs-comment"># 创建字符设备，主设备号=123，次设备号=0</span><br>$ <span class="hljs-built_in">sudo</span> <span class="hljs-built_in">chmod</span> 666 /dev/mydevice      <span class="hljs-comment"># 设置权限，允许读写</span><br></code></pre></td></tr></table></figure><h3 id="2-动态设备管理（udev）"><a href="#2-动态设备管理（udev）" class="headerlink" title="(2) 动态设备管理（udev）"></a><strong>(2) 动态设备管理（<code>udev</code>）</strong></h3><p>现代 Linux 使用 <code>udev</code> 动态管理 <code>/dev</code> 目录，设备插拔时自动创建&#x2F;删除设备文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ udevadm monitor  <span class="hljs-comment"># 监控设备事件</span><br></code></pre></td></tr></table></figure><hr><h2 id="4-设备文件的使用示例"><a href="#4-设备文件的使用示例" class="headerlink" title="4. 设备文件的使用示例"></a><strong>4. 设备文件的使用示例</strong></h2><h3 id="1-终端设备（-dev-pts-）"><a href="#1-终端设备（-dev-pts-）" class="headerlink" title="(1) 终端设备（/dev/pts/*）"></a><strong>(1) 终端设备（<code>/dev/pts/*</code>）</strong></h3><ul><li><p><strong><code>/dev/pts/0</code></strong> 是当前终端（伪终端），可以直接写入数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Hello, terminal!&quot;</span> &gt; /dev/pts/0  <span class="hljs-comment"># 在当前终端显示消息</span><br></code></pre></td></tr></table></figure></li><li><p><strong><code>/dev/tty</code></strong> 是当前进程的控制终端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Hello, tty!&quot;</span> &gt; /dev/tty  <span class="hljs-comment"># 直接输出到当前终端</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="2-空设备（-dev-null）"><a href="#2-空设备（-dev-null）" class="headerlink" title="(2) 空设备（/dev/null）"></a><strong>(2) 空设备（<code>/dev/null</code>）</strong></h3><p>丢弃所有写入的数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">cat</span> /etc/passwd &gt; /dev/null  <span class="hljs-comment"># 数据被丢弃，不显示</span><br></code></pre></td></tr></table></figure><h3 id="3-随机数设备（-dev-random-dev-urandom）"><a href="#3-随机数设备（-dev-random-dev-urandom）" class="headerlink" title="(3) 随机数设备（/dev/random &amp; /dev/urandom）"></a><strong>(3) 随机数设备（<code>/dev/random</code> &amp; <code>/dev/urandom</code>）</strong></h3><ul><li>**<code>/dev/random</code>**：阻塞式随机数生成（熵耗尽时等待）。</li><li>**<code>/dev/urandom</code>**：非阻塞式随机数生成（速度更快）。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">head</span> -c 16 /dev/urandom | <span class="hljs-built_in">base64</span>  <span class="hljs-comment"># 生成 16 字节随机 Base64 字符串</span><br></code></pre></td></tr></table></figure><h3 id="4-磁盘设备（-dev-sd-）"><a href="#4-磁盘设备（-dev-sd-）" class="headerlink" title="(4) 磁盘设备（/dev/sd*）"></a><strong>(4) 磁盘设备（<code>/dev/sd*</code>）</strong></h3><p>直接读写磁盘（需 root 权限）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">sudo</span> <span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/sda of=disk_backup.img bs=4M  <span class="hljs-comment"># 备份整个磁盘</span><br></code></pre></td></tr></table></figure><hr><h2 id="5-设备文件与普通文件的区别"><a href="#5-设备文件与普通文件的区别" class="headerlink" title="5. 设备文件与普通文件的区别"></a><strong>5. 设备文件与普通文件的区别</strong></h2><table><thead><tr><th><strong>特性</strong></th><th><strong>普通文件</strong></th><th><strong>设备文件</strong></th></tr></thead><tbody><tr><td><strong>存储方式</strong></td><td>存储在磁盘上</td><td>无实际存储，直接映射到设备驱动</td></tr><tr><td><strong>读写操作</strong></td><td>由文件系统管理</td><td>由内核设备驱动处理</td></tr><tr><td><strong>文件类型</strong></td><td><code>-</code>（普通文件）</td><td><code>c</code>（字符设备）或 <code>b</code>（块设备）</td></tr><tr><td><strong>示例</strong></td><td><code>/etc/passwd</code></td><td><code>/dev/sda</code>, <code>/dev/tty1</code></td></tr></tbody></table><hr><h2 id="6-高级应用"><a href="#6-高级应用" class="headerlink" title="6. 高级应用"></a><strong>6. 高级应用</strong></h2><h3 id="1-重定向到设备"><a href="#1-重定向到设备" class="headerlink" title="(1) 重定向到设备"></a><strong>(1) 重定向到设备</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">cat</span> myfile.txt &gt; /dev/pts/1  <span class="hljs-comment"># 将文件内容发送到另一个终端</span><br></code></pre></td></tr></table></figure><h3 id="2-设备文件与管道结合"><a href="#2-设备文件与管道结合" class="headerlink" title="(2) 设备文件与管道结合"></a><strong>(2) 设备文件与管道结合</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ tar -czf - /home | ssh user@host <span class="hljs-string">&quot;cat &gt; /dev/sdc1/home_backup.tar.gz&quot;</span><br><span class="hljs-comment"># 压缩 /home 并直接写入远程硬盘</span><br></code></pre></td></tr></table></figure><h3 id="3-调试设备"><a href="#3-调试设备" class="headerlink" title="(3) 调试设备"></a><strong>(3) 调试设备</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">sudo</span> strace -e open,<span class="hljs-built_in">read</span>,write <span class="hljs-built_in">cat</span> /dev/input/mouse0<br><span class="hljs-comment"># 监控鼠标设备的读写操作</span><br></code></pre></td></tr></table></figure><hr><h2 id="7-安全注意事项"><a href="#7-安全注意事项" class="headerlink" title="7. 安全注意事项"></a><strong>7. 安全注意事项</strong></h2><ul><li>**<code>/dev/kmem</code>、<code>/dev/mem</code>**：直接访问内核内存，危险！</li><li>**<code>/dev/sd*</code>**：误操作可能导致数据丢失（如 <code>dd if=/dev/zero of=/dev/sda</code> 会清空硬盘）。</li><li><strong>权限管理</strong>：默认设备文件权限较严格（如 <code>/dev/sda</code> 仅 root 可写），避免随意修改。</li></ul><hr><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>UNIX 的设备文件机制使得硬件管理变得极其灵活，用户可以通过文件操作直接控制设备。理解 <code>/dev</code> 目录的结构和设备号（Major&#x2F;Minor）有助于进行系统调试、驱动开发和自动化管理。</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>UNIX</tag>
      
      <tag>复习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BUAA-OO-Unit3</title>
    <link href="/2025/05/14/BUAA-OO-Unit3/"/>
    <url>/2025/05/14/BUAA-OO-Unit3/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是2025年北航OO第三单元的总结博客，希望能对你有所帮助😊</p>          </div><h1 id="各种测试"><a href="#各种测试" class="headerlink" title="各种测试"></a>各种测试</h1><ul><li>作业要求，这里就不说了😆</li></ul><h1 id="测试数据构造"><a href="#测试数据构造" class="headerlink" title="测试数据构造"></a>测试数据构造</h1><blockquote><p><strong>未测代码永远是错的</strong>  ——jyy</p></blockquote><ul><li><p>&#x3D;&#x3D;以hw10数据生成器为例&#x3D;&#x3D;</p></li><li><p>在我的测试数据生成器<code>generator.py</code>中，设计了几种测试场景，分别测试<strong>不同的输入模式与程序功能</strong>，主要测试几个需要<strong>动态维护</strong>的属性相关的指令</p></li><li><p>由于通过了中测，所以大部分的基本指令单独的功能是没有问题的，因此我设计了几种设计模式来重点测试<code>qba</code>、<code>qtvs</code>以及<strong>公众号、文章相关的指令</strong></p></li><li><p>采用随机策略+指定概率来生成</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">mode_probabilities = &#123;<br>    <span class="hljs-string">&#x27;normal_data&#x27;</span>: <span class="hljs-number">0.05</span>,<br>        <span class="hljs-string">&#x27;stress_test_data&#x27;</span>: <span class="hljs-number">0.1</span>,                                        <br>        <span class="hljs-string">&#x27;special_qtvs&#x27;</span>: <span class="hljs-number">0.3</span>,                        <br>        <span class="hljs-string">&#x27;structured_social_media_test&#x27;</span>: <span class="hljs-number">0.25</span>,      <br>        <span class="hljs-string">&#x27;special_qba_stress_test&#x27;</span>: <span class="hljs-number">0.3</span>         <br>    &#125;<br></code></pre></td></tr></table></figure><ul><li>总共5种模式<ul><li><code>normal_data</code>：常规测试数据，添加用户、标签、公众号后，剩余的指令随机生成</li><li><code>stress_test_data</code>：在常规测试的基础上，增大数据量，同时会先随机设置关系值，然后随机生成指令</li><li><code>special_qtvs</code>：专门针对<code>qtvs</code>的生成模式。首先创建用户，接着建立所有其他人与1号人物的关系，并随机添加一些关系，然后全部加入1号人物的标签中，然后分2阶段测试，第一阶段在网络中随机修改关系，添加关系，查询qtvs；第二阶段大致与第一阶段类似，但添加了指令<code>dft</code>，以及增加了<code>qtvs</code> 的概率。</li><li><code>special_qba_stress_test</code>：专门针对<code>qba</code>的测试。首先建立一个总人数20%的核心用户群，然后在核心用户中每个用户随机添加2-3个关系，然后开始第二阶段，混合测试：40%概率执行qba查询，并且80%概率查询核心用户，20%概率查询其他用户；30%概率新增或修改核心用户与其他用户之间的关系；30%概率新增用户和关系值，其中一半概率新增用户并添加关系，一半概率在所有用户中随机添加关系</li><li><code>structured_social_media_test</code>：第一阶段：添加用户和关系；第二阶段：添加公众号，并添加少量关注和文章；第三阶段：海量的数据来随机进行以下几种操作：贡献文章、删除文章、添加关注、删除公众号、查询接受文章、查询最佳贡献者</li></ul></li><li>😁现在回头看，还是有很多可以完善的地方 qwq</li></ul><h1 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h1><ul><li>jml的架构已经很好了，<del>跟着无脑实现就可以了</del></li></ul><h1 id="维护策略"><a href="#维护策略" class="headerlink" title="维护策略"></a>维护策略</h1><h2 id="query-shortest-path"><a href="#query-shortest-path" class="headerlink" title="query_shortest_path"></a>query_shortest_path</h2><ul><li>这个不太好维护，我是直接计算的，时间复杂度也不会超</li><li>可以采用的方法比较多：Dijkstra、BFS、双向BFS，可以选择自己喜欢的</li></ul><h2 id="query-best-contributor"><a href="#query-best-contributor" class="headerlink" title="query_best_contributor"></a>query_best_contributor</h2><ul><li>直接算，一层循环就可以</li></ul><h2 id="query-triple-sum"><a href="#query-triple-sum" class="headerlink" title="query_triple_sum"></a>query_triple_sum</h2><ul><li>这个是可以动态维护的，只需要考虑下面这几点</li><li>写一个<strong>计算共同好友数量</strong>的方法<code>private int countCommonNeighbors(Person p1, Person p2)</code></li><li>每次添加关系时，<code>triple_sum</code>加上<code>countCommonNeighbors</code>返回值</li><li>每次删除关系时，减去方法返回值即可</li></ul><h2 id="query-best-acquaintance"><a href="#query-best-acquaintance" class="headerlink" title="query_best_acquaintance"></a>query_best_acquaintance</h2><ul><li>动态维护的，而且后面的<code>query_couple_sum</code>依赖这个方法，所以这个方法最好要优一点，不然后面的方法可能要寄</li><li>在Person中添加<code>int bestAcquaintance, bestValue</code>和<code>bestDirty</code>这三个属性</li><li>朴素遍历实现一个<code>public void reSetBestAcquaintance()</code>方法</li><li>添加关系时直接与目前的最值比较，然后维护</li><li>删除关系时，如果删除的是<code>bestAcquaintance</code>，那么置位<code>bestDirty</code></li><li>修改关系时，有这么一些情况<ul><li>如果修改的就是<code>bestAcquaintance</code>，如果是把value改大了，那么直接维护<code>bestValue</code>即可；如果是value改小了，那么置位<code>bestDirty</code></li><li>如果修改的不是最好的，那么只需要在<code>newValue</code>大于现在的<code>bestValue</code>或者二者相等但id更小的时候维护<code>bestAcquaintance 和 bestValue</code></li></ul></li><li>查询时，如果<code>bestDirty</code>置位了，那么调用<code>reSetBestAcquaintance()</code>方法更新，否则直接返回即可</li></ul><h2 id="query-couple-sum"><a href="#query-couple-sum" class="headerlink" title="query_couple_sum"></a>query_couple_sum</h2><ul><li>没想好怎么维护，比较麻烦。直接计算，一层循环就可以解决</li><li>遍历每个人p1,然后得到p1的最好朋友p2，如果p2的最好朋友也是p1，那么<code>num++</code></li><li>由于每一对算了2遍，所以最后<code>return num / 2;</code></li></ul><h2 id="query-tag-age-var"><a href="#query-tag-age-var" class="headerlink" title="query_tag_age_var"></a>query_tag_age_var</h2><ul><li>参考的是<code>hyggge</code>的博客，<a href="https://hyggge.github.io/2022/06/04/oo/oo-di-san-dan-yuan-zong-jie/">链接在这里</a></li><li>有一个小坑点，学长已经说的很详细了</li></ul><h2 id="query-tag-value-sum"><a href="#query-tag-value-sum" class="headerlink" title="query_tag_value_sum"></a>query_tag_value_sum</h2><ul><li>动态维护试了很久，最后放弃了 qwq，用了一个阉割版的动态维护</li><li><code>Tag</code>里的<code>addPerson</code>和<code>delPerson</code>方法中，遍历一遍person，如果与这个要添加、删除的人有关联，就将<code>valueSum</code> 加上或减去二者关系值的2倍，<strong>一定注意是 2 倍</strong>，看看jml应该就能知道</li><li><code>Network</code>中添加关系、修改关系时，遍历全局所有tag,如果同时包含这两个人，就将tag的<code>dirty</code>置位</li><li>查询时，如果<code>dirty</code>，就重新算一遍，否则直接返回</li><li>重新算的方法也有讲究，肯定不能直接用<code>jml</code>的双层全遍历，我们可以发现，在我们的关系网中，两个人之间的value一定是相等的，那么在遍历时，第二层循环就可以直接从<code>i + 1</code>开始，不算重复的，最后返回 sum * 2即可</li></ul><h2 id="query-received-articles"><a href="#query-received-articles" class="headerlink" title="query_received_articles"></a>query_received_articles</h2><ul><li>主要是维护这个<code>received_articles</code>这个数据结构，用链表实现的话，头部插入没问题，但是删除的话，需要遍历链表；用hashmap的话并没有办法实现顺序、头部插入这些操作，于是我便自行设计了一个类，<code>LinkedMap</code>，在类中用hashmap存下<index-object>索引对，然后主体使用链表，这样在删除时，直接根据索引得到目标Object,再设置它的<code>prev</code> <code>next</code>指针即可，这样便实现了<strong>头部插入和删除均为O(1)的复杂度</strong></li><li>但是在hw11中，比较变态的是articles可以重复，也就是一个人可以有多篇id一致的<code>received_articles</code>，这样我们要修改我们的自制类，将HashMap的value替换为一个list即可，如果不太明白可以去看看我的源代码，<code>MultiLinkedMap.java</code>这个类</li></ul><h1 id="大模型"><a href="#大模型" class="headerlink" title="大模型"></a>大模型</h1><ul><li><p>使用大模型，关键在于<code>prompt</code>，一个好的<code>prompt</code>往往能达到意想不到的效果，同时要提供较为<strong>详细充足的上下文环境</strong>，<code>exp_6</code>实验提供了一个不错的借鉴</p></li><li><p>在使用的时候，让大模型根据<code>jml</code>先实现个大概，避免一些重复机械的动作，比如：异常识别与抛出，简单方法的实现等</p></li><li><p>然后可以新开一个话题或者换一个模型，一次将一个或者几个方法的jml描述投喂进去，然后让它一行一行分析解释这些jml，然后你就可以对照着它的分析，检查代码的正确性了</p></li><li><p>最后还是要自己走查几遍，多做测试</p></li><li><p>当然，模型的选择也是重要的一环，我自己体验过的：<code>Gemini 2.5 Pro</code> <code>Grok</code> <code>deepseek r1 / v3</code>都很优秀，<code>claude 3.7</code>更是无需多言，只不过用不起😭</p></li></ul><h1 id="Junit-测试"><a href="#Junit-测试" class="headerlink" title="Junit 测试"></a>Junit 测试</h1><ul><li>需要我们构造数据足够全面，覆盖各种测试场景</li><li>同时，在检验正确性时候，需要检查<code>ensures</code> <code>assignable</code> <code>pure</code> <code>safe</code> <code>result</code>等种种条件是否被正确实现，需要比较细致全面的检查，也给同学们带来较大的麻烦</li><li>为了测试不能更改的属性前后是否一致，我选择在生成测试数据时，就生成两份一模一样的，一份调用方法，另一份作为对拍</li><li>（超小声）可以借鉴学长的代码，在此基础上修改以适应自己的架构，应该能省一些事儿🫠</li></ul><h1 id="学习体会"><a href="#学习体会" class="headerlink" title="学习体会"></a>学习体会</h1><ul><li>jml确实提供了一个优良的编码指导和架构设计，使得这单元的作业能省不少事儿，一五一十地按照jml的实现应该是不会出错的，只不过可能时间复杂度上不太友好</li><li>不过一些复杂的项目，使用jml描述可能会把本就复杂的问题更加复杂化，带来理解上的困难，虽然现在已经有一些方法的jml描述我不想看，直接让AI分析了，所以，Jml也有其不足之处</li><li>善用AI，辅助自己<strong>高效地学习与完成作业</strong>，比如：可以将看不懂或者很复杂的jml描述让AI分析解释；可以自己的代码给AI让其提出可以优化时间复杂度的方法；可以用AI学习一些知识：并查集、双向BFS等，用好之后，能大大提升自己的效率。</li><li>但也不能完全依赖AI，缺乏自己的思考与设计，直接不加思考地拷贝粘贴，那样只会<strong>适得其反，学无所获</strong></li></ul>]]></content>
    
    
    <categories>
      
      <category>OO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>OO</tag>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BUAA-OO-Unit2</title>
    <link href="/2025/04/18/BUAA-OO-Unit2/"/>
    <url>/2025/04/18/BUAA-OO-Unit2/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是2025年北航OO第二单元的总结博客，希望能对你有所帮助😊</p>          </div><ul><li>魔鬼般的OO-Unit2终于结束啦😭😭😭😁😁😁</li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/%E9%BE%99%E7%8C%AB3.jpg" alt="龙猫3"></p><hr><ul><li>进入正题</li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/image-20250418125328192.png"></p><ul><li>这是三次迭代后<code>hw7</code>的代码行数统计，源代码行数762行，应该算少的吧</li><li>这也反映出本人<strong>力求简洁的迭代追求</strong>，<del>能少写的就少写</del>，几行代码就能解决的问题绝不去卷高深复杂的方法</li><li>下面是本单元的<strong>总结与分析</strong></li></ul><h1 id="同步块与锁的选择"><a href="#同步块与锁的选择" class="headerlink" title="同步块与锁的选择"></a>同步块与锁的选择</h1><p>我们为什么需要同步块？因为多个线程同时运行时可能会对同一共享资源进行访问和修改，这样就会造成<strong>竞争与同步问题</strong>，因此我们需要锁和同步块来解决这一问题。</p><p>在我的代码中，hw5最开始我有一个<code>MainRequests</code>主类和<code>SubRequests</code>子类，后来我发现这两个类里面的属性和方法竟然一模一样(●’◡’●)，后面索性就只有<code>Requests</code>这一个类了</p><ul><li>因为<code>Requests</code>类被<code>inputThread</code>和<code>DispatchThread</code>所共享，因此这两个类中对<code>Requests</code>类调用的方法都需要加锁，设置同步块，具体如下：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">sortByPriority</span><span class="hljs-params">()</span> &#123;<br>       requests.sort(Comparator.comparingInt(r -&gt; ((PersonRequest) r).getPriority()).reversed());<br>   &#125;<br><br>   <span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">push</span><span class="hljs-params">(Request request)</span> &#123;<br>       <span class="hljs-comment">//TimableOutput.println(&quot;from-&quot; + elevatorId + &quot;-request need</span><br>       <span class="hljs-comment">// to be dispatched-&quot; + isDone() + &quot;-&quot; + isEmpty());</span><br>       requests.add(request);<br>       notifyAll();<br>   &#125;<br><br>   <span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> Request <span class="hljs-title function_">pop</span><span class="hljs-params">()</span> &#123;<br>       <span class="hljs-keyword">if</span> (!isDone() &amp;&amp; isEmpty()) &#123;<br>           <span class="hljs-keyword">try</span> &#123;<br>               wait();<br>           &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>               <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e);<br>           &#125;<br>       &#125;<br>       <span class="hljs-keyword">if</span> (isEmpty()) &#123;<br>           <span class="hljs-keyword">return</span> <span class="hljs-literal">null</span>;<br>       &#125;<br>       notifyAll();<br>       <span class="hljs-keyword">return</span> requests.remove(<span class="hljs-number">0</span>);<br>   &#125;<br><br>   <span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setDone</span><span class="hljs-params">()</span> &#123;<br>       done = <span class="hljs-literal">true</span>;<br>       notifyAll();<br>   &#125;<br><br>   <span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">isDone</span><span class="hljs-params">()</span> &#123;<br>       <span class="hljs-keyword">return</span> done &amp; (arrivedRequests == needRequests.size());<br>       <span class="hljs-comment">//return done;</span><br>   &#125;<br><br>   <span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">isEmpty</span><span class="hljs-params">()</span> &#123;<br>       <span class="hljs-keyword">return</span> requests.isEmpty();<br>   &#125;<br></code></pre></td></tr></table></figure><ul><li>此外还有电梯线程中，在临时调度到达目的地需要将人请下去时，需要<strong>设置同步块，保证这是一个原子操作</strong>，在这个过程不允许分派线程再将请求添加给电梯的请求队列中</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">scheOutPerson</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">synchronized</span> (subRequests) &#123;<br>            Iterator&lt;Request&gt; iterator = subRequests.getRequests().iterator();<br>            <span class="hljs-keyword">while</span> (iterator.hasNext()) &#123;<br>                <span class="hljs-type">PersonRequest</span> <span class="hljs-variable">preq</span> <span class="hljs-operator">=</span> (PersonRequest)iterator.next();<br>                <span class="hljs-type">String</span> <span class="hljs-variable">strFloor</span> <span class="hljs-operator">=</span> Strategy.toStr(floor);<br>                <span class="hljs-type">int</span> <span class="hljs-variable">toFloor</span> <span class="hljs-operator">=</span> Strategy.toInt(preq.getToFloor());<br>                <span class="hljs-type">int</span> <span class="hljs-variable">id</span> <span class="hljs-operator">=</span> preq.getPersonId();<br>                <span class="hljs-keyword">if</span> (peopleInEle.contains(id)) &#123;<br>                    people--;<br>                    peopleInEle.remove(Integer.valueOf(id));<br>                    <span class="hljs-keyword">if</span> (toFloor == floor) &#123;<br>                        iterator.remove();<br>                        mainRequests.addArrive();<br>                        TimableOutput.println(<span class="hljs-string">&quot;OUT-S-&quot;</span> + id + <span class="hljs-string">&quot;-&quot;</span> + strFloor + <span class="hljs-string">&quot;-&quot;</span> + elevatorId);<br>                    &#125; <span class="hljs-keyword">else</span> &#123;<br>                        <span class="hljs-type">ReArrangeRequest</span> <span class="hljs-variable">raq</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ReArrangeRequest</span>(floor, preq);<br>                        TimableOutput.println(<span class="hljs-string">&quot;OUT-F-&quot;</span> + id + <span class="hljs-string">&quot;-&quot;</span> + strFloor + <span class="hljs-string">&quot;-&quot;</span> + elevatorId);<br>                        mainRequests.push(raq);<br>                        iterator.remove();<br>                    &#125;<br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<br></code></pre></td></tr></table></figure><h1 id="调度分配策略"><a href="#调度分配策略" class="headerlink" title="调度分配策略"></a>调度分配策略</h1><h2 id="hw5"><a href="#hw5" class="headerlink" title="hw5"></a>hw5</h2><ul><li>hw5的请求是指定了电梯的，因此直接分配给对应编号的电梯即可，无需过多设计</li></ul><h2 id="hw6"><a href="#hw6" class="headerlink" title="hw6"></a>hw6</h2><ul><li><p>最开始我的设计是将所有的主请求按照 1 2 3 ……6 1 2 ……的循环&#x3D;&#x3D;顺序分配&#x3D;&#x3D;给6部电梯，这样的设计比较简单，而且在大量的请求加持下，性能也不会很差劲，写法大致如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">personDispatch</span><span class="hljs-params">(PersonRequest request)</span> &#123;<br>        <span class="hljs-type">int</span> <span class="hljs-variable">elevatorId</span> <span class="hljs-operator">=</span> inOrder();<br>        subRequestMap.get(elevatorId).push(request);<br>        TimableOutput.println(<span class="hljs-string">&quot;RECEIVE-&quot;</span> + request.getPersonId() + <span class="hljs-string">&quot;-&quot;</span> + elevatorId);<br>    &#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">inOrder</span><span class="hljs-params">()</span> &#123;<br>        counter++;<br>        counter %= <span class="hljs-number">6</span>;<br>        <span class="hljs-keyword">return</span> counter + <span class="hljs-number">1</span>;<br>    &#125;<br></code></pre></td></tr></table></figure></li><li><p>但随后发现hw6新增了临时调度，<strong>临时调度期间的电梯无法进行分配</strong>，因此在分配调度时必须要考虑电梯是否在临时调度，这也<strong>会带来一系列问题</strong>：</p><ul><li>遇到了临时调度的电梯怎么办，是等待它结束还是重新分派一个新的电梯呢？</li><li>如果所有的电梯都临时调度了，怎么办？</li></ul><p>我的策略是遇到了临时调度的电梯，就遍历电梯，找一个<code>Free</code>的电梯分配给它，如果没有电梯<code>Free</code>，则随机决定一个电梯并等它结束临时调度。</p><p>这里有一个<strong>很经典也很有趣</strong>的问题，如果5部电梯都临时调度，并且在同一时间来了大量请求，那么是否要将这些请求全部分配给那一个空闲的电梯呢？这就是&#x3D;&#x3D;经典的围师必阙问题&#x3D;&#x3D;，也是中测mid5的测试意图，如果盲目地全部塞给一个电梯，那么其他电梯结束临时调度后无事可干，但这个电梯后面很有可能吃不消，最终导致<code>TLE</code>，因此我们还需要加一些逻辑，比如<strong>设定一个最高分配上限</strong>，我设计的是20，当然你也完全可以设置为其他你认为合适的值。把之前的判断加一个条件即可</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">if</span> (subRequestMap.get(i).getFree() &amp;&amp; subRequestMap.get(i).getSize() &lt; <span class="hljs-number">20</span>)<br></code></pre></td></tr></table></figure><p>我的调度器的核心代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">personDispatch</span><span class="hljs-params">(PersonRequest request)</span> &#123;<br>        counter++;<br>        counter %= <span class="hljs-number">6</span>;<br>        <span class="hljs-type">Requests</span> <span class="hljs-variable">requests</span> <span class="hljs-operator">=</span> subRequestMap.get(counter + <span class="hljs-number">1</span>);<br>        <span class="hljs-keyword">synchronized</span> (requests) &#123;<br>            <span class="hljs-keyword">if</span> (!requests.getFree()) &#123;<br>                <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">1</span>;i &lt;= <span class="hljs-number">6</span>;i++) &#123;<br>                    <span class="hljs-keyword">synchronized</span> (subRequestMap.get(i)) &#123;<br>                        <span class="hljs-keyword">if</span> (subRequestMap.get(i).getFree() &amp;&amp; subRequestMap.get(i).getSize() &lt; <span class="hljs-number">20</span>) &#123;<br>                            TimableOutput.println(<span class="hljs-string">&quot;RECEIVE-&quot;</span> + request.getPersonId() + <span class="hljs-string">&quot;-&quot;</span> + i);<br>                            subRequestMap.get(i).push(request);<br>                            <span class="hljs-keyword">return</span>; &#125;<br>                    &#125;<br>                &#125;<br>            &#125; <span class="hljs-keyword">else</span> &#123;<br>                TimableOutput.println(<span class="hljs-string">&quot;RECEIVE-&quot;</span> + request.getPersonId() + <span class="hljs-string">&quot;-&quot;</span> + (counter + <span class="hljs-number">1</span>));<br>                subRequestMap.get(counter + <span class="hljs-number">1</span>).push(request);<br>                <span class="hljs-keyword">return</span>;<br>            &#125;<br>        &#125;<br>    <span class="hljs-comment">//all elevators are not free</span><br>        <span class="hljs-type">int</span> <span class="hljs-variable">random</span> <span class="hljs-operator">=</span> random();<br>        requests = subRequestMap.get(random);<br>        <span class="hljs-keyword">synchronized</span> (requests) &#123;<br>            <span class="hljs-keyword">while</span> (!requests.getFree()) &#123;<br>                <span class="hljs-keyword">try</span> &#123;<br>                    requests.wait();<br>                &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>                    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e); &#125;<br>            &#125;<br>            TimableOutput.println(<span class="hljs-string">&quot;RECEIVE-&quot;</span> + request.getPersonId() + <span class="hljs-string">&quot;-&quot;</span> + random);<br>            subRequestMap.get(random).push(request);<br>        &#125;<br>    &#125;<br></code></pre></td></tr></table></figure></li></ul><h2 id="hw7"><a href="#hw7" class="headerlink" title="hw7"></a>hw7</h2><ul><li>在hw6中我们的分派策略已经相对完善了，<strong>我们此时只需要考虑哪些地方变了，根据需要增加条件即可</strong></li><li>不难发现，如果采用<code>LOCK</code>运行策略，那么在经过<code>UPDATE</code>改造后决不能给电梯分配一个无法到达的请求，我们在分配时还需要获取电梯可到达楼层范围的属性，在我的设计中，可到达范围由一个<code>int</code>型的共享楼层和一个<code>boolean</code>型的是否为A属性来表征。如果为A，也就是电梯改造后在上面运行的电梯，那么运行范围就是共享楼层到顶楼，反之就是底层到共享楼层</li><li>明白了hw7中变化的地方后，我们<strong>在每次分配前，还需要判断该电梯是否可到达</strong>，具体来说，如下</li><li>我给电梯的共享楼层属性默认值为100，则未改造前，电梯可到达B4-F7任意楼层，因此直接返回true。下面的判断主要由两种情况，一种是请求的起始楼层无法到达，另一种是起始楼层是共享楼层，但终点无法到达</li><li>之后再分配的条件里加上这个<code>isInRange</code>的判断即可</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">isInRange</span><span class="hljs-params">(<span class="hljs-type">int</span> eleId, PersonRequest request)</span> &#123;<br>        <span class="hljs-type">ElevatorThread</span> <span class="hljs-variable">ele</span> <span class="hljs-operator">=</span> elevatorMap.get(eleId);<br>        <span class="hljs-type">int</span> <span class="hljs-variable">sharedFloor</span> <span class="hljs-operator">=</span> ele.getSharedFloor();<br>        <span class="hljs-keyword">if</span> (sharedFloor == <span class="hljs-number">100</span>) &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>        &#125;<br>        <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-type">int</span> <span class="hljs-variable">fromFloor</span> <span class="hljs-operator">=</span> Strategy.toInt(request.getFromFloor());<br>            <span class="hljs-type">int</span> <span class="hljs-variable">toFloor</span> <span class="hljs-operator">=</span> Strategy.toInt(request.getToFloor());<br>            <span class="hljs-keyword">if</span> (ele.isA()) &#123;<br>                <span class="hljs-keyword">if</span> (fromFloor &gt; sharedFloor) &#123;<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>                &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (fromFloor == sharedFloor) &#123;<br>                    <span class="hljs-keyword">return</span> toFloor &gt; sharedFloor;<br>                &#125;<br>            &#125; <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-keyword">if</span> (fromFloor &lt; sharedFloor) &#123;<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>                &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (fromFloor == sharedFloor) &#123;<br>                    <span class="hljs-keyword">return</span> toFloor &lt; sharedFloor;<br>                &#125;<br>            &#125;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>        &#125;<br>    &#125;<br></code></pre></td></tr></table></figure><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><p>强测的请求数量一般比较多，而且很多是同一时间投放的，<strong>因此个人觉得无论是采用随机分配还是顺序分配，性能都不会太差</strong>，也无需去卷影子电梯了，而且影子电梯在hw7中的修改似乎还比较麻烦，有不少hw6成功实现影子电梯的同学在hw7中放弃了影子电梯，因为模拟的难度比较大，而且可能还有别的bug。<strong>而本人的这种杂糅随机分配和顺序分配的方式，在hw7的强测中，性能也还不错</strong>😁没有<code>WA</code>的点基本都是94-96之间（WA了两个点。。。😭）</p><h1 id="协作图"><a href="#协作图" class="headerlink" title="协作图"></a>协作图</h1><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/Unit2_seq-%E7%94%B5%E6%A2%AF%E7%B3%BB%E7%BB%9F%E5%8D%8F%E4%BD%9C%E5%9B%BE.png" alt="Unit2_seq-电梯系统协作图"></p><h2 id="稳定内容"><a href="#稳定内容" class="headerlink" title="稳定内容"></a>稳定内容</h2><ul><li>整体架构，<code>MainClass</code> –&gt; <code>Inputthread</code> –&gt; <code>DispatchThread</code> –&gt; <code>ElevatorThread</code></li><li>电梯运行策略：<code>LOOK</code>算法</li><li>电梯运行架构：<code>Strategy</code> –&gt; <code>Action</code> –&gt; <code>Elevator</code></li></ul><h2 id="变化内容"><a href="#变化内容" class="headerlink" title="变化内容"></a>变化内容</h2><ul><li>分派策略：指定电梯 –&gt; 混合策略</li><li>结束逻辑： hw6以后的线程结束需要增加逻辑，详见后面的bug分析部分</li><li>电梯运行限制增加，hw7有了楼层限制，共享层进入限制</li></ul><h1 id="双轿厢与共享楼层"><a href="#双轿厢与共享楼层" class="headerlink" title="双轿厢与共享楼层"></a>双轿厢与共享楼层</h1><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><ul><li>双轿厢输出<code>begin</code>必须要等两部电梯都把人清空完毕，这就需要建立电梯线程之间的通信机制，这也是之前的架构与设计中所没有的内容。</li><li>那么如何建立这种通信机制呢？如果将电梯视为主、子电梯，直接将子电梯的属性或者子电梯本身暴露给主电梯，这样的设计或许实现起来不难，但是却违背了设计的初衷与简洁性，将一个电梯线程暴露给另一个电梯线程，也不符合直觉与常理。</li><li>那么，要想实现两电梯线程之间的痛心，就<strong>得有一个共享对象</strong>，然后由这个共享对象来监听电梯的状态，然后输出<code>begin</code>和<code>end</code>。这里我们&#x3D;&#x3D;把共享楼层作为一种共享资源，建立一个协作类-Coordinate&#x3D;&#x3D;</li><li>在这个协作类中，不仅可以实现双轿厢改造，还可以实现对共享楼层的互斥访问</li></ul><h2 id="双轿厢改造"><a href="#双轿厢改造" class="headerlink" title="双轿厢改造"></a>双轿厢改造</h2><ul><li>在输入类读取到双轿厢改造请求时，实例化一个coordinate对象并将其传递给两个电梯线程</li><li>在电梯线程内进行改造时，按照如下步骤：<ul><li>free置为false，<strong>不再添加请求</strong></li><li>将电梯内的人赶下去，赶完后通过coordinate的方法<code>setReady()</code><strong>告诉</strong>协作类准备好了</li><li>电梯线程准备好后即开始<strong>等待</strong>另一个电梯准备，在协作类中收到两个电梯准备好的信号后即输出<code>UPDATE-BEGIN-</code> 并<strong>唤醒</strong>等待的电梯线程</li><li>电梯进行改造，设置楼层、速度（其实可以提到前面就完成改造）</li><li><strong>告诉</strong>协作类改造好了并开始等待，协作类收到两个电梯改造好的信号后即输出<code>UPDATE-END-</code>并且<strong>唤醒</strong>等待的电梯线程</li><li>改造完毕，free置为true</li></ul></li></ul><h2 id="共享楼层互斥"><a href="#共享楼层互斥" class="headerlink" title="共享楼层互斥"></a>共享楼层互斥</h2><ul><li>在进入共享楼层前查询是否已被占用，如果是则等待</li><li>退出共享楼层前释放锁</li><li>就这么简单</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//Coordinate.java</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">inShared</span><span class="hljs-params">( )</span> &#123;<br>        <span class="hljs-keyword">if</span> (isbusy) &#123;<br>            <span class="hljs-keyword">try</span> &#123;<br>                wait();<br>            &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>                <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e);<br>            &#125;<br>        &#125;<br>        isbusy = <span class="hljs-literal">true</span>;<br>    &#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">outShared</span><span class="hljs-params">( )</span> &#123;<br>    isbusy = <span class="hljs-literal">false</span>;<br>    notifyAll();<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h1><h2 id="hw5-1"><a href="#hw5-1" class="headerlink" title="hw5"></a>hw5</h2><ul><li>hw5大部分人都很漂亮，没有bug，而我重蹈第一单元的覆辙，犯了一个愚蠢的错误：<strong>没有控制电梯内人数小于等于6</strong></li><li>好笑吧，确实有点幽默了😆😆</li></ul><h2 id="hw6-1"><a href="#hw6-1" class="headerlink" title="hw6"></a>hw6</h2><ul><li><p>最大的bug是<strong>两条语句的顺序反了</strong>，，导致我强测爆了近一半的点，气死我了😭😭</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">mainRequests.push(raq);<br>TimableOutput.println(<span class="hljs-string">&quot;OUT-F-&quot;</span> + id + <span class="hljs-string">&quot;-&quot;</span> + strFloor + <span class="hljs-string">&quot;-&quot;</span> + elevatorId);<br></code></pre></td></tr></table></figure><ul><li>这是我&#x3D;&#x3D;错误的代码&#x3D;&#x3D;，在乘客尚未到达目的地而下电梯的时候，我先将其送回了主请求队列，再进行了输出</li><li>如果分派器很快地将这个请求分派给其他电梯，则到导致该乘客还未下电梯就重新分配了</li><li>将这两条语句换一下顺序即可</li></ul></li><li><p>这里我不得不提一嘴，凡是涉及到输出的地方，都&#x3D;&#x3D;一定要认真认真再认真地检查语句的执行顺序是否有潜在隐患&#x3D;&#x3D;，不然就会像我一样寄掉</p></li><li><p>还有一个值得一提的是&#x3D;&#x3D;程序结束的问题&#x3D;&#x3D;，在hw5中主请求对列为空且输入线程结束，那么分派器线程也就结束了，但是本次hw6不一样，可能在主请求对列为空且输入线程结束后，还会有人被电梯“遣送”回住请求队列，如果此时分派器线程已经结束，那么这个人就被抛弃了😆，因此需要统计所有要送的乘客数和已到达的乘客数，二者相等才能结束</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">isDone</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> done &amp; (arrivedRequests == needRequests.size());<br>    &#125;<br></code></pre></td></tr></table></figure><h2 id="hw7-1"><a href="#hw7-1" class="headerlink" title="hw7"></a>hw7</h2><ul><li>没什么特别的bug</li></ul><h2 id="debug方法"><a href="#debug方法" class="headerlink" title="debug方法"></a>debug方法</h2><ul><li>如果有精力有兴趣，最好自己或者与别人合作&#x3D;&#x3D;搭建评测机&#x3D;&#x3D;。从数据的生成，到批量运行测试点，再到评测结果与性能，自己一个人还是有些费力的，如果搭建了一个数据够强，覆盖全面，并且能正确评测，批量运行的评测机。相信我，你肯定能找出你的bug。当然，<del>如果你能要到别人的评测机，坐享其成也是不错的</del></li><li>在调试过程中，本人最擅长的还是&#x3D;&#x3D;print大法&#x3D;&#x3D;，在各个关键节点，将你所需要的信息统统打印出来，这样可以轻松判断<ul><li>数据是否正确，状态是否正确</li><li>分支进入是否符合预期</li><li>线程是否等待、被唤醒、结束</li><li>……</li></ul></li><li>如果程序无法结束或者死锁了，我建议用睿睿的输入类结合重定向输入，然后<strong>在IDEA里面调试</strong>，可以查看各个线程的状态。其他情况还是建议用课程组提供的官方输入包运行，毕竟二者行为可能会不一样。</li><li>&#x3D;&#x3D;多和室友、朋友、同学交流，别自己闭门造车&#x3D;&#x3D;，很多时候大家都在踩同样的坑或者你踩的坑别人早就踩过啦</li></ul><h1 id="心得体会"><a href="#心得体会" class="headerlink" title="心得体会"></a>心得体会</h1><ul><li>从最初对多线程一无所知到现在勉强能写个简单的多线程程序，我在第二单元还是学会了很多新的东西，<strong>对多线程编程、同步与互斥，并发、数据竞争</strong>等有了进一步的更深的理解。</li><li>Unit2也让我对线程安全有了更深刻的理解，以后的业务方向毫无疑问大部分都是多线程的问题，这时候线程的安全、数据读取的安全就尤为重要，必须要对共享资源的读写进行保护，除了基本的<code>synchronized</code>与<code>notifyAll</code>，我们还可以用读写锁来实现更高的效率与性能。当然，还有更多更高深的方法等待我去学习</li><li>印象最深的还是本单元每周五的通宵debug时刻，我<strong>清楚地记得</strong><ul><li>hw5的周五我一直de到第二天早上4点</li><li>hw6几乎占据了整个清明假期，最后也是在第三天凌晨2点通过了中测</li><li>hw7我周五晚上才开始写，直接通宵了，到第二天5点通过中测，浑然不知天已经亮了</li></ul></li><li>我也在社交平台上每周发一篇帖子，写下自己对OO的抱怨、对debug痛苦的倾诉、对压力的宣泄，也得到了很多学长学姐的鼓励与指导（甚至还有U2猪脚）</li></ul><h1 id="鸣谢❤️"><a href="#鸣谢❤️" class="headerlink" title="鸣谢❤️"></a>鸣谢❤️</h1><ul><li><strong>感谢OO课程组的老师以及可爱的助教们</strong></li><li><strong>感谢与我讨论架构、交流bug、分享经验的朋友们</strong></li><li><strong>感谢一起搭建评测机的室友小哥哥</strong></li><li><strong>感谢给予我温暖与鼓励的学长学姐们</strong></li><li><strong>感谢一路奋斗、顽强拼搏的自己</strong></li></ul>]]></content>
    
    
    <categories>
      
      <category>OO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>OO</tag>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>个人图床搭建</title>
    <link href="/2025/03/31/%E4%B8%AA%E4%BA%BA%E5%9B%BE%E5%BA%8A%E6%90%AD%E5%BB%BA/"/>
    <url>/2025/03/31/%E4%B8%AA%E4%BA%BA%E5%9B%BE%E5%BA%8A%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是本人配置个人图床的过程，希望能对各位有帮助</p>          </div><h1 id="为什么需要配置个人图床？"><a href="#为什么需要配置个人图床？" class="headerlink" title="为什么需要配置个人图床？"></a>为什么需要配置个人图床？</h1><ul><li>因为我的上一篇博客有两张图片怎么都加载不出来，或许是命名问题？但其他图片也有中文名的，又或许是中间有空格？我也不知道，反正把我惹火了，而且我也不想每次插入图片手动编辑图片名，多麻烦啊！🤔</li><li>有个人图床后，不止博客，其他需要网络图片的地方也可以用自己的图床了，好耶😀</li><li><del>没事找事，喜欢捣鼓</del></li></ul><h2 id="方案优势"><a href="#方案优势" class="headerlink" title="方案优势"></a>方案优势</h2><p>好，不啰嗦了，进入正题，我选择的是方案是<code>GitHub + jsDelivr + PicGo </code></p><p>有什么优势呢？</p><ul><li><strong>免费！免费！免费！</strong>，其实就是穷，没米儿用服务器，好像用服务器只做一个图床也duck不必🫠</li><li><strong>快！</strong>如果直接用<code>Github</code>，那么国内网络加载慢或者很可能加载不了，这时候就需要<code>jsDeliver</code>来进行加速了。</li><li>此外，还需要一个本地的图床管理器，你总不能每次修改本地图片文件夹后手动Push吧，这时候就得请出<code>PicGo</code>了</li><li>请看ds是怎么夸它们的</li></ul><p>这种组合是<strong>目前最流行且免费的个人图床解决方案</strong>，具有以下优势：</p><h3 id="1-GitHub-作为存储空间"><a href="#1-GitHub-作为存储空间" class="headerlink" title="1. GitHub 作为存储空间"></a><strong>1. GitHub 作为存储空间</strong></h3><ul><li><strong>免费</strong>：GitHub 提供<strong>免费的存储空间</strong>（每个仓库最大 1GB，单个文件最大 100MB）。</li><li><strong>稳定可靠</strong>：GitHub 是全球最大的代码托管平台，稳定性高。</li><li><strong>版本控制</strong>：GitHub 支持版本管理，可以随时回滚图片的历史版本。</li><li><strong>开源属性</strong>：GitHub 仓库可以设置为公开或私有，方便灵活。</li></ul><h3 id="2-jsDelivr-作为-CDN-加速"><a href="#2-jsDelivr-作为-CDN-加速" class="headerlink" title="2. jsDelivr 作为 CDN 加速"></a><strong>2. jsDelivr 作为 CDN 加速</strong></h3><ul><li><strong>免费 CDN</strong>：jsDelivr 是一个免费的 CDN 服务，可以加速 GitHub 仓库中的文件加载。</li><li><strong>全球加速</strong>：jsDelivr 的节点分布全球，确保图片<strong>加载速度快</strong>。</li><li><strong>简单易用</strong>：通过 jsDelivr 可以直接访问 GitHub 仓库中的图片，无需额外配置。</li></ul><h3 id="3-PicGo-作为图床管理工具"><a href="#3-PicGo-作为图床管理工具" class="headerlink" title="3. PicGo 作为图床管理工具"></a><strong>3. PicGo 作为图床管理工具</strong></h3><ul><li><strong>便捷上传</strong>：PicGo 是一款开源的图床管理工具，支持拖拽上传、剪贴板上传等多种方式。</li><li><strong>多平台支持</strong>：PicGo 支持 Windows、macOS、Linux 等操作系统。</li><li><strong>自动生成链接</strong>：上传图片后，PicGo 会自动生成图片的 CDN 链接，方便直接使用。</li><li><strong>插件扩展</strong>：PicGo 支持多种插件，可以扩展功能（如 GitHub 图床插件）。</li></ul><p>怎么样，不赖吧，下面进入正题😁</p><hr><h1 id="下载PicGo"><a href="#下载PicGo" class="headerlink" title="下载PicGo"></a>下载PicGo</h1><ul><li>直接上链接，<a href="https://github.com/Molunerfinn/PicGo/releases">点击下载</a>，你可以选择Beta测试版体验一些新功能，也可以追求稳定版。根据你的系统选择对应的即可，我是x64_exe</li><li>正常安装后打开，可能默认是静默启动的，需要你点击电脑右下角的图标打开窗口</li><li>打开后依次点击<code>图床设置</code> -&gt; <code>Github</code>,然后有一个默认的应该，点击编辑，然后你会进入以下页面</li><li><strong>这时候你就可以先配置<code>Github</code>了，回头再来</strong><ul><li>图床配置名：自己随便起个名字哦</li><li>设定仓库名：你的<code>Github</code>图床仓库名，也可以加上用户名&#x2F;</li><li>分支：默认为master,或者你自己设置的分支</li><li>Toekn：把生成的Token直接粘贴过来</li><li>设定存储路径，也就是你仓库里放置图片的路径，可以指定为一个文件夹<code>images/</code>,这样上传的图片会放到你仓库的这个文件夹下</li><li>设定自定义域名：如果你用<code>jsDeliver</code>服务，格式为<code>https://cdn.jsdelivr.net/gh/用户名/仓库名</code>，比如我的就是<code>https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting</code></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/image-20250331180510405.png" alt="image-20250331180510405"></p><h1 id="Github设置"><a href="#Github设置" class="headerlink" title="Github设置"></a>Github设置</h1><h2 id="1-建立图床仓库"><a href="#1-建立图床仓库" class="headerlink" title="1. 建立图床仓库"></a>1. 建立图床仓库</h2><p>新建一个repo，第一次可以手动push一些图片上去，最后like this<img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/image-20250331175917321.png" alt="image-20250331175917321"></p><h2 id="2-生成-GitHub-Token"><a href="#2-生成-GitHub-Token" class="headerlink" title="2. 生成 GitHub Token"></a>2. 生成 GitHub Token</h2><ol><li>在 GitHub 右上角点击头像，选择 <code>Settings</code>。</li><li>左侧菜单选择 <code>Developer settings</code> &gt; <code>Personal access tokens</code> </li><li>这时候你会发现有两个选项，一个是<code>Fine-grained</code>，一个是<code>classic</code>的，选第一个，因为第一个<strong>权限粒度更细，安全性更高</strong></li><li>点击 <code>Generate new token</code>，名称，介绍，到期时间等自定义</li><li>接下来选择<code>Only select repositories</code>,然后把你的图床仓库勾选上即可</li><li><code>Repository permissions</code><strong>只选择一个<code>Contents</code>可读可写即可</strong>，这样就可以访问你图床仓库的图片并上传了，<strong>其他的选项最好不要给予access，不然可能会造成麻烦</strong>，<code>Account permission</code>不需要添加，然后点击生成即可</li><li>复制生成的 Token（<strong>只会显示一次，务必保存好</strong>），填入到PicGo的配置中</li></ol><p>嗯，当你按部就班完成上面的步骤，应该就可以了，你可以上传一张图片试试，看看是否成功</p><h1 id="Typora"><a href="#Typora" class="headerlink" title="Typora"></a>Typora</h1><p>最后说一下，如果你使用这个编辑器的话，可以配置一下插入图片自动上传，点击偏好设置-图像</p><ul><li>插入图片时选择上传图片</li><li>上传服务选择PicGo(app)</li><li>路径填写你安装的路径，注意要直接指定为.exe</li></ul><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/image-20250331182700964.png" alt="image-20250331182700964"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>通过上述配置，你就可以丝滑地在Typora插入图片了，还不用担心图片丢失，博客也可以直接链接网络图片，速度的话相当快</li><li>希望能对你有所帮助😁😁😁</li></ul>]]></content>
    
    
    <categories>
      
      <category>爱捣鼓</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客</tag>
      
      <tag>配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BUAA-OO-Unit1</title>
    <link href="/2025/03/22/BUAA-OO-Unit1/"/>
    <url>/2025/03/22/BUAA-OO-Unit1/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>这是2025年OO第一单元的总结博客</p>          </div><h1 id="基于度量分析程序结构"><a href="#基于度量分析程序结构" class="headerlink" title="基于度量分析程序结构"></a>基于度量分析程序结构</h1><h2 id="类的设计考虑"><a href="#类的设计考虑" class="headerlink" title="类的设计考虑"></a>类的设计考虑</h2><p>先放一张类图</p><p><img src="/../img_post/hw3.svg"></p><p>我采用了<code>poly-Mono</code>的核心结构，即将各因子、项、表达式都转化为多项式Poly，而多项式是由一个个的单项式Mono所构成，经过迭代之后，Mono的基本结构为<br>$$<br>Mono &#x3D; a<em>x^b</em>\prod sin(Factor)^c*\prod cos(Factor)^d<br>$$<br>至于类的设计，我借鉴了OOpre hw7以及上机实验的框架，沿袭了Parser Lexer Expr Term以及诸多Factor类，然后根据<strong>归一化思想</strong>以及<strong>迭代需求</strong>，新增了<code>DiyFunc</code> <code>NormalFunc</code> <code>Poly</code> <code>Mono</code>  <code>TriFactor</code> <code>DeriveFactor</code> 等类。</p><p>我将我所设计的类分为了几个package，处理过程大致如下：</p><ol><li>首先是MainClass读入字符串，然后首先先将自定义函数分别放到Functions的<code>DiyFunc</code>和<code>NormalFunc</code>类进行处理，这是两个<strong>静态类</strong>，分别用于解析递归函数和普通函数，然后将解析后的函数存储在类自身的静态属性里面，后面可以直接<strong>通过类名进行调用，而无需实例化对象</strong></li><li>然后读入要化简展开的表达式，首先调用Lexer进行词法分析，分析完毕后交给Parser进行处理，<strong>Parser采用递归下降法一层一层地解析表达式</strong>，从表达式到项再到因子</li><li>解析完表达式后，下面就要将各个因子所构成的项<strong>进行归一化</strong>，在每一个类里面都有一个<code>toPoly()</code>方法，用于将本类的对象转换为一个多项式</li><li>特别地，在转换求导因子时，我是直接将求导因子括号内的表达式转换为多项式，然后调用多项式的求导方法，将<strong>返回的多项式存储为这一个求导因子的属性</strong></li><li>最后，遍历多项式的每一个单项式，打印输出</li></ol><h2 id="度量"><a href="#度量" class="headerlink" title="度量"></a>度量</h2><h3 id="类的复杂度"><a href="#类的复杂度" class="headerlink" title="类的复杂度"></a>类的复杂度</h3><ul><li>WMC 表示一个类中所有方法的复杂度之和。通常，每个方法的复杂度用 <strong>圈复杂度 (Cyclomatic Complexity)</strong> 来衡量，WMC 越高，类的复杂度越高，维护和测试的难度也越大</li><li>OCavg 反映了类中方法的平均复杂度，如果 OCavg 较高，说明类中的方法普遍较复杂，可能需要重构。</li><li>OCmax 反映了类中最复杂方法的复杂度，如果 OCmax 过高，说明类中可能存在过于复杂的方法，需要重点关注和重构</li></ul><p>![](..&#x2F;img_post&#x2F;2025-03-22 152021.png)</p><p>分析结果可知，<code>DiyFunc</code>类、<code>Lexer</code>类和<code>Parser</code>类复杂度较高，为低内聚类。这三个类分别为递归函数类、词法分析类和递归下降执行类，均为程序运行流程的直接体现，因此它们作为低内聚类是可以理解的。同时，其它类耦合度低，可以认为整份代码的抽象化程度还算比较合适。当然，我的<code>DiyFunc</code>类还是比较“笨重”的，因为对于一个参数和两个参数的递归函数，我并没有将其统一起来，而是设计了两个方法，recurrence1和recurrence2，这样的设计无疑是不太好的，但是我刚开始写的时候怕出错，也就分开设计了，后面也没有整改🫠。其实还是有改进空间的。</p><h3 id="方法的复杂度"><a href="#方法的复杂度" class="headerlink" title="方法的复杂度"></a>方法的复杂度</h3><p>这里选了复杂度的前几位</p><p><img src="/../img_post/image-20250322155006492.png"></p><p>其中，Lexer在解析输入时，有非常非常多的分支判断，因此复杂度异常地高，这也是合理的。</p><p>然后Mono的toString方法是将单项式转换为字符串，这里面牵扯到三角函数嵌套表达式的诸多问题，因此复杂度也比较高</p><h3 id="代码行数"><a href="#代码行数" class="headerlink" title="代码行数"></a>代码行数</h3><p>![](..&#x2F;img_post&#x2F;2025-03-22 151812.png)</p><ul><li>三次迭代下来，总共大概1000行代码，应该还算少的了，因为本人对于卷性能卷优化不太感冒，而且很多地方追求精简，所以代码量少了些</li></ul><h1 id="架构设计体验"><a href="#架构设计体验" class="headerlink" title="架构设计体验"></a>架构设计体验</h1><ul><li>本人架构倒是还行，至少没经历过重构，但是依旧bug多多🫠</li></ul><h2 id="第一次迭代"><a href="#第一次迭代" class="headerlink" title="第一次迭代"></a>第一次迭代</h2><p>本人在浏览了一些学长博客和实验代码后，果断选择了<strong>递归下降法+Poly-Mono结构</strong>，即最后的整个式子是一个多项式Poly,它有若干个单项式Mono相加减组成，在第一次迭代中<br>$$<br>Mono &#x3D; a*x^b<br>$$</p><h2 id="第二次迭代"><a href="#第二次迭代" class="headerlink" title="第二次迭代"></a>第二次迭代</h2><p>第二次迭代引入了三角函数因子和自定义递归函数这两个新特性，难度可谓是直线上升，思考良久后，我发现，依旧可以将其统一为Poly-Mono结构，只是Mono的结构复杂了一些<br>$$<br>Mono &#x3D; a<em>x^b</em>\prod sin(Factor)^c<em>\prod cos(Factor)^d<br>$$<br>表现在代码上就是Mono类多了一个属性，那就是<code>private ArrayList&lt;TriFactor&gt; triFactors = new ArrayList&lt;&gt;()</code>，因此，合并同类项时也带来了极大的麻烦，两个单项式可以合并的条件变得异常复杂，需要对应的三角函数的值相等，而怎么判断值相等呢？比如2</em>x^3 * sin(x+1) 和 3*x^3 * sin(1+x)，那么又需要设置三角函数的类的值相等的判断方法，但三角函数括号里可能是任何表达式或者因子，因此理论上也需要递归下降判断，但是我是直接带调用了toString方法再加上sort方法，判断字符串是否相等，对于大部分简单的情况还是能够拿捏的😁</p><h2 id="第三次迭代"><a href="#第三次迭代" class="headerlink" title="第三次迭代"></a>第三次迭代</h2><p>第三次增加了普通函数和求导因子，普通函数的各种思想和方法在递归函数里已经实现了，问题不大，求导因子的话我是选择在Poly和Mono里求导，这里需要注意的是，Mono求导返回的应该是一个Poly，然后根据乘法法则和链式法则求导即可，还要写一下三角函数的求导。这样设计的好处是<strong>只需要在三个类里面写求导方法，而不是每一层结构和每一种因子都要写，比较简洁</strong>。</p><h1 id="自己的bug"><a href="#自己的bug" class="headerlink" title="自己的bug"></a>自己的bug</h1><h2 id="第一次迭代-1"><a href="#第一次迭代-1" class="headerlink" title="第一次迭代"></a>第一次迭代</h2><p>是的，我第一次都有bug，大致就是这样😰</p><p><img src="/../img_post/image-20250322161658859.png"></p><h2 id="第二次迭代-1"><a href="#第二次迭代-1" class="headerlink" title="第二次迭代"></a>第二次迭代</h2><p><strong>这次迭代直接就爆了</strong></p><ul><li>首先是我在第一次迭代中，实现了把符号直接读入数字中，比如指数的数字以及乘号后面的数字，因此我解析到的符号都是运算符，但是第二次有函数调用，f()括号里的数字如果有符号，那么我就会解析错，属于是没考虑到吧，也或许是第一次这样设计的不太合理，第一次的设计如下：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (expr.charAt(pos) == <span class="hljs-string">&#x27;+&#x27;</span>) &#123;<br>                <span class="hljs-keyword">if</span> (pos &gt; <span class="hljs-number">1</span> &amp;&amp; (expr.charAt(pos - <span class="hljs-number">1</span>) == <span class="hljs-string">&#x27;^&#x27;</span> || expr.charAt(pos - <span class="hljs-number">1</span>) == <span class="hljs-string">&#x27;*&#x27;</span>)) &#123;<br>                    pos++;<br>                    parseNumber(<span class="hljs-number">1</span>);<br>                &#125;<br>                <span class="hljs-keyword">else</span> &#123;<br>                    tokens.add(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Token</span>(Token.Type.OP, <span class="hljs-string">&quot;+&quot;</span>));<br>                    pos++;<br>                &#125;<br>            &#125;<br>            <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (expr.charAt(pos) == <span class="hljs-string">&#x27;-&#x27;</span>) &#123;<br>                <span class="hljs-keyword">if</span> (pos &gt; <span class="hljs-number">1</span> &amp;&amp; expr.charAt(pos - <span class="hljs-number">1</span>) == <span class="hljs-string">&#x27;*&#x27;</span>) &#123;<br>                    pos++;<br>                    parseNumber(-<span class="hljs-number">1</span>);<br>                &#125;<br>                <span class="hljs-keyword">else</span> &#123;<br>                    tokens.add(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Token</span>(Token.Type.OP, <span class="hljs-string">&quot;-&quot;</span>));<br>                    pos++;<br>                &#125;<br>            &#125;<br></code></pre></td></tr></table></figure><ul><li>还有一个大的bug是我递归函数代入参数时，直接先后代入的，如果第一个实参里有第二个形参，那么也会被替换，修改后如下：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">return</span> func.replace(formPara1, <span class="hljs-string">&quot;$0&quot;</span>).replace(formPara2, <span class="hljs-string">&quot;$1&quot;</span>)<br>                    .replace(<span class="hljs-string">&quot;$0&quot;</span>, <span class="hljs-string">&quot;(&quot;</span> + para1 + <span class="hljs-string">&quot;)&quot;</span>).replace(<span class="hljs-string">&quot;$1&quot;</span>, <span class="hljs-string">&quot;(&quot;</span> + para2 + <span class="hljs-string">&quot;)&quot;</span>);<br></code></pre></td></tr></table></figure><h2 id="第三次迭代-1"><a href="#第三次迭代-1" class="headerlink" title="第三次迭代"></a>第三次迭代</h2><ul><li>主要就一个，感谢zhench的评测机测出来了，就是第二次我是用逗号切割的函数参数，但是第三次迭代，函数调用里还可能有函数调用，因此不能直接盲目用逗号切割，需要考虑括号层数</li></ul><h1 id="发现别人程序bug"><a href="#发现别人程序bug" class="headerlink" title="发现别人程序bug"></a>发现别人程序bug</h1><ul><li>主要就是用自己踩过的一些坑来测试别人的代码</li><li>构造一些卡时间或者内存的样例，来测试一些实现不够敏捷的代码</li></ul><p>这里不得不小小吐槽一下：感觉课程组的cost卡的很严，很多的数据其实开销并不算大，但是就是刀不了人，感觉这方面卡的太严了，互测的体验很糟糕，最后不得已交了一些001这样的水数据上去</p><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><p>个人并未做过度的优化，仅仅是在输出时，指数为0,1,系数为0，+-1的特殊情况简化了一下输出，看起来更加舒适，至于过度的卷性能，卷二倍角公式、和差化积公式等，我个人并不感冒，也不觉得有多么高大尚</p><h1 id="心得体会"><a href="#心得体会" class="headerlink" title="心得体会"></a>心得体会</h1><ul><li>Unit1还是压力很大的，至少我是感到比较吃力的，尤其是第二次迭代，让多少朋友们周五半夜，周六白天还在奋战。当然，可能课程组看我们上学期基本都学了oopre，所以加了些难度吧🫠</li><li>第一次最开始感觉无从下手，后面果断开始写，边写拜年思考，后面也就明悟了；但是第二次迭代感觉难度一下上去了，三角函数的引入让合并同类项出现许多难题，递归函数也是很棘手的，最后强测结果也有不少人直接寄掉，比如我；然后第三次迭代，感觉又太容易了一点儿，三下五除二就解决了。所以，这样的难度梯度设计是否合理，还是值得思考的🤔</li><li>说归说，还是很感谢课程组的，还是很感谢Unit1的，无论是递归下降法的精妙还是架构设计的经验，都让我受益良多，也锻炼了我的debug能力和抗压能力。递归下降法的精妙或许编译原理课程我们还会感受到</li></ul><h1 id="意见建议"><a href="#意见建议" class="headerlink" title="意见建议"></a>意见建议</h1><ul><li>合理设计难度梯度，避免出现某一次迭代过于困难或者过于简单的情况</li><li>合理设计互测的cost限制，让互测的体验能够更好一些</li></ul><p>UPDATE：20225.3.31<br>测试一下新建的图床是否成功<br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-07-12%20145211.png"></p>]]></content>
    
    
    <categories>
      
      <category>OO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>OO</tag>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图论复习</title>
    <link href="/2024/12/22/%E5%9B%BE%E8%AE%BA%E5%A4%8D%E4%B9%A0/"/>
    <url>/2024/12/22/%E5%9B%BE%E8%AE%BA%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<p>因为博客本身的渲染引擎对于许多latex公式很不友好，所以索性将导出的pdf上传了，希望大家有所收获<span class="github-emoji" alias="smiley" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f603.png?v8">&#x1f603;</span></p><p>有两个版本，，一个浅色，一个深色，但是深色的里面插入的图片还是浅色的，这没办法，将就下吧，hhh<br><a href="https://pzhwuhu.github.io/Notes/%E5%9B%BE%E8%AE%BA%E5%A4%8D%E4%B9%A0-%E6%B7%B1%E8%89%B2.pdf">深色pdf链接点这里</a></p><p><a href="https://pzhwuhu.github.io/Notes/%E5%9B%BE%E8%AE%BA%E5%A4%8D%E4%B9%A0-%E6%B5%85%E8%89%B2.pdf">浅色pdf链接点这里</a></p>]]></content>
    
    
    <categories>
      
      <category>离散</category>
      
    </categories>
    
    
    <tags>
      
      <tag>复习</tag>
      
      <tag>离散</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P7课下</title>
    <link href="/2024/12/15/P7%E8%AF%BE%E4%B8%8B/"/>
    <url>/2024/12/15/P7%E8%AF%BE%E4%B8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>怎么会有这么笨的人啊啊啊啊啊<span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span></p><p>弱测通过后，de了一天中测bug，最后中测通过了，但是弱测的代码没有更新</p><p>这也就导致我课上用的是最开始的那版代码，甚至连冒险测试都过不去，但此时的我<strong>浑然不知</strong>，依然用瞪眼法debug，最后遗憾离场，回来测试了半天，没啥问题，找不出bug后，我彻底慌了，后面又打开提交记录，才发现弱测提交通过后就没再更新了,寄！<span class="github-emoji" alias="innocent" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f607.png?v8">&#x1f607;</span></p><p>白白浪费掉一次上机机会，甚至不知道课下到底有无bug，总共机会也就只有一次了，此时的我，很难受，心里很不是滋味……</p><h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><ul><li>摆了摆了，贴几个非常优秀的P7博客，对我个人启发很大，我也基本是学习的他们的<ul><li><a href="https://lazyfish-lc.github.io/2024/11/21/BUAA-CO-P7/">2306 CO之神</a></li><li><a href="https://roife.github.io/posts/buaa-co-lab-p7/">roife学长</a></li><li><a href="https://flyinglandlord.github.io/2021/12/15/BUAA-CO-2021/P7/P7%E8%AF%BE%E4%B8%8A&%E8%AF%BE%E4%B8%8B/">flyinglandlord</a></li></ul></li></ul><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><ul><li>祭祖并不是全部，后面的路还很长，继续加油吧，少年！😀</li></ul>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>CPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P6课下</title>
    <link href="/2024/12/02/P6%E8%AF%BE%E4%B8%8B/"/>
    <url>/2024/12/02/P6%E8%AF%BE%E4%B8%8B/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>通过阅读本文，你可以大致了解BUAA 2024秋季 P6课下测试的相关内容<br>题目每年都会发生变化，题面仅靠回忆，题意描述可能与原题有一定差异</p>          </div><ul><li>emmm……，感觉P6比较容易，加一个乘法器，再加一些指令即可，这里就摆了~</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P6课上</title>
    <link href="/2024/11/25/P6%E8%AF%BE%E4%B8%8A/"/>
    <url>/2024/11/25/P6%E8%AF%BE%E4%B8%8A/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>通过阅读本文，你可以大致了解BUAA 2024秋季 P6课上测试的相关内容<br>题目每年都会发生变化，题面仅靠回忆，题意描述可能与原题有一定差异</p>          </div><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这次P6挂辽<span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span></p><p>T1卡了老半天才A掉,确实不难，但是细节和小坑点比较多，自己一上来还是太心急了，没有把握好细节。</p><p>晃了一眼T2，是访存，按照经验，比较难受，果断切T3，可惜的是，“经验主义”并不总是对的，这次T2很容易，卡时间周期的反而是T3 的跳转，而我最后也就输在了最后一个点的TLE的上</p><p><strong>2024.12.02 Updated</strong></p><p>这次T3极其抽象，最后刚做完，就交不了了，本地还没来得及测试，估计还得调一会儿<span class="github-emoji" alias="innocent" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f607.png?v8">&#x1f607;</span></p><p>顺便吐槽下今天的机房电脑屏幕是19寸的，14,40*900，跟平常比起来，小太多了，不过其实也没啥<span class="github-emoji" alias="grinning" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f600.png?v8">&#x1f600;</span></p><h1 id="T1-mpm"><a href="#T1-mpm" class="headerlink" title="T1 mpm"></a>T1 mpm</h1><h2 id="题面"><a href="#题面" class="headerlink" title="题面"></a>题面</h2><p>mpm rd, rs, rt</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs verilog">tmp1 = GRF(rs) + GRF(rt) <br>k = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i in <span class="hljs-number">32</span>……<span class="hljs-number">0</span><br>tmp2 = <span class="hljs-number">1</span> &lt;&lt; i<br><span class="hljs-keyword">if</span>(tmp2 &gt; tmp1) k = i<br>endfor<br>GRF(rd) = k<br></code></pre></td></tr></table></figure><p>其实就是求最小的大于tmp1的2的幂</p><p><strong>忽略加法过程中的溢出</strong></p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>直接对着翻译差不多，不过几个细节需要注意</p><ul><li><strong>tmp2 应该至少为33位</strong>，因为tmp1 可能为0xffff_ffff，这样需要0x1_0000_0000才能大于tmp1，这一点从循环 i in 32……0也可以看出</li><li><strong>RTL语言中是从32遍历到0</strong>，而我们可能习惯的是0到32，这样的话无法求出“最小的”，因为当满足条件时，<strong>你缺少了一个break</strong></li></ul><p>毫无疑问，两个坑我都踩了，并且第一个坑我开始还把tmp1也改成33位的了，<strong>这样导致忽略溢出这一操作无法实现</strong>，并且我最后才发现这个问题<span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span></p><p>至于第二个问题，也好办，如果你是32到0 的遍历，那没有任何问题，如果你是0到32 的遍历，需要在满足条件时直接把 i 赋值为32即可，因为verilog中没有break语句</p><h1 id="T1-2"><a href="#T1-2" class="headerlink" title="T1(2)"></a>T1(2)</h1><h2 id="题面-1"><a href="#题面-1" class="headerlink" title="题面"></a>题面</h2><p>就是将GRF(rs) 循环右移32次的结果相加，将GRF(rt) 循环右移32次的结果相加，两者比较，前者小于后者就赋值rd 为-1， 大于的话就赋值1，相等赋值0</p><h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p>RTL语言是说</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs verilog">res = x<br><span class="hljs-keyword">for</span> i from <span class="hljs-number">1</span> to <span class="hljs-number">31</span><br>    res = res + x[i-<span class="hljs-number">1</span>:<span class="hljs-number">0</span>] || x[<span class="hljs-number">31</span>:i]<br>    <br></code></pre></td></tr></table></figure><p>注意，位宽的中括号两边不能有变量，因此无法直接翻译RTL语言</p><p>解法如下,最后比较tmp3和4就好</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs verilog">tmp1 = inA;<br>tmp2 = inB;<br>tmp3 = inA;<br>tmp4 = inB;<br><span class="hljs-keyword">for</span>(i=<span class="hljs-number">1</span>;i&lt;<span class="hljs-number">32</span>;i=i+<span class="hljs-number">1</span>) <span class="hljs-keyword">begin</span><br>    tmp1 = &#123;tmp1[<span class="hljs-number">0</span>], tmp1[<span class="hljs-number">31</span>:<span class="hljs-number">1</span>]&#125;;<br>    tmp2 = &#123;tmp2[<span class="hljs-number">0</span>], tmp2[<span class="hljs-number">31</span>:<span class="hljs-number">1</span>]&#125;;<br>    tmp3 = tmp1;<br>    tmp4 = tmp2;<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><h1 id="T2"><a href="#T2" class="headerlink" title="T2"></a>T2</h1><h2 id="题面-2"><a href="#题面-2" class="headerlink" title="题面"></a>题面</h2><p>没有详细的RTL语言，我就描述一下吧</p><p>跟正常的sw差不多，但是 high_1 &#x3D; GRF(rt)中最高位1 的位置，addr &#x3D; base + offset + high_1 || 00，</p><p>最后Mem[addr[31:2] || 00] &#x3D; GRF(rt)，大致是这样</p><h2 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h2><p>感觉实现起来不难，DM加一个输入信号即可，可悲的是我并没有做这道题</p><h1 id="T3"><a href="#T3" class="headerlink" title="T3"></a>T3</h1><h2 id="题面-3"><a href="#题面-3" class="headerlink" title="题面"></a>题面</h2><p>似乎是叫mpl，不管了</p><p>大致是说</p><p>tmp &#x3D; GRF(rs) + GRF(rt)</p><p>如果rs &#x3D;&#x3D; rt，就跟beq一样跳转，否则如果rs rt奇偶性相同，就跳转到 tmp[31:2] || 00，如果都不满足就不跳</p><p><strong>无论是否跳转都要链接</strong></p><p><strong>注意比较的是rs  rt 本身这个编号，而不是寄存器的值</strong></p><h2 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h2><p>整体上搭建起来也不算太难，CMP， NPC新增端口， 加一点逻辑，不需要改基本上就可以实现</p><p>但是，你交上去，会发现你过了9个点，剩下的那个点会告诉你，<strong>你TLE了</strong></p><p>课下我基本已经全速了，只缺一个lui可以E到D的转发，懒得加了，后来证明，加这个还是会被卡</p><p>所以，也就是说，<strong>如果只将新指令的Tuse设为0，并且不对阻塞单元加点逻辑，那么一定会被卡</strong> <span class="github-emoji" alias="innocent" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f607.png?v8">&#x1f607;</span><del>助教坏坏</del></p><p>具体怎么办呢？我们发现<strong>并不是所有情况下这条新指令都需要使用寄存器</strong>，当且仅当 rs !&#x3D; rt &amp;&amp; rs[0] &#x3D;&#x3D; rt[0]的情况下才需要使用寄存器，那么我们就需要在阻塞单元加一个特判,举一个例子</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">assign</span> freeze_RS0_E2 = NewSign ? (D_A1 != D_A2 &amp;&amp; D_A1[<span class="hljs-number">0</span>] == D_A2[<span class="hljs-number">0</span>]) : 原来的逻辑<br></code></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>决策失误了，默认T2访存恶心，结果反倒简单,还是建议大家把3道题都读一读，思考一下，再做决策吧，不要像某人一样<span class="github-emoji" alias="innocent" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f607.png?v8">&#x1f607;</span></li><li>T1太过着急，细节没把握好，浪费大量时间</li><li>T3属实没想到这么卡时间，因为之前都是访存类W级才确定写寄存器和写数据，所以无脑阻塞2周期卡时间，这次属实没想到这么卡，有点阴间了</li></ul><h2 id="下次加油吧🫠"><a href="#下次加油吧🫠" class="headerlink" title="下次加油吧🫠"></a>下次加油吧🫠</h2><h2 id="P7加油-x1f60e"><a href="#P7加油-x1f60e" class="headerlink" title="P7加油 &#x1f60e;"></a>P7加油 <span class="github-emoji" alias="sunglasses" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60e.png?v8">&#x1f60e;</span></h2>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>Verilog</tag>
      
      <tag>CPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P5课上</title>
    <link href="/2024/11/18/P5%E8%AF%BE%E4%B8%8A/"/>
    <url>/2024/11/18/P5%E8%AF%BE%E4%B8%8A/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>通过阅读本文，你可以大致了解BUAA 2024秋季 P5课上测试的相关内容<br>题目每年都会发生变化，题面仅靠回忆，题意描述可能与原题有一定差异</p>          </div><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>容我先BB几句，极限极限极限极限极限极限极限极限极限极限</p><p>十分侥幸通过了P5课上测试，没能做完T2，留下些许遗憾吧，不过，就像jzy哥哥说的那样“过了就好<span class="github-emoji" alias="grin" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png?v8">&#x1f601;</span>”，课上出了几处状况，曾一度以为要寄，最好侥幸通过了……课上状况留到最后再说吧，先来看看题目</p><h1 id="必看"><a href="#必看" class="headerlink" title="必看"></a>必看</h1><div class="note note-danger">            <p>不要盲目顺序做题 ！</p><p>不要盲目顺序做题 ！</p><p>不要盲目顺序做题 ！</p>          </div><p><strong>一般来说，优先级，计算 &gt; 跳转 &gt; 访存</strong></p><h1 id="T1-bmoal"><a href="#T1-bmoal" class="headerlink" title="T1 bmoal"></a>T1 bmoal</h1><h2 id="题面"><a href="#题面" class="headerlink" title="题面"></a>题面</h2><p><code>bmoal rs, offset</code></p><p>若 GRF(rs) 中 1 的 个数大于 0  的个数，就和<code>beq</code>一样跳转,并且 链接，也就是 GRF(31) &lt;- PC + 8；否则，啥也不干</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>这是一道跳转类的题，需要修改CmpOp以及CMP内部比较逻辑，Control模块，以及流水线寄存器</p><p>至于NPC可以复用beq的接口，因为跳转方式一样</p><p>对于Tuse，Tuse_rs设为0，Tuse_rt设为3(默认就为3)就好了，Tnew的话1或者0都没影响，具体见P5课下</p><p>因为是否写31号寄存器需要根据比较结果来判断<strong>而无法直接由Control解码得到</strong>，因此需要将比较结果往后传递，也就是加入后面各级的流水线寄存器</p><p><strong>所以需要在E M W级将比较结果condition加入到Control模块中，再输出RegWrite信号</strong></p><p>具体一点就是</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">case</span>(opcode)<br>newOp:<br>            <span class="hljs-keyword">begin</span><br>                newSign  = <span class="hljs-number">1&#x27;b1</span>;<br>                CmpOp    = <span class="hljs-number">2&#x27;b11</span>;<br>                AluOp    = <span class="hljs-number">4&#x27;b0101</span>;<span class="hljs-comment">//和课下jal一样</span><br>                A3       = <span class="hljs-number">5&#x27;d31</span>;<br>                Tuse_rs  = <span class="hljs-number">2&#x27;d0</span>;<br>                Tnew     = <span class="hljs-number">2&#x27;d1</span>;<br>                <span class="hljs-keyword">if</span>(condition) <span class="hljs-keyword">begin</span><br>                    RegWrite = <span class="hljs-number">1&#x27;b1</span>;<br>                <span class="hljs-keyword">end</span><br>            <span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>所以，<strong>建议提前准备好相应的接口</strong>，课上临时加会非常麻烦，而且紧张的条件下很可能会出一些小bug，提前准备将<code>CmpResult</code>流水至后面的寄存器</p><h1 id="T2-basel"><a href="#T2-basel" class="headerlink" title="T2 basel"></a>T2 basel</h1><h2 id="题面-1"><a href="#题面-1" class="headerlink" title="题面"></a>题面</h2><p>lc base1, offset(base)</p><p>GRF(base)和GRF(base1)分别作为基地址，offset为偏移量，从内存中取出来两个数据，分别为data1和data,将两者的低五位作为寄存器编号</p><p>if(data1 &gt; data) GRF[data1[4:0]] &lt;- data1</p><p>else    GRF[data[4:0]] &lt;- data</p><p>需要<strong>魔改DM，然后在W级才知道要写哪个寄存器</strong>，比较抽象，<strong>暴力阻塞可能会TLE</strong>，需要优化一下，不多说了，毕竟考场上没时间啃了，也无法确定自己的想法是否正确，就不误导大家了。</p><h1 id="T3-ae"><a href="#T3-ae" class="headerlink" title="T3 ae"></a>T3 ae</h1><h2 id="题面-2"><a href="#题面-2" class="headerlink" title="题面"></a>题面</h2><p><code>ae rt, Imm_16</code></p><p><code>temp[15] = Imm[15] ^ Imm[14]</code></p><p>……</p><p><code>temp[1] = Imm[1] ^ Imm[0]</code></p><p><code>temp[0] = Imm[0] ^ 0</code></p><p><code>GRF[rt]</code> &#x3D; <code>temp[15] || temp[14] || …… || temp[0]</code> || <code>Imm_16</code></p><h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p>没啥特别的，直接运算就行，跟add唯一的区别就是AluOp以及具体的运算</p><h1 id="课上出状况"><a href="#课上出状况" class="headerlink" title="课上出状况"></a>课上出状况</h1><h2 id="ISE无法仿真"><a href="#ISE无法仿真" class="headerlink" title="ISE无法仿真"></a>ISE无法仿真</h2><ul><li>某一次仿真后关闭，修改后再次仿真时就显示仿真已经在运行，，无语了，直接新建工程再重新导入了</li><li>室友说可以直接在Project栏下Clearup project files</li></ul><h2 id="T3"><a href="#T3" class="headerlink" title="T3"></a>T3</h2><p>首先不得不承认，本人课下确实有一些问题，但是测试却测不出来，具体一点就是ALU模块里全用的非阻塞赋值，非阻塞赋值是在块结束时并发进行的，对于结果，似乎也没啥问题，但这也埋下了隐患</p><p>具体一点</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">always</span> @(*) <span class="hljs-keyword">begin</span><br>        <span class="hljs-keyword">case</span> (AluOp)<br>            <span class="hljs-number">4&#x27;b0000</span>: <span class="hljs-keyword">begin</span><br>                result &lt;= inA + inB;<br>            <span class="hljs-keyword">end</span> <br></code></pre></td></tr></table></figure><p>对于新指令，<strong>我却采用的是阻塞赋值</strong>，具体如下</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">always</span> @(*) <span class="hljs-keyword">begin</span><br>        <span class="hljs-keyword">case</span> (AluOp)<br>            <span class="hljs-number">4&#x27;b0000</span>: <span class="hljs-keyword">begin</span><br>                result &lt;= inA + inB;<br>            <span class="hljs-keyword">end</span><br>            <br>            newOp  : <span class="hljs-keyword">begin</span><br>            tmp1 = inB[<span class="hljs-number">15</span>:<span class="hljs-number">0</span>];<br>            tmp2 = <span class="hljs-number">16&#x27;b0</span>;<br>                <span class="hljs-keyword">for</span>(i = <span class="hljs-number">1</span>;i &lt; <span class="hljs-number">16</span>;i = i + <span class="hljs-number">1</span>) <span class="hljs-keyword">begin</span><br>                    tmp2[i] = tmp1[i] ^ tmp1[i-<span class="hljs-number">1</span>];<br>            <span class="hljs-keyword">end</span><br>                tmp2[<span class="hljs-number">0</span>] = tmp1[<span class="hljs-number">0</span>] ^ <span class="hljs-number">0</span>;<br>                result  = &#123;tmp2, tmp1&#125;;<br>            <span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>但是这样，我每次执行新指令，<strong>result会输出前一条指令的result</strong>，无论我后来怎么改，都是这样，期间进行了无数改造，无数测试，包括但不限于把tmp1, tmp2整成32位的，再或一下，赋给result，但还是不对，但是tmp1. tmp2各自分别是对的，这我就恼火了，又傻看了半天，最后又整了一个tmp3,先tmp3 &#x3D; tmp1 | tmp2,再result &#x3D; tmp3;可惜，tmp3在波形图里对了，但是result还是之前那条指令的值。</p><p>这时候已经快到八点了，可我还在做最简单的计算题，几乎快要崩溃时，我注意到了 <code>=</code> 和 <code>&lt;=</code>,这里应该阻塞赋值啊，但我前面都用的非阻塞赋值，也没错啊，半信半疑中，我把最后一句改为</p><p>result &lt;&#x3D; tmp3;</p><p>一仿真，对了，艹，至于你问我为什么最后一句改成非阻塞赋值就对了，我只能说：经典<code>verilog</code>抽象日常，不过<strong>正确写法应该是全部用阻塞赋值</strong>，但课上也没时间细想了<span class="github-emoji" alias="thinking" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f914.png?v8">&#x1f914;</span>。</p><p>到头来还是课下留下的隐患<span class="github-emoji" alias="innocent" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f607.png?v8">&#x1f607;</span>唉唉，而且是测不出来的那种，至于为什么这里会写非阻塞赋值呢？我直接copy的P4的ALU，当时为什么非阻塞呢，我也不知道，可能就一时敲错了。。（话说我P4课上咋没有这个问题<span class="github-emoji" alias="thinking" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f914.png?v8">&#x1f914;</span>抽象bug，抽象verilog，抽象ISE<span class="github-emoji" alias="innocent" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f607.png?v8">&#x1f607;</span>）</p><h2 id="T1"><a href="#T1" class="headerlink" title="T1"></a>T1</h2><p>刚整完T3,已经八点多了，回头来解T1，是条件跳转，且条件链接，我提前准备了相关接口，直接连上，调了一会儿之后，本地测试没问题了，此时是20:33，我交上去了，正当我喝了口水准备交卷时，出现了一个红色的<code>Wrong Answer</code>，我一点开，发现10 个点过了8个，还有两个在该跳转的地方并没有跳转，这下麻烦了，因为10个点过了8个，说明你的代码在大部分情况下没问题，这就说明你很难自己构造数据卡出bug，只能瞪眼看代码debug了，我是这样的<span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span></p><p>于是回头检查我的代码，我的Cmp代码如下：</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">module</span> D_CMP(<br>    <span class="hljs-keyword">input</span> [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>] data1,<br>    <span class="hljs-keyword">input</span> [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>] data2,<br>    <span class="hljs-keyword">input</span> [<span class="hljs-number">1</span>:<span class="hljs-number">0</span>] CmpOp,<br>    <span class="hljs-keyword">output</span> <span class="hljs-keyword">reg</span> out<br>    );<br>    <span class="hljs-keyword">reg</span> [<span class="hljs-number">4</span>:<span class="hljs-number">0</span>] num1;<br>    <span class="hljs-keyword">integer</span> i;<br>    <span class="hljs-keyword">always</span> @(*) <span class="hljs-keyword">begin</span><br>        <span class="hljs-keyword">if</span> (CmpOp == <span class="hljs-number">2&#x27;b00</span>) <span class="hljs-keyword">begin</span><br>            out = (data1 == data2);<br>        <span class="hljs-keyword">end</span><br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (CmpOp == <span class="hljs-number">2&#x27;b01</span>) <span class="hljs-keyword">begin</span><br>            out = (data1 != data2);<br>        <span class="hljs-keyword">end</span> <br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (CmpOp == <span class="hljs-number">2&#x27;b10</span>) <span class="hljs-keyword">begin</span><br>            num1 = <span class="hljs-number">5&#x27;d0</span>;<br>            <span class="hljs-keyword">for</span>(i=<span class="hljs-number">0</span>;i&lt;<span class="hljs-number">32</span>;i=i+<span class="hljs-number">1</span>) <span class="hljs-keyword">begin</span><br>                <span class="hljs-keyword">if</span>(data1[i]) num1 = num1 + <span class="hljs-number">5&#x27;d1</span>;<br>            <span class="hljs-keyword">end</span><br>            out = (num1 &gt; <span class="hljs-number">5&#x27;d16</span>) ? <span class="hljs-number">1&#x27;b1</span> : <span class="hljs-number">1&#x27;b0</span>;<br>        <span class="hljs-keyword">end</span> <br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>            out = <span class="hljs-number">1&#x27;b0</span>;<br>        <span class="hljs-keyword">end</span><br>    <span class="hljs-keyword">end</span><br> <br><span class="hljs-keyword">endmodule</span><br></code></pre></td></tr></table></figure><p>看了一会儿，发现num1可能为32啊，5位只能表示0~31啊，艹，于是把所有的5位都改成6位了，这下窃喜了，以为可以走了，结果交上去，还是WA，此时已经快20:45了，说实话，心里真急了<span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span>但这题也不难啊<span class="github-emoji" alias="broken_heart" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f494.png?v8">&#x1f494;</span>于是我反复对照RTTL语言，做了如下修改，交上去A了</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">reg</span> [<span class="hljs-number">4</span>:<span class="hljs-number">0</span>] num1, num0;<br><span class="hljs-keyword">integer</span> i;<br><br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (CmpOp == <span class="hljs-number">2&#x27;b10</span>) <span class="hljs-keyword">begin</span><br>            num1 = <span class="hljs-number">6&#x27;d0</span>;<br>        num0 = <span class="hljs-number">6&#x27;d0</span>;<br>            <span class="hljs-keyword">for</span>(i=<span class="hljs-number">0</span>;i&lt;<span class="hljs-number">32</span>;i=i+<span class="hljs-number">1</span>) <span class="hljs-keyword">begin</span><br>                <span class="hljs-keyword">if</span>(data1[i]) num1 = num1 + <span class="hljs-number">6&#x27;d1</span>;<br>            <span class="hljs-keyword">end</span><br>      <span class="hljs-comment">//out = (num1 &gt; 6&#x27;d16) ? 1&#x27;b1 : 1&#x27;b0;</span><br>    num0 = <span class="hljs-number">6&#x27;d32</span> - num1;<br>    out = (num1 &gt; num0) ? <span class="hljs-number">1&#x27;b1</span> : <span class="hljs-number">1&#x27;b0</span>;<br>        <span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>注释掉的是之前的写法，改之后交上去A了，<strong>我趣，天不亡我啊</strong>  <span class="github-emoji" alias="grin" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png?v8">&#x1f601;</span></p><p>回来和两个室友讨论，测试了半天，也不知道哪有问题，室友说我可能写的是num1 &gt; 5’d16，16这里忘记改了，但是我们测试发现，写5‘d16问题也不大，verilog比较时会把6’ &gt; 5’这样的比较式右边扩展</p><p>chatgpt作如下回答：</p><p><img src="/../img_post/p5_3.png"></p><p>标准如下：</p><p><img src="/../img_post/p5_1.png"></p><p>最后也没搞懂哪出问题了，，哎，<code>verilog</code>就这样，经常会有一些奇奇怪怪的问题，让人摸不着头脑，再比如极其抽象的符号法则等等，搭配上使用舒适度几乎为0的ISE，一个字，绝！</p><p>不多说了，说到底还是自身能力有欠缺，接下来就是P6了，加油吧<span class="github-emoji" alias="smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">&#x1f604;</span></p>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>Verilog</tag>
      
      <tag>CPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P5课下</title>
    <link href="/2024/11/18/P5%E8%AF%BE%E4%B8%8B/"/>
    <url>/2024/11/18/P5%E8%AF%BE%E4%B8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>P5的工程量真的很大，<strong>建议不要盲目开始</strong>，上来就敲代码，而是先规划好自己的设计，确定架构，命名规范，这样搭建起来就容易多了，在开始P5之前，你得完成以下工作</p><ul><li>阅读完P5教程</li><li>阅读理解完相应的流水线CPU设计的PPT或者黑书的相关章节</li><li>多看几位学长的博客，确定好自己的架构</li><li>完成草图设计，或者直接用别人的或者书上的，但一定要清楚自己的整体设计</li></ul><div class="note note-danger">            <ul><li>可能老师PPT上的设计图并不完整，没有必要的转发线路，<strong>自己注意辨别</strong></li><li>如果你懒得自己设计或者找正确的设计图，下面这份我自己的设计图或许能对你有所帮助</li><li>在阅读下文之前，你得至少完成上文提到的必备工作的前两点</li></ul>          </div><h1 id="设计要点"><a href="#设计要点" class="headerlink" title="设计要点"></a>设计要点</h1><p>下面是我用<code>logisim</code>设计的一份架构草图，直接在P3上面改的，没那么美观，有些细节也没有，大家看清楚结构和逻辑就好</p><p><strong>我的设计是分布式译码，但是需要将D级产生的Tnew信号流水至E M级</strong>，<strong>通过AT法判断冲突</strong></p><p><strong><img src="/../img_post/p5_2.png"></strong></p><p>在P5我们将指令的执行分为了五个阶段</p><ul><li>F级取指令</li><li>D级取寄存器的值，进行跳转判断，并且根据判断结果决定NPC</li><li>E级进行ALU运算</li><li>M级对DM进行存取</li><li>W级写寄存器</li></ul><p><strong>每个阶段的一些状态信息和数据都通过流水线寄存器传递给下一个阶段</strong>，从而继续执行该指令</p><p>那么，我们需要在W级才能写入寄存器，但此时在D E M级的指令又需要寄存器的值来进行一系列操作，如果是同一个寄存器，那么就发生了数据冲突，怎么解决呢？<strong>就是采用转发或者阻塞</strong></p><h2 id="转发"><a href="#转发" class="headerlink" title="转发"></a>转发</h2><p>试想一下下面这一段代码</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs verilog">add $<span class="hljs-number">4</span>, $<span class="hljs-number">2</span>, $<span class="hljs-number">3</span><br>lw $<span class="hljs-number">5</span>, <span class="hljs-number">0</span>($<span class="hljs-number">4</span>)<br>sw $<span class="hljs-number">5</span>, <span class="hljs-number">0</span>($<span class="hljs-number">6</span>)<br>add $<span class="hljs-number">6</span>, $<span class="hljs-number">5</span>, $<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>lw 在D级时从GRF中取出4号寄存器的值，但是这时候add指令在E级，4号寄存器的值还未更新，我们取到的并不是一个正确的寄存器的值，那怎么办呢？我们不妨想想，对于lw指令，我们在D级真的需要4号寄存器的正确值吗？并不需要，因为在E级<code>lw</code>指令才会参与运算，所以我们<strong>真正需要4号寄存器的值是在E级</strong>，也就是说<code>Tnew_rs</code> &#x3D; 1,此时add指令在M级，虽然此时我们lw指令得到的值并非正确，但真正正确的值已经产生了，就在<code>AluOut</code>里面，那么此时我们将M级<code>AluOut</code>的值给ALU的第一个输入inA,这样inA的值就<strong>得到了矫正</strong></p><p>上面就是转发的分析过程，这组数据还有不少其他的转发情况，请自行分析清楚</p><p>同时，由于beq指令的判断在D级，我们此时必须要得到正确的寄存器的值，也就是<code>Tuse</code>为0</p><p>总的来看，转发线路至少要有这么几条</p><p><em>M到D，W到D, M到E，W到E，W到M</em>(<strong>有老师的PPT里没有这个线路</strong>)</p><p>W到D其实就是所谓的内部转发，但其实把GRF看成两个部件的话，也就没有内部转发这一说了</p><h2 id="阻塞"><a href="#阻塞" class="headerlink" title="阻塞"></a>阻塞</h2><p>上面的例子中，我们要得到正确寄存器的值的时候，这个值已经产生了，就在某个流水线寄存器里面，我们可以直接转发，但是如果这个值没有产生呢？</p><p>比如说<code>add</code>后面跟一个<code>beq</code>指令，<code>beq</code>在D级就需要得到正确寄存器的值，但此时E级的add还没有正确的值（课程组要求必须从流水线寄存器后面转发，所以不能直接E级ALU的结果）</p><p>很遗憾，我们这时候只能够暂停后面的流水线，直到正确的寄存器的值产生，这就是阻塞</p><p>具体的说，我们只需要将DE级流水线寄存器的指令设为0，同时保持IFU里的PC不变，F级的指令不变，FD流水线寄存器不变，这样等效于在<code>add</code>和<code>beq</code>之间插入了一个<code>nop</code>指令，阻塞之后M级就可以向D级转发正确的数据了</p><h2 id="综合"><a href="#综合" class="headerlink" title="综合"></a>综合</h2><h3 id="T信号"><a href="#T信号" class="headerlink" title="T信号"></a>T信号</h3><ul><li><code>Tuse</code>：指令进入 <strong>D 级</strong>后，其后的某个功能部件<strong>再</strong>经过多少时钟周期就<strong>必须</strong>要使用寄存器值。对于有两个操作数的指令，其<strong>每个操作数的 Tuse 值可能不等</strong>（如 store 型指令 rs、rt 的 Tuse 分别为 1 和 2 ）。</li><li>Tnew：位于 <strong>E 级及其后各级</strong>的指令，再经过多少周期就能够<strong>产生要写入寄存器的结果</strong>。在我们目前的 CPU 中，W 级的指令Tnew 恒为 0；对于同一条指令，Tnew_next &#x3D; max(Tnew_now - 1, 0),本表格中我们分析D级的Tnew,并让Tnew随着流水线传递并递减,x表示不需要使用寄存器，代码中设置为3即可</li></ul><table><thead><tr><th>指令分类</th><th><code>Tuse_rs</code></th><th><code>Tuse_rt</code></th><th><code>Tnew</code></th></tr></thead><tbody><tr><td>calc_R</td><td>1</td><td>1</td><td>2</td></tr><tr><td>calc_I</td><td>1</td><td>x</td><td>2</td></tr><tr><td>load</td><td>1</td><td>x</td><td>3</td></tr><tr><td>store</td><td>1</td><td>2</td><td>0</td></tr><tr><td>branch</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Jal</td><td>x</td><td>x</td><td>2</td></tr><tr><td>Jr</td><td>0</td><td>x</td><td>0</td></tr></tbody></table><h3 id="冲突控制单元CU"><a href="#冲突控制单元CU" class="headerlink" title="冲突控制单元CU"></a>冲突控制单元CU</h3><p>Tuse &lt; Tnew时必须要阻塞<code>Freeze</code>，Tuse&gt;Tnew时转发<code>Forward</code>即可</p><table><thead><tr><th><code>Tuse</code>&#x2F;<code>Tnew</code></th><th><code>Tnew_E</code></th><th><code>Tnew_E</code></th><th><code>Tnew_E</code></th><th><code>Tnew_M</code></th><th><code>Tnew_M</code></th><th><code>Tnew_W</code></th></tr></thead><tbody><tr><td><code>Tuse</code></td><td><code>2</code></td><td><code>1</code></td><td><code>0</code></td><td><code>1</code></td><td><code>0</code></td><td><code>0</code></td></tr><tr><td><code>0</code></td><td>Freeze</td><td>Freeze</td><td>Forward</td><td>Freeze</td><td>Forward</td><td>Forward</td></tr><tr><td><code>1</code></td><td>Freeze</td><td>Forward</td><td>Forward</td><td>Forward</td><td>Forward</td><td>Forward</td></tr><tr><td><code>2</code></td><td>Forward</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>因此我们需要设置好每条指令的<code>Tnew</code>和<code>Tuse</code>，再根据<code>RegWrite</code>等信号以及寄存器编号是否相等来判断转发或者阻塞逻辑,当<code>Tnew</code> &gt; <code>Tuse</code>的时候我们就必须要阻塞了</p><p><strong>同时必须对0号寄存器进行判断</strong>，因为我们无法对0号寄存器进行写入</p><p>下面放一个CU模块，也就是冲突处理模块的代码片段</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs verilog"> <span class="hljs-comment">//freeze module</span><br> <span class="hljs-keyword">wire</span> freeze_rs0_E2, freeze_rs0_E1, freeze_rs0_M1, freeze_rs1_E2,<br>      freeze_rt0_E2, freeze_rt0_E1, freeze_rt0_M1, freeze_rt1_E2;<br><span class="hljs-comment">//freeze_rs0_E2表示D级要使用寄存器，但是E级的指令还需要2个周期才能得到寄存器的正确值，如lw + beq</span><br> <span class="hljs-keyword">assign</span> freeze_rs0_E2 = (Tuse_rs == <span class="hljs-number">2&#x27;d0</span>) &amp; (Tnew_E == <span class="hljs-number">2&#x27;d2</span>) &amp; E_RegWrite &amp; (D_A1 == E_A3) &amp; (E_A3 &gt; <span class="hljs-number">5&#x27;d0</span>);<br> <span class="hljs-comment">//…………waiting for you to finish</span><br><br> <span class="hljs-keyword">assign</span> freeze = freeze_rs0_E2 | freeze_rs0_E1 | freeze_rs0_M1 | freeze_rs1_E2 |<br>                 freeze_rt0_E2 | freeze_rt0_E1 | freeze_rt0_M1 | freeze_rt1_E2 ;<br><br><br> <span class="hljs-comment">//forward module</span><br> <span class="hljs-comment">//D_RD1</span><br> <span class="hljs-keyword">wire</span> M_to_D_RD1, W_to_D_RD1;<span class="hljs-comment">//M W级向D级RD1的转发</span><br> <span class="hljs-keyword">assign</span> M_to_D_RD1 = (D_A1 == M_A3  &amp;&amp; M_RegWrite &gt; <span class="hljs-number">1&#x27;d0</span> &amp;&amp; M_A3 &gt; <span class="hljs-number">5&#x27;d0</span> &amp;&amp; Tnew_M == <span class="hljs-number">2&#x27;d0</span>);<br> <span class="hljs-keyword">assign</span> W_to_D_RD1 = (D_A1 == W_A3  &amp;&amp; W_RegWrite &gt; <span class="hljs-number">1&#x27;d0</span> &amp;&amp; W_A3 &gt; <span class="hljs-number">5&#x27;d0</span>);<br><br> <span class="hljs-keyword">assign</span> D_Sel_RD1 = M_to_D_RD1 ? <span class="hljs-number">2&#x27;b01</span> :<br>                    W_to_D_RD1 ? <span class="hljs-number">2&#x27;b10</span> : <span class="hljs-number">2&#x27;b00</span>;<br><br> <span class="hljs-comment">//…………waiting for you to finish</span><br> <br></code></pre></td></tr></table></figure><p>解决完转发阻塞后，P5的重大工程已经完成，其他的就照着电路图连线即可，这里想说说jal</p><h2 id="jal"><a href="#jal" class="headerlink" title="jal"></a>jal</h2><p>乍看<code>jal</code>，似乎D级解码之后我们就知道了PC+8的值，也就是说<code>Tnew</code>为0，但事实如此吗？</p><p>在不少人设计中，<code>jal</code>的PC是流水到W级才写入寄存器的，这时候，正确的<code>Tnew</code>应该为3,这样的设计没有问题，但是可能因此<strong>造成不必要的阻塞，导致CPU跑不快</strong></p><p>我们仔细思考，确确实实D级解码之后我们就知道了PC+8的值，那么我们就需要增加转发线路了，实现E, M, W级PC+8值的转发，这样确实也可以</p><p>但我们采用归一化的思想，这样并不好，并不能系统地解决该问题，而是采取了类似特判的形式，<strong>我们总得尝试下更优雅的做法吧</strong></p><p>我自己摸索出来的一种方法是将E级的PC连到ALU里面，ALU输出PC+8，也就是让<code>jal</code>某种形式上也成了运算类指令,看RTL语言，不也类似么？</p><p>这样做的好处就是我们无需对CU模块进行任何修改，只需要像其他运算指令一样将<code>jal</code>的<code>Tnew</code>设置为2，A3设置为31，即可，此时要转发的数据（PC+8)已经在<code>AluOut</code>中了,<strong>这样便获得了更好的统一性、归一性</strong></p><p>有同学说这样无法实现E到D级的转发啊，仔细想想，对于<code>jal</code>，我们需要E到D的转发么？<strong>并不需要</strong></p><p>因为考虑延迟槽，当<code>jal</code>在E级时,它的后一条指令也就是D级的指令不会是<code>beq</code>指令，这样的测试数据是非法的，也就是说不会连着跳</p><p>那么对于其他的非分支指令，D级并不需要寄存器，这样就不需要转发了</p><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p><strong>课下的测试数据仅供参考，对了也要自己测一测强一点的数据</strong>，避免把课下的bug留到课上，那样就比较难受了</p><p>因此，如果对自己的设计没有信心，可以多多构造数据或者用别人的数据狠狠测试，不留bug</p><p>对于奆佬，可以按照教程的方式自行构造数据生成器，如果懒得折腾我这里分享一些我的<a href="https://bhpan.buaa.edu.cn/link/AA9C30ED17AB7F415BB0C9C1D86D6894EA">测试数据</a></p><p>测试样例来自于我魔改的学长的数据生成器，同时修复了一些源代码，让生成的数据更合理，冲突更多，测试数据中有<code>j</code>指令，自己加一下，很容易</p><p><strong>Mars配置在前面的文章已经说过了，这里别忘了开延时槽噢</strong><span class="github-emoji" alias="grin" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png?v8">&#x1f601;</span></p><p>同时数据较多，建议搭配评测机使用，<strong>每年都有奆佬发布评测机，大家多多关注</strong>，这里也贴一个<a href="https://bhpan.buaa.edu.cn/link/AA5D9DC67428B04FD9AB80ED843D195B66"><code>lcy</code>同学写的评测机</a>，非常好用，再次感谢<span class="github-emoji" alias="heart" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2764.png?v8">&#x2764;</span>。相关的源码我已经修改好了，大家<strong>只需要修改</strong><code>config.json</code>中的<code>CPU</code> 和<code>MIPS</code>的路径即可，分别对应要测试的CPU文件夹，以及测试数据文件夹，测试结果在tmp文件夹下，我修改为了有不同才会生成.html文件，可以按需求修改源码，然后运行main.py即可（如果刚开始无输出，等一会儿或者多试几次就好了，神奇<span class="github-emoji" alias="laughing" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8">&#x1f606;</span>)</p>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>Verilog</tag>
      
      <tag>CPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P4课上</title>
    <link href="/2024/11/11/P4%E8%AF%BE%E4%B8%8A/"/>
    <url>/2024/11/11/P4%E8%AF%BE%E4%B8%8A/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>通过阅读本文，你可以大致了解BUAA 2024秋季 P4课上测试的相关内容<br>题目每年都会发生变化，题面仅靠回忆，题意描述可能与原题有一定差异</p>          </div><h1 id="T1-SWMM"><a href="#T1-SWMM" class="headerlink" title="T1 SWMM"></a>T1 SWMM</h1><h2 id="题面"><a href="#题面" class="headerlink" title="题面"></a>题面</h2><p><strong>swmm rt, offset(base)</strong></p><p><code>addr = offset + base</code></p><p>如果<code>GRF[$rt]</code> 为偶数，则将<code>GRF[$rt]</code> 存到<code>Memory[ addr[31:2] | 00 ]</code>,否则存到<code>Memory[ addr[29:0] || 00 ]</code></p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>其实和sw大差不差，但是需要<strong>增加DM写入地址的选择器</strong>，对于我的设计，我每一条新指令都会有一个<strong>新信号newSign</strong>，当且仅当执行该指令时，该信号为1，以便于将新指令与原来的指令分隔开，互不影响，具体见<strong>P3课上易错点中的实现</strong></p><p>对于本题，只需要在顶层模块加一个<code>MUX</code>，直接<code>assign</code> <code>DM_WA</code> &#x3D; <code>(newSign == 1&#39;b1)</code> ? <strong>(</strong><code>GRF_RD2[0] == 1&#39;b1</code> ? <code>&#123; AluOut[29:0], 2&#39;b00&#125;</code> : <code>&#123; AluOut[31:2], 2&#39;b00&#125;</code><strong>)</strong> : <code>AluOut</code></p><p>这样就不会与原来的指令冲突</p><h1 id="T2-ES"><a href="#T2-ES" class="headerlink" title="T2 ES"></a>T2 ES</h1><h2 id="题面-1"><a href="#题面-1" class="headerlink" title="题面"></a>题面</h2><p><strong>es rs, rt, Imm</strong></p><p><code>AluOut = GRF[$rs] - GRF[$rt]</code> </p><p><strong>beq的变体</strong>，减法如果不溢出，就将结果存到<code>$rt</code>,如果溢出了，<strong>就跟beq一样跳转</strong></p><h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p><strong>怎么判断溢出</strong>，就不赘述了，指令集第一条add中有详细描述，课上描述也易懂</p><p>我新增了一位ALU的输出信号<code>new_branch</code>，连接到<code>NPC模块</code>，当且仅当是该指令且满足跳转条件时置为1，NPC中只要该信号为1就跳转</p><p>同时，<strong>GRF的写使能也需要改变</strong>，跟T1差不多，newSign为0时，直接将Controller的RegWrite连过去，否则就判断<code>new_branch</code>,为0的话就可以写入，直接将减法结果写入即可。</p><p><strong>值得一提的是，必须先判断newSign，再判断new_branch</strong></p><p>我这么做有点麻烦，新增信号，连线不少，务必小心小心再仔细，我就因为两个低级错误调了好一会儿</p><h1 id="T3-IBS"><a href="#T3-IBS" class="headerlink" title="T3 IBS"></a>T3 IBS</h1><h2 id="题面-2"><a href="#题面-2" class="headerlink" title="题面"></a>题面</h2><p><strong>ibs rs, rt, rd</strong></p><p>如果<code>GRF[$rt]</code> 是对称的，即[0] &#x3D;&#x3D; [31],[1] &#x3D;&#x3D; [30]……那么就将<code>GRF[$rs]</code><strong>按位取反</strong>后写入<code>$rd</code>,否则直接将<code>GRF[$rs]</code>写入<code>$rt</code></p><p><strong>注意两种条件下写入的寄存器不同</strong></p><h2 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h2><p>新增一位运算，用<code>for循环</code>判断是否对称，根据判断结果决定ALU的输出</p><p>同时跟T2一样，<strong>新增一位ALU的输出信号</strong>，就叫ALU_new吧，<strong>当且仅当是该指令且满足对称条件时置为1</strong>，根据它来判断新指令条件下的寄存器写入地址，我是直接在原来<code>GRF_WA_MUX</code>后新增一个MUX，如果ALU_new为1，则选择$rd,其他情况为原来的MUX输出，这样改动不算太大</p><h1 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h1><p>这是第二次P4的课上测试，想看第一次的同学可以去其他的博客呀<span class="github-emoji" alias="smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">&#x1f604;</span></p><p>从这几周看来，第一次的P3,P4,P5挂的人可都不少啊，唉唉，失误机会总共有3次，冷静一周，放缓节奏，让自己轻松一下，也未尝是一件坏事。<br>本人因为P3挂了，这周就还比较轻松，因为不用搭P5，而有些朋友就很紧张了，极限一两天搭P5，还要面对一堆bug，课上可能一直在de课下的bug，因为课下的弱测实在是太弱了，导致很有可能遗留Bug到课上。而我这周呢，还和室友打了几次游戏，随便复习了一下数分，也终于有时间在操场上漫无目的地闲逛，真切体会到生活的乐趣。P5也有中测可以依赖，不用太过焦虑，也不用变成计组的形状了。</p><div class="note note-info">            <p>总的来看，挂一次缓缓节奏不一定是件坏事。🫠<br>下周就是P5了，加油，大家一起加油呀<span class="github-emoji" alias="grinning" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f600.png?v8">&#x1f600;</span> <span class="github-emoji" alias="grinning" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f600.png?v8">&#x1f600;</span> <span class="github-emoji" alias="grinning" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f600.png?v8">&#x1f600;</span></p>          </div>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>Verilog</tag>
      
      <tag>CPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P4课下</title>
    <link href="/2024/11/11/P4%E8%AF%BE%E4%B8%8B/"/>
    <url>/2024/11/11/P4%E8%AF%BE%E4%B8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>P4课下其实就只需要对着P3的电路翻译即可，具体的实现就不多说了，这里<strong>只讲讲一些细节和可优化的地方</strong></p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><h2 id="顶层模块"><a href="#顶层模块" class="headerlink" title="顶层模块"></a>顶层模块</h2><p>我们要尽量让顶层模块只起到连线的作用，最多有一些多路选择器，将具体的实现细节放到模块当中去</p><p>对于外部的连线，无论模块output的规格是reg还是wire，<strong>外部的接线都得定义为wire</strong>，或者你就将wire型变量想象为一根根具体的导线<span class="github-emoji" alias="grin" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png?v8">&#x1f601;</span></p><p><strong>同时一定要建立一套自己的命名规范，到了P5你就知道这有多么重要了</strong></p><h2 id="NPC"><a href="#NPC" class="headerlink" title="NPC"></a>NPC</h2><p>对于NPC模块的跳转判断，一定要做到不重不漏，比如说branch为1，但不相等时是否有赋值，贴上我的代码</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">assign</span> pc4 = pc + <span class="hljs-number">32&#x27;h0000_0004</span>;<br><span class="hljs-keyword">wire</span> [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>] tmp1, tmp2;<br><span class="hljs-keyword">assign</span> tmp1 = pc4 + &#123;&#123;<span class="hljs-number">14</span>&#123;offset_16[<span class="hljs-number">15</span>]&#125;&#125;, offset_16, <span class="hljs-number">2&#x27;b00</span>&#125;;<br><span class="hljs-keyword">assign</span> tmp2 = &#123;pc[<span class="hljs-number">31</span>:<span class="hljs-number">28</span>], Imm_26, <span class="hljs-number">2&#x27;b00</span>&#125;;<br><span class="hljs-keyword">always</span> @(*) <span class="hljs-keyword">begin</span><br>        <span class="hljs-keyword">if</span>(jump) <span class="hljs-keyword">begin</span><br>            <span class="hljs-keyword">if</span>(jSrc) <span class="hljs-keyword">begin</span><br>                npc = rs;<span class="hljs-comment">//jr跳转</span><br>            <span class="hljs-keyword">end</span> <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                npc = tmp2;<span class="hljs-comment">//j or jal跳转</span><br>            <span class="hljs-keyword">end</span><br>        <span class="hljs-keyword">end</span> <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>            <span class="hljs-keyword">if</span>(branch) <span class="hljs-keyword">begin</span><br>                <span class="hljs-keyword">if</span>(zero) <span class="hljs-keyword">begin</span><br>                    npc = tmp1;<span class="hljs-comment">//beq跳转</span><br>                <span class="hljs-keyword">end</span><br>                <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                    npc = pc4;<span class="hljs-comment">//beq不跳转</span><br>                <span class="hljs-keyword">end</span><br>            <span class="hljs-keyword">end</span> <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                npc = pc4;<span class="hljs-comment">//非跳转指令</span><br>            <span class="hljs-keyword">end</span><br>        <span class="hljs-keyword">end</span><br>    <span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>也可以用assign实现</p><h2 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h2><p>教程中有两种实现方法，我采取了比较直观但是码量更大的方式，如下</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">case</span> (opcode)<br>            <span class="hljs-number">6&#x27;b000000</span>:<br>            <span class="hljs-keyword">begin</span><br>                <span class="hljs-keyword">case</span> (funct)<br>                    addFunct: <span class="hljs-keyword">begin</span><br>                        RegWrite = <span class="hljs-number">1&#x27;b1</span>;<br>                        DmWrite = <span class="hljs-number">1&#x27;b0</span>;<br>                        RegAddSrc = <span class="hljs-number">2&#x27;b01</span>;<br>                        RegWDSrc = <span class="hljs-number">2&#x27;b00</span>;<br>                        AluOp = <span class="hljs-number">4&#x27;b0000</span>;<br>                        AluSrc = <span class="hljs-number">1&#x27;b0</span>;<br>                        SignExt = <span class="hljs-number">1&#x27;b0</span>;<br>                        branch = <span class="hljs-number">1&#x27;b0</span>;<br>                        IfJump = <span class="hljs-number">1&#x27;b0</span>;<br>                    <span class="hljs-keyword">end</span> <br>                   ......<br></code></pre></td></tr></table></figure><p>这个就看个人选择了</p><h2 id="P4课上准备"><a href="#P4课上准备" class="headerlink" title="P4课上准备"></a>P4课上准备</h2><h3 id="新指令信号"><a href="#新指令信号" class="headerlink" title="新指令信号"></a>新指令信号</h3><p>对与新的指令，可以预留一个信号的接口，提前在Controller模块里准备一个新信号<code>NewSign</code>，具体实现看以下代码</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">parameter</span>   addFunct = <span class="hljs-number">6&#x27;b100000</span>,<br>              subFunct = <span class="hljs-number">6&#x27;b100010</span>,<br>              <span class="hljs-comment">//I</span><br>              oriOp    = <span class="hljs-number">6&#x27;b001101</span>,<br>              luiOp    = <span class="hljs-number">6&#x27;b001111</span>,<br>              lwOp     = <span class="hljs-number">6&#x27;b100011</span>,<br>              swOp     = <span class="hljs-number">6&#x27;b101011</span>,<br>              beqOp    = <span class="hljs-number">6&#x27;b000100</span>,<br>              <span class="hljs-comment">//J</span><br>              jOp      = <span class="hljs-number">6&#x27;b000010</span>,<br>              jrFunct  = <span class="hljs-number">6&#x27;b001000</span>,<br>              jalOp    = <span class="hljs-number">6&#x27;b000011</span>,<br>              <span class="hljs-comment">//new</span><br>              newOp    = <span class="hljs-number">6&#x27;b111111</span>,<br>              newFunct = <span class="hljs-number">6&#x27;b111111</span>;<br>    <br>    <span class="hljs-keyword">always</span> @(*) <span class="hljs-keyword">begin</span><br>        NewSign = <span class="hljs-number">1&#x27;b0</span>;<br>        <span class="hljs-keyword">case</span> (opcode)<br>            <span class="hljs-number">6&#x27;b000000</span>:<br>            <span class="hljs-keyword">begin</span><br>                <span class="hljs-keyword">case</span> (funct)<br>                    addFunct: <span class="hljs-keyword">begin</span><br>                        RegWrite = <span class="hljs-number">1&#x27;b1</span>;<br>                        DmWrite = <span class="hljs-number">1&#x27;b0</span>;<br>                        RegAddSrc = <span class="hljs-number">2&#x27;b01</span>;<br>                        RegWDSrc = <span class="hljs-number">2&#x27;b00</span>;<br>                        AluOp = <span class="hljs-number">4&#x27;b0000</span>;<br>                        AluSrc = <span class="hljs-number">1&#x27;b0</span>;<br>                        SignExt = <span class="hljs-number">1&#x27;b0</span>;<br>                        branch = <span class="hljs-number">1&#x27;b0</span>;<br>                        IfJump = <span class="hljs-number">1&#x27;b0</span>;<br>                    <span class="hljs-keyword">end</span> <br>                    <br>                    newFunct: <span class="hljs-keyword">begin</span><br>                        RegWrite = <span class="hljs-number">1&#x27;b0</span>;<br>                        DmWrite = <span class="hljs-number">1&#x27;b0</span>;<br>                        RegAddSrc = <span class="hljs-number">2&#x27;b00</span>;<br>                        RegWDSrc = <span class="hljs-number">2&#x27;b00</span>;<br>                        AluOp = <span class="hljs-number">4&#x27;b0000</span>;<br>                        AluSrc = <span class="hljs-number">1&#x27;b0</span>;<br>                        SignExt = <span class="hljs-number">1&#x27;b0</span>;<br>                        branch = <span class="hljs-number">1&#x27;b0</span>;<br>                        IfJump = <span class="hljs-number">1&#x27;b0</span>;<br>                        JumpSrc = <span class="hljs-number">1&#x27;b0</span>;<br>                        NewSign = <span class="hljs-number">1&#x27;b1</span>;<br>                    <span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p><strong>课上只需要改newOp以及对应的控制信号即可</strong>，同时每次case前<code>NewSign = 1&#39;b0</code>（18行）这样就不用每一条其他指令都赋值了，同时顶层模块也提前连接好这个信号</p><h3 id="ALU模块"><a href="#ALU模块" class="headerlink" title="ALU模块"></a>ALU模块</h3><ul><li>考虑到有些题目可能涉及PC，比如与PC比较之类的，可以提前将PC接到ALU输入，要用时直接取</li><li>有些新指令可能两种情况的行为完全不同，<strong>这时无法从Controller直接获得我们想要的控制信号的最终取值</strong>，需要根据ALU运算结果来判断，可以提前预留一个新的<code>output</code>输出new，后面根据new信号来控制一些模块。例子见P4课上T2，一种情况只需要写入寄存器，另一种情况只需要跳转，完全不同。</li><li>可以提前将上一点的new信号连接到可能的模块，如<code>GRF、NPC</code>，先不管，到时候特判即可</li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p><strong>做课下时可以提前看看课上加指令的模式，提前准备一些必要的接口和连线，到时候课上只需要稍微修改，加几行代码即可</strong></p><h1 id="课下测试"><a href="#课下测试" class="headerlink" title="课下测试"></a>课下测试</h1><div class="note note-info">            <p><strong>每年都有会奆佬在讨论区或者水群发评测机和测试数据生成器，大家多多关注</strong></p>          </div><p>如果没什么数据的话可以试试下面这个数据，用的是学长的魔改MARS，<a href="https://github.com/Toby-Shi-cloud/Mars-with-BUAA-CO-extension">链接在这里</a></p><p>可以直接输出GRF\DM的写入信息，与自己的对拍即可，注意在Settings下勾选<code>Ignore Arithmetic Overflow</code>以及<code>Output Log level 1</code>,并在<code>Memory Configuration</code>下选择<code>large text</code>，这样可以一次运行4000多行<code>mips</code>代码</p><p>同时<code>testbench</code>时钟周期要减小，我减小到了<code>always #0.15 clk = ~clk;</code></p><p>测试数据如下（没有对<code>jr跳转31号以外寄存器</code>的情况进行测试）</p><hr><div class="note note-info">            <p>Update: 2025.11.12</p><p>今天有同学跟我反映数据太长直接复制会导致浏览器卡死，哈哈哈确实，没考虑到这个DOM相关的问题，我把数据上传到这里了，<strong>请大家有需要点击下载</strong>，<a href="https://pzhwuhu.github.io/Notes/CO_P4_testfile.txt">链接在此</a>，如果对你有帮助，可以在评论区say thanks (doge)</p>          </div><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs routeros">lui <span class="hljs-variable">$23</span>, 58778<br>sw <span class="hljs-variable">$11</span>, 3004(<span class="hljs-variable">$0</span>)<br>lui <span class="hljs-variable">$8</span>, 36882<br>sub <span class="hljs-variable">$16</span>, <span class="hljs-variable">$0</span>, <span class="hljs-variable">$18</span><br>nop<br>ori <span class="hljs-variable">$2</span>, <span class="hljs-variable">$25</span>, 39090<br><span class="hljs-built_in">add</span> <span class="hljs-variable">$6</span>, <span class="hljs-variable">$24</span>, <span class="hljs-variable">$17</span><br>beq <span class="hljs-variable">$11</span>, <span class="hljs-variable">$15</span>, label_508<br>label_508_back:<br>j label_561<br>label_561_back:<br>ori <span class="hljs-variable">$14</span>, <span class="hljs-variable">$0</span>, 1110<br>j label_337<br>label_337_back:<br><span class="hljs-built_in">add</span> <span class="hljs-variable">$18</span>, <span class="hljs-variable">$2</span>, <span class="hljs-variable">$3</span><br>nop<br>lw <span class="hljs-variable">$19</span>, 8808(<span class="hljs-variable">$0</span>)<br>ori <span class="hljs-variable">$15</span>, <span class="hljs-variable">$0</span>, 56195<br>j label_678<br>label_678_back:<br>lui <span class="hljs-variable">$14</span>, 42089<br>nop<br>sw <span class="hljs-variable">$7</span>, 11968(<span class="hljs-variable">$0</span>)<br>lui <span class="hljs-variable">$21</span>, 5507<br>lui <span class="hljs-variable">$15</span>, 5025<br>sub <span class="hljs-variable">$15</span>, <span class="hljs-variable">$22</span>, <span class="hljs-variable">$23</span><br>sub <span class="hljs-variable">$25</span>, <span class="hljs-variable">$15</span>, <span class="hljs-variable">$17</span><br>nop<br>jal label_852<br>label_852_back:<br>nop<br>j label_422<br>label_422_back:<br>nop<br>sw <span class="hljs-variable">$8</span>, 5060(<span class="hljs-variable">$0</span>)<br>lui <span class="hljs-variable">$16</span>, 896<br>lui <span class="hljs-variable">$17</span>, 49975<br><span class="hljs-built_in">add</span> <span class="hljs-variable">$11</span>, <span class="hljs-variable">$5</span>, <span class="hljs-variable">$21</span><br><span class="hljs-built_in">add</span> <span class="hljs-variable">$8</span>, <span class="hljs-variable">$24</span>, <span class="hljs-variable">$23</span><br>sw <span class="hljs-variable">$8</span>, 564(<span class="hljs-variable">$0</span>)<br>sub <span class="hljs-variable">$5</span>, <span class="hljs-variable">$8</span>, <span class="hljs-variable">$23</span><br>sw <span class="hljs-variable">$3</span>, 10960(<span class="hljs-variable">$0</span>)<br>j label_672<br>label_672_back:<br>ori <span class="hljs-variable">$15</span>, <span class="hljs-variable">$10</span>, 5713<br>ori <span class="hljs-variable">$13</span>, <span class="hljs-variable">$6</span>, 53986<br>lui <span class="hljs-variable">$15</span>, 50874<br>sub <span class="hljs-variable">$11</span>, <span class="hljs-variable">$25</span>, <span class="hljs-variable">$7</span><br>lui <span class="hljs-variable">$17</span>, 34811<br>nop<br>sub <span class="hljs-variable">$19</span>, <span class="hljs-variable">$23</span>, <span class="hljs-variable">$18</span><br>jal label_232<br>label_232_back:<br>nop<br>sub <span class="hljs-variable">$11</span>, <span class="hljs-variable">$15</span>, <span class="hljs-variable">$23</span><br><span class="hljs-built_in">add</span> <span class="hljs-variable">$17</span>, <span class="hljs-variable">$21</span>, <span class="hljs-variable">$5</span><br>jal label_281<br>label_281_back:<br>lui <span class="hljs-variable">$21</span>, 8699<br>ori <span class="hljs-variable">$12</span>, <span class="hljs-variable">$16</span>, 59274<br><span class="hljs-built_in">add</span> <span class="hljs-variable">$13</span>, <span class="hljs-variable">$13</span>, <span class="hljs-variable">$10</span><br>nop<br>nop<br>========================<br>========================<br>========================<br></code></pre></td></tr></table></figure><ul><li><strong>后面的省略了，请下载源文件查看</strong></li></ul>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>CPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BUAA-OOpre课程总结</title>
    <link href="/2024/11/09/OOpre%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    <url>/2024/11/09/OOpre%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>我太爱OOP啦，非常不错的一次体验，好想继续迭代，但是计组让我喘不过气来捏 <span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span><span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span><span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span><br><img src="/../img_post/oop.png"></p><h1 id="最终架构"><a href="#最终架构" class="headerlink" title="最终架构"></a>最终架构</h1><p><img src="/../img_post/oop1.png"></p><p>我的整个架构中<strong>类</strong>的个数并不多，仅在前几次迭代给<code>Bottle</code>和<code>Equipment</code> 增加了几个子类，并没有对战斗、副本等操作增加新的类或者接口，而是全部集成在了<code>Adventure</code>类中,可能实现上简单，但是这样导致我的<code>Adventure</code>类过于臃肿，其下的子方法过多</p><p><img src="/../img_post/oop2.png"></p><p>这样<strong>不利于调试和增量开发以及类和方法的复用，</strong>归根结底还是有点懒，懒得折腾，直接在<code>Adventure</code>类下写一个方法得了🫠</p><p><strong>以后还是要多多改进</strong></p><h2 id="迭代调整"><a href="#迭代调整" class="headerlink" title="迭代调整"></a>迭代调整</h2><p>给<code>Mainclass</code>中的每一种操作增加对应的case方法，传相应的参数进去。这样使得整体结构较为清晰，<del>也不会超过方法的行数</del></p><p><img src="/../img_post/oop3.png"></p><h1 id="使用Junit的心得体会"><a href="#使用Junit的心得体会" class="headerlink" title="使用Junit的心得体会"></a>使用Junit的心得体会</h1><ul><li>使用<code>Junit</code>可以很方便快捷地进行单元测试，从而知道一个方法是否正确执行，这样可以让我们在整体运行测试之前，先对各个单元进行充分测试，很有效地降低了测试的难度。</li><li>同一类方法可以copy一些语句，比如测试<code>Bottle</code>类的许多方法，都需要我们先实例化一个饼子，这时候就可以<strong>写一个通用的模版</strong>，从而节省时间</li><li>不同的类测试有先后顺序，应该<strong>先测试</strong>基本的独立的方法，再测试需要调用该方法的方法。</li></ul><h1 id="OOpre心得体会"><a href="#OOpre心得体会" class="headerlink" title="OOpre心得体会"></a>OOpre心得体会</h1><p>在本课程中，我经历了从面向过程编程（POP）到面向对象编程（OOP）的转变。这一转变让我对编程有了更深的理解。</p><ul><li>从前我们写C语言程序，通常会注重过程，也就是一系列按顺序执行的步骤。程序通常是围绕“做什么、怎么做”来设计的，主要通过函数来组织代码</li><li>但是OOP要求我们把<strong>问题抽象成具有属性和行为（方法）的对象，并让这些对象之间进行交互</strong>。这种方式更加贴近我们对现实世界的认知，我们所要解决的问题也更加切合实际，比如指导书中的宠物店、迭代中的类似原神的冒险者游戏，都很贴近我们的生活</li><li>从面向过程转向面向对象时，<strong>最大的挑战是要学会将问题抽象为一个模型</strong>，而不是简单地按顺序编写代码。（<em>本课程中这一点指导书已经帮我们完成了</em>）这种抽象能力的提升可以帮助我们更好地组织代码的结构，有一个全局的掌握，并提高代码的复用性和可扩展性</li><li>面向对象编程真正让我感受到<strong>“模块化”的威力</strong>。当项目规模变大时，面向对象的设计方式使得整个系统更容易理解和扩展，同时，由于<strong>封装性的提高</strong>，对象内部的修改对外部代码影响较小，代码的可维护性也大大提升</li></ul><h1 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h1><ul><li>希望能够指导书的知识讲解与题目分隔开，从而更加清晰</li><li>指导书中，对于一些比较绕的文字表述，希望能够提供一些例子来具体说明，有时候纯文字确实有点抽象</li></ul>]]></content>
    
    
    <categories>
      
      <category>OO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>OOpre</tag>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P3课上易错点</title>
    <link href="/2024/11/04/P3%E8%AF%BE%E4%B8%8A%E6%8A%80%E5%B7%A7/"/>
    <url>/2024/11/04/P3%E8%AF%BE%E4%B8%8A%E6%8A%80%E5%B7%A7/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>通过阅读本文，你可以大致了解BUAA 2024秋季 P3课上添加指令的常见坑点和小技巧</p>          </div><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>做了两次P3，这次较为顺利，40min就整完了，从上周挂掉P3到现在，本人发现了不少大家共性的问题或者叫易错点，也有一些个人的经验与技巧，写在这里，希望下一届 的朋友们能够更加顺利地通过P3😁😁(●’◡’●)</p><p>本人课下重构版如下：<img src="/../img_post/p3_4.png"></p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><h2 id="易错点总结"><a href="#易错点总结" class="headerlink" title="易错点总结"></a>易错点总结</h2><p>其实大家添加指令或许并不成问题，但是可能会出现哪些问题呢？包括但不限于：</p><ul><li>1.输出信号值随意更改</li><li>2.添加指令影响了已有指令</li><li>3.没有预留充足空间和信号端口，导致添加指令麻烦、耗时长，也就容易产生问题</li><li>4.课下充分测试，裸交一发，对了几个点的话就不要怀疑课下的bug了</li></ul><p>针对前三个问题，我分别给出错误示范和改进方法</p><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>参考我之前课上的设计，<img src="/../img_post/p3_0.png"></p><p><code>Reg_Addr</code>是直接连接到输出信号的，表示寄存器写入地址，如果你在端口和这个Tunnel之间对信号做了处理，那会造成本地运行正确，但是交上去输出错误，且debug很困难</p><ul><li><strong>解决</strong>：就是将这种输出信号直接接在端口上，像这样,这样，你在更改信号，比如GRF_WA时，就直接在Tunnel前面改，如图红色箭头处，加MUX之类的</li></ul><p><img src="/../img_post/p3_6.png"></p><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>这也是一个很常见的问题，我以第二次课上的一个题为例，来说一说</p><p>第二次P3课上第一题为BA指令，RTL语言如下：</p><p><code>ba rs, rt, offset</code></p><p><code>ans &lt;- GRF(rs) ^ GRF(rt) </code>(按位异或)</p><p>if <code>ans &lt; 0x10000</code> then</p><p>​<code>PC &lt;- PC +4 + sign_extend(ans[15:0] || 00)</code></p><p>else </p><p>​<code>PC &lt;- PC +4 + sign_extend(offset[15:0] || 00)</code></p><p>这其实就是<code>beq</code>指令的变体，对于<code>beq</code>指令，我是branch信号置1同时判断ALU相减结果是否为0来决定是否跳转，具体的跳转在NPC模块实现,对于此题发现只要该指令执行，就一定会跳转，同时ALU模块需要执行异或运算，也无法直接用zero信号，所以我<strong>选择了新输出一个信号，就叫new</strong>（事实上提前预留了端口，见Controller模块），只要该指令，则new为1，把new与之前的最终跳转信号或一下，连到NPC模块，即可</p><p>接着看看跳转地址，可以直接在NPC输入端口前加一个MUX，根据判断条件选一个进去，就是这样![](..&#x2F;img_post&#x2F;p3_7 (2).png)</p><p><strong>看起来似乎没啥问题，但这样是错的！！！</strong>，因为你把之前正常跳转<code>beq</code>的跳转地址也改了，回头想一想，我们<strong>只需要在执行这条新指令时这样选择，对于其他的指令，用之前的Tunnel就好了</strong>，想到这里后，实现就很简单了，<strong>对于每一条新指令，如果需要改变之前输入输出的端口，则加一个MUX，确保只对新指令作用，同时新增一个输出信号作为MUX的选择信号,由于新信号new只在新信号时为1，这样就保证了原来指令的正确性</strong><br>![](..&#x2F;img_post&#x2F;p3_7 (1).png)</p><h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p>我之前是这样设计的，课上修改数据通路，把我累的够呛，特别是在一些很小的地方加一些MUX，那简直了(┬┬﹏┬┬)，整个电路连线很复杂，也容易犯错<br><img src="/../img_post/p3_5.png"></p><p>因此，我对电路进行了重构，增加了很多的Tunnel，也扩大了空间，这样增添通路时更加方便，同时Controller预留了一个新的信号，到时候把指令在里面连起来，直接就能用,看起来也更加简洁，可能没那么直观只是。</p><p><img src="/../img_post/p3_4.png"></p><h1 id="碎碎念"><a href="#碎碎念" class="headerlink" title="碎碎念"></a>碎碎念</h1><p>哈哈哈，问答时助教还问我CPU画的怎么这么好看，嘻嘻😎😎😆😆，<code>Logisim</code>可以直接编辑子电路外观、形状、颜色，大家自行搜索学习即可，确实好看很多<del>颜狗来了</del></p><p>P3挂了，唉唉，但回头一想，也没啥，今晚P4似乎难度不小，感觉很多人没出来，如果我今晚做P4，说不定也挂，下次兴许能简单点！？（<del>未经</del>），不知道，全面准备吧。</p><p>最后送大家一句话：</p><p><strong>请不要相信胜利就像山坡上的蒲公英一样唾手可得，但请相信，世上总有美好值得自己全力以赴，哪怕粉身碎骨</strong></p><p>😊😊😊</p>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>Logisim</tag>
      
      <tag>CPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P3课下</title>
    <link href="/2024/11/04/P3%E8%AF%BE%E4%B8%8B/"/>
    <url>/2024/11/04/P3%E8%AF%BE%E4%B8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="CPU设计部分"><a href="#CPU设计部分" class="headerlink" title="CPU设计部分"></a>CPU设计部分</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本次笔者用<code>logisim</code>搭建了一个单周期<code>CPU</code>，能够处理9条指令，分别为<code>add</code> <code>sub</code> <code>ori</code> <code>lw</code> <code>sw</code> <code>beq</code> <code>lui</code> <code>nop</code>，采用模块化和层次化设计，让结构更加清晰明了。所设计的模块如下：</p><ul><li><code>IFU</code> 取指令单元，通过<code>PC</code>的值来确定当前要执行的指令</li><li><code>GRF</code> 通用寄存器组，通过该模块实现对32个通用寄存器的读和写</li><li><code>ALU</code> 算术逻辑运算单元</li><li><code>DM</code> 数据存储器，通过<code>RAM</code>实现</li><li><code>NPC</code> 计算Next_PC</li><li><code>Controller</code> 控制器，解析<code>funct</code> 和<code>op</code> 从而将相应的控制信号置0、1</li><li>最终效果如图：</li></ul><p><img src="/../img_post/p3_4.png"></p><h2 id="数据通路部件"><a href="#数据通路部件" class="headerlink" title="数据通路部件"></a>数据通路部件</h2><h3 id="IFU：指令单元"><a href="#IFU：指令单元" class="headerlink" title="IFU：指令单元"></a>IFU：指令单元</h3><p>该模块包含<code>PC</code> 和 <code>IM</code>两部分，分别为程序计数器和指令存储器。</p><ul><li>PC用寄存器直接实现，应具有异步复位功能，复位值为起始地址，地址范围为0x00003000 ~ 0x00006FFF</li><li>IM用<code>ROM</code>实现，容量为 <strong>4096 * 32 bit</strong>. <code>ROM</code>的0位置对应 <code>PC</code> 为 0x00003000的指令</li><li>不难发现 ROM 实际地址宽度仅需 12 位，这是因为每一条程序占32位，即4byte，每次地址改变是以4byte为最小单位进行的</li></ul><table><thead><tr><th>信号名</th><th>方向</th><th>位宽</th><th>说明</th></tr></thead><tbody><tr><td><code>clk</code></td><td>I</td><td>1</td><td>时钟信号</td></tr><tr><td><code>reset</code></td><td>I</td><td>1</td><td>异步复位信号</td></tr><tr><td><code>Next_PC</code></td><td>I</td><td>32</td><td>下一条指令的地址</td></tr><tr><td><code>Instr</code></td><td>O</td><td>32</td><td>正在执行的指令的编码</td></tr><tr><td><code>PC</code></td><td>O</td><td>32</td><td>当前执行指令的地址</td></tr></tbody></table><h3 id="GRF-通用寄存器堆"><a href="#GRF-通用寄存器堆" class="headerlink" title="GRF:通用寄存器堆"></a>GRF:通用寄存器堆</h3><p>该模块P1课下已经实现了，直接用即可，copy copy ！（DMX记得开三态噢！<span class="github-emoji" alias="sweat_smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8">&#x1f605;</span>)</p><table><thead><tr><th>信号名</th><th>方向</th><th>位宽</th><th>说明</th></tr></thead><tbody><tr><td><code>RA1</code></td><td>I</td><td>5</td><td>1_读取的寄存器编号</td></tr><tr><td><code>RA2</code></td><td>I</td><td>5</td><td>2_读取的寄存器编号</td></tr><tr><td><code>WE</code></td><td>I</td><td>1</td><td>寄存器写入使能端</td></tr><tr><td><code>WA</code></td><td>I</td><td>5</td><td>写入的寄存器编号</td></tr><tr><td><code>WD</code></td><td>I</td><td>32</td><td>写入寄存器的数据</td></tr><tr><td><code>reset</code></td><td>I</td><td>1</td><td>异步复位</td></tr><tr><td><code>clk</code></td><td>I</td><td>1</td><td>时钟信号</td></tr><tr><td><code>RD1</code></td><td>O</td><td>32</td><td>1_读取的寄存器数据</td></tr><tr><td><code>RD2</code></td><td>O</td><td>32</td><td>2_读取的寄存器数据</td></tr></tbody></table><h3 id="ALU-算术逻辑运算单元"><a href="#ALU-算术逻辑运算单元" class="headerlink" title="ALU:算术逻辑运算单元"></a>ALU:算术逻辑运算单元</h3><h4 id="端口定义"><a href="#端口定义" class="headerlink" title="端口定义"></a>端口定义</h4><table><thead><tr><th>信号名</th><th>方向</th><th>位宽</th><th>说明</th></tr></thead><tbody><tr><td><code>inA</code></td><td>I</td><td>32</td><td>操作数1</td></tr><tr><td><code>inB</code></td><td>I</td><td>32</td><td>操作数2</td></tr><tr><td><code>AluCtrol</code></td><td>I</td><td>4</td><td>运算类型</td></tr><tr><td><code>result</code></td><td>O</td><td>32</td><td>运算结果</td></tr><tr><td><code>zero</code></td><td>O</td><td>1</td><td>结果是否为0</td></tr></tbody></table><h4 id="运算编码"><a href="#运算编码" class="headerlink" title="运算编码"></a>运算编码</h4><table><thead><tr><th>编码</th><th>功能</th></tr></thead><tbody><tr><td>0000</td><td>A + B</td></tr><tr><td>0001</td><td>A - B</td></tr><tr><td>0010</td><td>A &amp; B</td></tr><tr><td>0011</td><td>A or B</td></tr><tr><td>0100</td><td>加载立即数至高位</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="DM：数据存储器"><a href="#DM：数据存储器" class="headerlink" title="DM：数据存储器"></a>DM：数据存储器</h3><ul><li>使用<code>RAM</code>实现，容量为 <strong>3072 * 32 bit</strong></li><li>使用异步复位功能，复位值为0</li><li><code>RAM</code>应使用双端口模式</li></ul><table><thead><tr><th>信号名</th><th>方向</th><th>位宽</th><th>说明</th></tr></thead><tbody><tr><td><code>clk</code></td><td>I</td><td>1</td><td>时钟信号</td></tr><tr><td><code>reset</code></td><td>I</td><td>1</td><td>异步复位信号</td></tr><tr><td><code>address</code></td><td>I</td><td>32</td><td>内存地址</td></tr><tr><td><code>WD</code></td><td>I</td><td>32</td><td>写入的数据</td></tr><tr><td><code>WE</code></td><td>I</td><td>1</td><td>写入使能</td></tr><tr><td><code>RD</code></td><td>O</td><td>32</td><td>读出的数据</td></tr></tbody></table><h3 id="EXT：扩展单元"><a href="#EXT：扩展单元" class="headerlink" title="EXT：扩展单元"></a>EXT：扩展单元</h3><p>直接使用<code>logisim</code>自带的<code>extender</code>即可，但是注意既有符号扩展又有0扩展（such as ori)</p><table><thead><tr><th>信号名</th><th>方向</th><th>位宽</th><th>说明</th></tr></thead><tbody><tr><td><code>in</code></td><td>I</td><td>16</td><td>待扩展的信号</td></tr><tr><td><code>ExtOp</code></td><td>I</td><td>1</td><td>扩展方式选择信号（为0则0扩展）</td></tr><tr><td><code>Out</code></td><td>O</td><td>32</td><td>扩展的结果</td></tr></tbody></table><h3 id="NPC：指令地址单元"><a href="#NPC：指令地址单元" class="headerlink" title="NPC：指令地址单元"></a>NPC：指令地址单元</h3><p><code>Next_PC</code>一共有 3 种情况，分别为 PC + 4、PC + 4 + <code>offest </code>&lt;&lt; 2 (<code>beq</code> 指令) 、<code>Imm</code>26 -&gt; <code>Imm</code>32(j 指令)，端口定义如下</p><table><thead><tr><th>信号名</th><th>方向</th><th>位宽</th><th>说明</th></tr></thead><tbody><tr><td><code>PC</code></td><td>I</td><td>32</td><td>当前指令地址</td></tr><tr><td><code>offest</code></td><td>I</td><td>16</td><td><code>beq</code>指令的相对偏移量</td></tr><tr><td><code>Imm</code></td><td>I</td><td>16</td><td>j指令的26位跳转地址</td></tr><tr><td><code>branch</code></td><td>I</td><td>1</td><td>是否分支（<code>beq</code>)</td></tr><tr><td><code>if_j</code></td><td>I</td><td>1</td><td>是否j跳转</td></tr><tr><td><code>Next_PC</code></td><td>O</td><td>32</td><td>下一条指令的地址</td></tr></tbody></table><h2 id="控制器设计"><a href="#控制器设计" class="headerlink" title="控制器设计"></a>控制器设计</h2><h3 id="与逻辑模块"><a href="#与逻辑模块" class="headerlink" title="与逻辑模块"></a>与逻辑模块</h3><p>先根据指令的<code>opcode</code> 和 <code>funct</code> 来确定具体的指令类型，见下表R指令类型</p><table><thead><tr><th>R型指令</th><th>Opcode</th><th>Rs</th><th>Rt</th><th>Rd</th><th>Shamt</th><th>Funct</th></tr></thead><tbody><tr><td>add rd rs rt</td><td>000000</td><td>rs</td><td>rt</td><td>rd</td><td>XXXXX</td><td>100000</td></tr><tr><td>sub rd rs rt</td><td>000000</td><td>rs</td><td>rt</td><td>rd</td><td>XXXXX</td><td>100010</td></tr></tbody></table><p>再来看看I型指令：</p><table><thead><tr><th>I型指令</th><th>Opcode</th><th>Rs</th><th>Rt</th><th>16 bits offest</th></tr></thead><tbody><tr><td>ori rt rs Imm</td><td>001101</td><td>rs</td><td>rt</td><td>Imm16</td></tr><tr><td>lui rt Imm</td><td>001111</td><td>rs</td><td>rt</td><td>Imm16</td></tr><tr><td>lw rt Imm(rs)</td><td>100011</td><td>rs</td><td>rt</td><td>Imm16</td></tr><tr><td>sw rt Imm(rs)</td><td>101011</td><td>rs</td><td>rt</td><td>Imm16</td></tr><tr><td>beq rs rt Imm</td><td>000100</td><td>rs</td><td>rt</td><td>Imm16</td></tr></tbody></table><p>J指令：</p><table><thead><tr><th>J型指令</th><th>Opcode</th><th>address</th></tr></thead><tbody><tr><td>J target</td><td>000010</td><td>target_26bit</td></tr><tr><td>Jr <code>$rs</code></td><td>000000(<code>funct:</code> 001000)</td><td>$rs</td></tr><tr><td>Jal target</td><td>000011</td><td>target_26bit &amp; $ra &#x3D; PC+4</td></tr></tbody></table><p>当然，也可以加入其他的指令来扩展功能，有了上表，我们就可以来设计与逻辑模块了……</p><h3 id="或逻辑模块"><a href="#或逻辑模块" class="headerlink" title="或逻辑模块"></a>或逻辑模块</h3><p>在这部分中，我们要根据与逻辑输出的具体指令来指定各个控制信号的高低电平，以此来实现对整个电路的控制。与此同时，这部分要求我们将各条指令的执行过程谙熟于心。</p><h4 id="总控信号说明"><a href="#总控信号说明" class="headerlink" title="总控信号说明"></a>总控信号说明</h4><table><thead><tr><th>信号</th><th>说明</th></tr></thead><tbody><tr><td><code>Reg_Write</code></td><td>寄存器文件写入使能端</td></tr><tr><td><code>DM_Write</code></td><td>内存区域写入使能端</td></tr><tr><td><code>Reg_Dst</code></td><td>决定被写入寄存器在指令中的位置，0为I型，1为R型</td></tr><tr><td><code>MemtoReg</code></td><td>决定写入寄存器的是Alu结果还是DM取出的数据（if 1 :DM)</td></tr><tr><td><code>Alu_Op</code></td><td>决定Alu运算方式，见Alu表</td></tr><tr><td><code>Alu_Src</code></td><td>决定Alu 第二个运算数为 寄存器还是立即数，0为寄存器</td></tr><tr><td><code>Ext_Ctrl</code></td><td>决定扩展方式，0表示0扩展，1为符号扩展</td></tr><tr><td><code>Branch</code></td><td>是否分支，1 as 分支</td></tr><tr><td><code>If_Jump</code></td><td>是否跳转，1 as 为跳转</td></tr></tbody></table><h4 id="指令对应的控制信号"><a href="#指令对应的控制信号" class="headerlink" title="指令对应的控制信号"></a>指令对应的控制信号</h4><table><thead><tr><th>指令</th><th><code>Reg_Write</code></th><th><code>DM_Write</code></th><th><code>Reg_Dst</code></th><th><code>MemtoReg</code></th><th><code>Alu_Op</code></th><th><code>Alu_Src</code></th><th><code>Ext_Ctrl</code></th><th><code>Branch</code></th><th><code>If_Jump</code></th></tr></thead><tbody><tr><td>add</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0000</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>sub</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0001</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>ori</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0011</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>lui</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0100</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>lw</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0000</td><td>1</td><td>1</td><td>0</td><td>0</td></tr><tr><td>sw</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0000</td><td>1</td><td>1</td><td>0</td><td>0</td></tr><tr><td>beq</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0001</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>J</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0000</td><td>x</td><td>0</td><td>0</td><td>1</td></tr><tr><td>Jr</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0101</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>Jal</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0000</td><td>1</td><td>0</td><td>0</td><td>1</td></tr></tbody></table><h1 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h1><ol><li><p>上面我们介绍了通过 FSM 理解单周期 CPU 的基本方法。请大家指出单周期 CPU 所用到的模块中，哪些发挥状态存储功能，哪些发挥状态转移功能。</p><ul><li>状态存储：IFU、DM、GRF</li><li>状态转移：NPC、Controller</li></ul></li><li><p>现在我们的模块中 IM 使用 ROM， DM 使用 RAM， GRF 使用 Register，这种做法合理吗？ 请给出分析，若有改进意见也请一并给出。</p><ul><li>合理，因为IM中的指令是只需要被读取解析的，使用ROM只有读取功能；而DM是数据存储器，需要读出和写入，因此需要使用RAM，同时不能用寄存器，因为我们需要的内存量非常的大，有限的Register无法满足；而GRF用寄存器堆实现则可以实现高速的读写，同时与ALU的输入相连，送操作数或者存储中间变量，使用寄存器快速读写较合理。</li></ul></li><li><p>在上述提示的模块之外，你是否在实际实现时设计了其他的模块？如果是的话，请给出介绍和设计的思路。</p><ul><li>我只增加了一个NPC模块，因为自行增加了J指令，PC的计算方式有 PC + 4, PC + 4 + offset&lt;&lt;2 , label，甚至如果加入jr指令的话，还会直接赋值为寄存器的值，考虑到后续扩展问题，引入NPC模块</li></ul></li><li><p>事实上，实现 <code>nop</code> 空指令，我们并不需要将它加入控制信号真值表，为什么？</p><ul><li>因为<code>nop</code>指令相当于<code>sll $0, $0, 0</code>,对应的操作码为0x0000_0000,因为0移动0位还是0，，并且0号寄存器不能写入，所以无需加入控制信号表。同时该指令有时被用于空循环，有时被编译器用于与体系结构相关的编译优化。</li></ul></li><li><p>阅读 Pre 的 <a href="http://cscore.buaa.edu.cn/tutorial/mips/mips-6/mips6-1/">“MIPS 指令集及汇编语言”</a> 一节中给出的测试样例，评价其强度（可从各个指令的覆盖情况，单一指令各种行为的覆盖情况等方面分析），并指出具体的不足之处。</p><ul><li>数据强度不够，没有涉及int边界附近的较大的数据的测试，没有覆盖边界情况</li><li>只测试了少数寄存器，并未对所有寄存器的读写进行测试，虽然有些寄存器实际上并不需要读写，如1号寄存器</li><li>跳转范围不够，没有测试向前跳转、本行跳转，也没有测试嵌套跳转、循环跳转等特殊情况</li></ul></li></ol><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><h2 id="必看"><a href="#必看" class="headerlink" title="必看"></a>必看</h2><div class="note note-info">            <p><strong>每年都有奆佬发布评测机，大家多多关注</strong><br>这里提供几个工具，对于懒得折腾以及没有科学上网工具的同学，直接点击下面的链接下载即可，简单的配置方法我会讲</p>          </div><p><a href="https://bhpan.buaa.edu.cn/link/AA9CBB652F65F647ECB6C5F5C44B4732E8">我是链接，快点我</a></p><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>一个是魔改MARS，能直接输出寄存器和内存的写入情况，与<code>$display</code>一致，来自<code>Toby-Shi</code>学长,<a href="https://github.com/Toby-Shi-cloud/Mars-with-BUAA-CO-extension">链接在这里</a></p><p>另一个是<code>logisim的debug工具</code>，一个是探针可以直接根据指令码显示当前的指令语言，另一个寄存器堆可以直接查看各个寄存器的值，也可以直接修改。</p><h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><h4 id="魔改MARS"><a href="#魔改MARS" class="headerlink" title="魔改MARS"></a>魔改MARS</h4><p><strong>注意</strong>在Settings下勾选<code>Ignore Arithmetic Overflow</code>以及<code>Output Log level 1</code>,并在<code>Memory Configuration</code>下选择<code>large text</code>，这样可以一次运行4000多行<code>mips</code>代码(如果用不到这么大的测试数据也可以选第二个)</p><h4 id="logisim-debug辅助工具"><a href="#logisim-debug辅助工具" class="headerlink" title="logisim debug辅助工具"></a>logisim debug辅助工具</h4><div class="note note-success">            <p>以下内容来源于讨论区助教帖子</p><p><strong>工具整合 &amp;&amp; 主题帖作者：徐俊豪助教</strong></p>          </div><p>下载工具包，然后在 Logisim 中点击 Project - Load Library - JAR Library，选择刚刚下载的 jar 包，如图</p><p><img src="/../img_post/p3_8.png"></p><div class="note note-info">            <p>然后在左侧可以看到这两个元件，其中寄存器堆 <code>RegisterFile</code> 效果如下，可以直接看到寄存器的值，也可以直接对寄存器的值进行编辑，同学们在调试的时候可以用它替换自己的寄存器堆来方便地观察寄存器状态</p>          </div><p>端口定义如下：</p><ul><li>RA1: 5 位读寄存器编号 1</li><li>RA2: 5 位读寄存器编号 2</li><li>WA: 5 位写寄存器编号</li><li>WD: 32 位写入值</li><li>WE: 1 位写使能</li><li>RD1: 32 位读出值 1</li><li>RD2: 32 位读出值 2</li><li>clk（图中未标出，三角形那个）：1 位时钟信号</li></ul><p><img src="/../img_post/p3_10.jpg"></p><div class="note note-info">            <p>Mips Probe 效果如下，可以直接将 32 位 MIPS 机器码翻译为汇编指令，支持本课程课下用到的全部指令及一些其它指令，同学们可以在调试时使用来查看机器码对应的汇编指令</p>          </div><p><img src="/../img_post/p3_9.jpg"></p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><div class="note note-danger">            <ul><li><strong>该工具的导入会导致测评错误！！！因此强烈建议大家将电路复制一份来导入上述库进行调试</strong></li><li><strong>课上不会提供该工具</strong></li></ul>          </div>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>CPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P3课上</title>
    <link href="/2024/11/01/P3%E8%AF%BE%E4%B8%8A/"/>
    <url>/2024/11/01/P3%E8%AF%BE%E4%B8%8A/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>通过阅读本文，你可以大致了解BUAA 2024秋季 P3课上测试的相关内容<br>题目每年都会发生变化，题面仅靠回忆，题意描述可能与原题有一定差异</p>          </div><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>唉唉，P3就挂了，当头一棒啊 <span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span> ＞﹏＜</p><p>刚过P4课下，想着写一篇博客来记录下挂P3的感受以及一些血与泪的经验教训，希望<code>generations to come</code> 不要重蹈覆辙呀 <span class="github-emoji" alias="sweat_smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8">&#x1f605;</span></p><p>不过确实P3课上有一定的压力，最后延长了半小时，不过本人倒是一题没过，菜到家了😭，想看课上心路历程（第一视角破防）的直接跳转后面即可</p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><h2 id="CWP"><a href="#CWP" class="headerlink" title="CWP"></a>CWP</h2><p>指令格式为CWP rs, rt, Imm_16</p><p>大致是说要将<code>Imm_16</code>符号扩展后左移两位与当前PC值进行比较，如果<code>Imm_16</code>大于PC，则将<strong>原来的</strong><code>Imm_16</code>存到rs,否则存到rt（也可能反过来，不太记得了）</p><p>让我们来思考本题中哪些地方需要更改，首先是ALU模块的第一个输入inA前加一个多路选择器，要把PC作为输入，同时GRF模块的写入地址和写入数据均需要加一个多路选择器。因为之前课下的指令均不涉及对rs寄存器的写入，所以写入地址前得加。整体思路没什么困难，但是如果你之前没有预留接口，以及Logiism电路很紧凑的话，你可能会改的比较吃力，别问我怎么知道的 QAQ</p><p>至于判断条件的话，可以在ALU里实现并且沿用之前beq的zero端口，当然对于本指令，需要在Controller模块新加一个输出信号。</p><h2 id="BGC"><a href="#BGC" class="headerlink" title="BGC"></a>BGC</h2><p>指令格式为BGC rs, rt, Imm_16</p><p>大致是说将GR[rs]和GRF[rt] 32位的每两位转换为对应的格雷码，并比较二者转换后的32位数中 1 的个数，如果一个大于另一个，则按照<code>offset_16</code>跳转</p><p>格雷码转换：00-0001-0110-1111-10</p><p>如果没什么思路可以设计一个2bit的格雷码转换程序，再暴力解决，最后别忘了<code>bit adder</code>这个元件，可以直接统计其中1的个数</p><p>当然，可以发现，格雷码转换其实就是：<strong>逻辑右移一位再与原数异或</strong>，其实直接真值表打表也挺容易</p><p>复用beq的跳转即可，<strong>注意对跳转信号的判断</strong></p><h2 id="LWSO"><a href="#LWSO" class="headerlink" title="LWSO"></a>LWSO</h2><p>比较复杂，我课上没想，，请移步其他大佬的博客 (❁´◡&#96;❁)</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>针对我课上出的问题以及课下来发现的问题及反思，总结了几点供后来的学弟学妹借鉴，希望你们不要重蹈覆辙了<span class="github-emoji" alias="sunglasses" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60e.png?v8">&#x1f60e;</span></p><ul><li>课下充分测试，与同学对拍，保证不留bug</li><li>预留接口给新的指令和新的信号，比如直接添加一个New信号在课下的Controller里面，但先不连接</li><li>模块间，端口处，留出适当的距离，不要像我一样加MUX时手忙脚乱，一个反面例子如下：<img src="/../img_post/p3_1.png"></li><li><strong>课程组要求输出的那些个信号对应的Tunnel，一定要检查是否是最终的正确信号，</strong>不要在Tunnel和端口间更改信号，这样会导致本地运行正确但输出错误，很难debug,因为课程组为了评测，会让你把相关信号输出出去，像这样：<img src="/../img_post/p3_2.png"></li><li>如果你像上面说的那样做了，会出问题，<strong>具体情况</strong>见我的心路历程-第二天部分debug发现的毛病，后来发现不少同学都有这个问题，希望后来人能够引以为戒，这都是学长学姐亲自踩的坑啊 ε&#x3D;(´ο｀*)))唉<br><strong>反面教材</strong>如下：<img src="/../img_post/p3_0.png"><br><strong>最后，如果你对我出的bug感兴趣并且想知道我课上有多难受的话，请接着往下看</strong></li></ul><h1 id="心路历程"><a href="#心路历程" class="headerlink" title="心路历程"></a>心路历程</h1><h2 id="课上"><a href="#课上" class="headerlink" title="课上"></a>课上</h2><p>你能知道我T1调了半天，本地各种测试各种没问题，加上去死活不对的感受嘛<span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span>麻烦的是，评测机并不会在出错的地方立马报错，而是会在后面几条指令的某一条报错，而我的则是一条lw指令。当时翻译出来人都蒙了，我课下测试了那么多数据，怎么能有问题呢？而且我裸交一发对了2个点，没问题呀，于是我返回去检查课上的通路，一通测试下来，找不出任何问题，各种数据都测完了，我已经筋疲力竭了，此时已快到8点，而我第一题还没过，，看看周围的同学，我知道，这次有点悬；于是我检查课下去了，检查一番又检查测试课上，在这样的反复中迷失了自我，又过了15min，这下我彻底慌神了，本地测试没有问题啊，交上去就是不对，能有什么办法呢？不知道有没有人能感受到我的绝望≡(▔﹏▔)≡</p><p>于是，我诚惶诚恐地打开了第二题，一看感觉直接将32位每两位转换为格雷码，好麻烦，也不想新建模块，于是开始冥思苦想，试图打开什么新思路，期间还瞄了一眼第三题，感觉很复杂，一看就不想做。</p><p>终于到了8点半，一事无成的我决定开始摆烂，思考人生，我趴在桌上，脑子想的什么呢？<span class="github-emoji" alias="thinking" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f914.png?v8">&#x1f914;</span>是从Pre以来的披荆斩棘吗？是P3课下测试对拍的欢乐吗？是校庆时在摊位上玩游戏的激情吗？还是来6系充实繁忙的每一天呢？我不知道，我的脑子一片空白。期间还看到zlr助教gg看了一眼我，又看了一眼平板，不知道他又对我有何感想呢？</p><p>时间很快来到了8.58，我一看，大部分人跟我一样，被硬控在了新北地下的机房里，只有少数寥寥的几个座位是空的，说实话，我的心里竟然感到一丝慰藉，还有这么多人也没过啊（buhsi)</p><p>但是，考试延迟了半小时，期间有一部分人做出来了走了，也有一些人像我一样留到了最后，九点半走出机房，看到同行的人也是垂头丧气，大家心里都很不是滋味<span class="github-emoji" alias="upside_down_face" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f643.png?v8">&#x1f643;</span></p><h2 id="第二天"><a href="#第二天" class="headerlink" title="第二天"></a>第二天</h2><p>回到寝室后，我望着课下的电路，脑子里一片茫然，一种无力感裹挟着我，让我说不出话来，期间看了紫玉哥哥的题解，也看了水群里大伙的讨论，还找亲爱的助教zjygg倾诉了一番，但心里依旧难过。</p><p>上床后，很快睡着了，第二天物理课，我和vach（室友）坐在一起，谈论着P3，才发现他也没过，偷笑</p><p><span class="github-emoji" alias="nerd_face" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f913.png?v8">&#x1f913;</span> ,我打开P3课下，准备重构一番，毕竟课上加通路加的难受，就在我给GRF的写入地址加一个标签的时候，我<strong>猛地意识到了自己的课上bug</strong>，见下图</p><p><img src="/../img_post/p3_0.png"></p><p>我课上是直接把多路选择器加在了红色位置，但是我的<strong>寄存器写入地址的<code>Tunnel</code>还是接在了原来的蓝色位置</strong>，也就导致了输出地址的问题。更恐怖的是，我课上debug是直接接了probe在GRF的WA前面，完全没有管Reg_Addr的输出问题，这也是我本地测试毫无问题，交上去报错奇奇怪怪的原因，哎<span class="github-emoji" alias="scream" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f631.png?v8">&#x1f631;</span></p><p>到这里，我真想穿越回去课上改掉这个bug,但时间无法倒流，哎。那个时刻，我真正知道了什么叫<strong>欲哭无泪</strong></p><p>中午去给空天杯篮球赛当志愿者，记分员，沉浸在了比赛里，忘掉了昨晚的悲痛，也感谢这个志愿活动，让我成功走了出来，<strong>忘掉烦恼，笑对生活</strong><span class="github-emoji" alias="smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">&#x1f604;</span></p>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>Logisim</tag>
      
      <tag>CPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P2课上</title>
    <link href="/2024/10/22/P2%E8%AF%BE%E4%B8%8A/"/>
    <url>/2024/10/22/P2%E8%AF%BE%E4%B8%8A/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>通过阅读本文，你可以大致了解BUAA 2024秋季 P2课上测试的相关内容<br>本次主要考察MIPS汇编，第一题很easy，二三题都给了C代码，直接翻译即可。<br>题目每年都会发生变化，题面仅靠回忆，题意描述可能与原题有一定差异</p>          </div><h1 id="P2-Q1（Mooncake"><a href="#P2-Q1（Mooncake" class="headerlink" title="P2_Q1（Mooncake)"></a>P2_Q1（Mooncake)</h1><h2 id="大意"><a href="#大意" class="headerlink" title="大意"></a>大意</h2><p>中秋之际，小班长为同学们搬月饼。小班 N 个人，最少搬 L 个月饼， 最多搬 R 个月饼。搬来月饼后，每人拿一个，直到剩下的数量 &lt; N，并将剩下的月饼还回去。要实现每个人拿到尽可能多的月饼，并减少还回去的负担，请你计算拿多少个月饼 （用K表示）合适，输出每个人<strong>拿完剩下的</strong>月饼数。</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>纯数学题，读懂题意后，直接用 R &#x2F; N，再将结果乘 N ，结果用 x 表示，那么此时 x 就是保证每个人拿到尽可能多的月饼的总月饼数，例如：n &#x3D; 10, R &#x3D; 36，则x&#x3D;30。此时便得到了理想情况下拿的月饼数K，但是有一个最小值 L 限制，并注意要输出同学们拿剩下的月饼数，而不是K，故直接输出 x &lt; L ? L-x : 0。然后翻译。</p><h1 id="P2-Q2"><a href="#P2-Q2" class="headerlink" title="P2_Q2"></a>P2_Q2</h1><h2 id="大意-1"><a href="#大意-1" class="headerlink" title="大意"></a>大意</h2><p>利用欧拉筛判断一个输入的数n是否为质数。大佬们说代码给的是欧式筛，我不太懂（逃）<span class="github-emoji" alias="sob" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span></p><h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p>没有思路，对着C代码翻译即可，也没有函数，无需维护变量，细心点，问题不大<span class="github-emoji" alias="smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">&#x1f604;</span></p><h1 id="P3-Q3"><a href="#P3-Q3" class="headerlink" title="P3_Q3"></a>P3_Q3</h1><h2 id="大意-2"><a href="#大意-2" class="headerlink" title="大意"></a>大意</h2><p>这道题题面很长，有点麻烦，我没看，既然给了C代码，<del>直接肉编就好了</del> 😁</p><p>听别人说是输出在一个有向图中从给定出发点出发可到达的出度为0的点的个数，假如出发点出度为0则直接输出0</p><h2 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h2><p>直接对着肉编，然后可以<strong>封装</strong>几个常用的函数为宏，比如<code>getInt</code> <code>putInt</code> <code>getIndex</code> <code>exit</code> ……（这几个宏很常用，<del>甚至可以开考前敲上去</del>）使用起来更加方便，也利于整体代码的美观与调试，值得一提的是，<strong>尽量避免在宏中使用寄存器</strong>，以此来避免可能的麻烦，除非迫不得已（如$a0, $v0等）,如果使用了寄存器，需<strong>注意上下文</strong>有无对寄存器值的维护或者压根这个寄存器就是随手用随手赋值的（这样并不被提倡）。</p><p>对于本题，每次递归前后，维护好 <code>$ra</code> 寄存器以及<code>u</code> <code>i</code> 的值即可，建议编写、调用宏<code>push</code> <code>pop</code></p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>值得一提的是，本次Q3来自我们亲爱的助教 zygg ，上次上机后我还问过他谁出题这个事来着（<del>未经</del>）<span class="github-emoji" alias="laughing" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8">&#x1f606;</span>,他跟我说你下次上机就能初见端倪了。</p><p>P2之后就是P3——单周期CPU的logisim实现了，加油！</p>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>mips</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>热力学基础复习</title>
    <link href="/2024/10/20/%E7%83%AD%E5%8A%9B%E5%AD%A6%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/"/>
    <url>/2024/10/20/%E7%83%AD%E5%8A%9B%E5%AD%A6%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h1><h2 id="热力学第零定律"><a href="#热力学第零定律" class="headerlink" title="热力学第零定律"></a>热力学第零定律</h2><ul><li>内容：如果两个物体都与处于确定状态的第三物体处于热平衡，则该两个物体彼此处于热平衡</li><li>宏观性质：处于同一热平衡状态的所有物体都具有<strong>共同</strong>的宏观性质，我们用<strong>温度</strong>来表征这一宏观性质</li></ul><h2 id="热力学过程"><a href="#热力学过程" class="headerlink" title="热力学过程"></a>热力学过程</h2><ul><li>系统：在热力学中，把所研究的物体或物体组叫做热力学系统(thermodynamic system),简称系统(system)</li><li>准静态过程：准静态过程是无限缓慢的状态变化过程，在过程进行的每一 步确保系统的状态均<strong>可视为平衡态</strong></li><li>热力学的研究是<strong>以准静态过程的研究为基础</strong>的</li></ul><h2 id="功-能量-内能"><a href="#功-能量-内能" class="headerlink" title="功   能量   内能"></a>功   能量   内能</h2><ul><li>引入：<br><img src="/../img_post/physics_1.png"><br>设有一气缸，其中气体的压强为p, 活塞的面积为S，活塞与气缸间无摩擦，为了维持气体时时处在平衡态，外界和气体对活塞的压力必须相等.当活塞缓慢移动一微小距离dl 时，在这一微小的变化过程中，可认为压强p 处处均匀且大小不变，在此过程中，气体所做的功为<br>$$<br>dA &#x3D; pSdl &#x3D; pdV<br>$$</li><li>式中dV 是气体体积的微小增量.<em>在气体膨胀时, dV是正的，dA也是正的</em>，表示系统对外做功；<em>在气体被压缩时，dV 是负的，dA 也是负的</em>，表示系统做负功，即外界对系统做功.</li><li><strong>注意</strong>：这里的dA与我们高中定义的不同，反正我们高中说的是<em>外界对气体做的功</em>，也就是把对象调转了，注意区别</li><li>做功是系统与外界相互作用的一种方式，也是两者的能量交换的 一种方式.这种能量交换的方式是通过<strong>宏观的有规则运</strong>动(如机械运动、电流等) 来完成的.而热传递和做功不同，这种交换能量的方式是通过<strong>分子的无规则运动</strong>来完成</li><li>热力学系统具有一定的能量，叫做热力学系统的“内能”.上述实验事实表明：<strong>内能的改变量只决定于初末状态，而与所经历的过程无关</strong>.换句话说，内能是系统状态的单值函数.</li></ul><h2 id="热力学第一定律"><a href="#热力学第一定律" class="headerlink" title="热力学第一定律"></a>热力学第一定律</h2><p>如果有 一系统，外界对它传递的热量为Q, 系统从内能为E₁ 的初态改变到内能为E₂ 的 末态，同时系统对外做功为A, 那么,不论过程如何，总有<br>$$<br>Q &#x3D; E_{2} - E_{1} + A\tag{6-2}<br>$$<br>上式的<strong>意义</strong>就是：外界对系统传递的热量，<em>一部分使系统的内能增加，另一部分用于系统对外做功</em></p><h1 id="热力学第一定律的应用"><a href="#热力学第一定律的应用" class="headerlink" title="热力学第一定律的应用"></a>热力学第一定律的应用</h1><h2 id="等容过程"><a href="#等容过程" class="headerlink" title="等容过程"></a>等容过程</h2><p>等容过程的特征是气体的体积保持不变，即V为常量，dV&#x3D;0，相应地，也就有dA &#x3D; 0.，所以，<strong>外界传递给气体的热量全部用来增加气体的内能</strong></p><ul><li><p>摩尔定容热容：</p><ul><li>定义：气体的摩尔定容热容,<strong>是指1 mol 气体在体积不变的条件下，温度改变1K(或1℃)所吸收或放出的热量</strong>，Cv,m表示，所以<br>$$<br> dE &#x3D; \frac mM C_{v,m} dT \tag{6-7}<br>$$</li><li>应用：应该注意，上式是计算过程中<strong>理想气体内能变化的通用公式</strong>，<em>不仅仅适用于等容过程</em>！！！</li><li>计算：由上一章推导的内能公式<br>$$<br>E &#x3D; \frac mM \frac i 2 R T<br>$$<br>两边微分之后与上式比较可得<br>$$<br>C_{v,m} &#x3D; \frac i 2 R \tag{6-8}<br>$$</li><li>上式说明：理想气体的摩尔定容热容是一个<strong>只与分子的自由度有关的量</strong>，它与气体的温度无关</li></ul></li></ul><h2 id="等压过程"><a href="#等压过程" class="headerlink" title="等压过程"></a>等压过程</h2><p>注：以吸热膨胀为例</p><ul><li><p>等压过程做的功（自己想象p-V图）<br>$$<br>A &#x3D; p(V_{2}-V_{1})&#x3D;\frac mM R (T_{2}-T_{1})\tag{6-10b}<br>$$</p></li><li><p>吸热<br>$$<br>Q &#x3D; \Delta E + A &#x3D;E_{2}-E_{1} +\frac mM R (T_{2}-T_{1})\tag{6-11}<br>$$</p></li><li><p>可见：气体在等压膨胀过程中，<strong>所吸收的热量一部分用来增加内能，另一部分用于气体对外做功</strong>；气体在等压压缩过程中，外界对气体做功，同时内能减小，放出 热量.</p></li><li><p>定义：我们把1 mol气体在压强不变的条件下，温度改变1K 所需要的热量叫做气 体的<em>摩尔定压热容</em>：<br>$$<br>C_{p,m} &#x3D; \frac{\delta Q_{p}}{\frac mM dT}<br>$$</p></li><li><p>计算：根据公式6-2,6-7以及6-11以及以上定义，可得<em>迈耶公式</em><br>$$<br>C_{p,m} &#x3D; C_{v,m}+R\tag{6-12}<br>$$</p></li><li><p>理解：根据定义，等压过程<em>所吸收的热量一部分用来增加内能，另一部分用于气体对外做功</em>，而<em>等容过程吸收的热量全部用来增加内能</em>，所以摩尔定压热容大于摩尔定容热容也很好理解</p></li><li><p>摩尔热容比：由公式6-8和6-12可以得出<br>$$<br>C_{p,m}&#x3D;\frac{i+2}2R \tag{6-13}<br>$$<br>我们把二者之比叫做<strong>摩尔热容比</strong>,用γ表示,该值只与<strong>气体分子的自由度有关， 而与气体温度无关</strong>.<br>$$<br>\gamma &#x3D; \frac{C_{p,m}}{C_{v,m}}&#x3D;\frac{i+2}i \tag{6-14}<br>$$</p></li></ul><h2 id="等温过程"><a href="#等温过程" class="headerlink" title="等温过程"></a>等温过程</h2><ul><li><p>特点：温度不变，内能不变</p></li><li><p>偷懒：由于书上只有一页介绍等温过程，而且比较精简，故直接附上图片(<del>其实是偷懒</del> ​ <span class="github-emoji" alias="smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">&#x1f604;</span>)</p></li></ul><p><img src="/../img_post/physics_3.png"></p><h2 id="绝热过程"><a href="#绝热过程" class="headerlink" title="绝热过程"></a>绝热过程</h2><p><img src="/../img_post/physics_2.png"></p>]]></content>
    
    
    <categories>
      
      <category>物理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>复习</tag>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北航计组-P1课下</title>
    <link href="/2024/10/06/P1%E8%AF%BE%E4%B8%8B/"/>
    <url>/2024/10/06/P1%E8%AF%BE%E4%B8%8B/</url>
    
    <content type="html"><![CDATA[<p>就是写篇博客记录下自己的做题过程&amp;心得，并非题解，如果寻求速通题解的朋友可以移步讨论区或者其他dalao的博客哟 <span class="github-emoji" alias="smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">&#x1f604;</span></p><h1 id="Q1-Splitter"><a href="#Q1-Splitter" class="headerlink" title="Q1 Splitter"></a>Q1 Splitter</h1><p>题面：使用 Verilog 搭建一个 32位  Splitter , 给定一个 32位 的二进制数作为输入，将其划分为 四个 8位 的二进制数作为输出<br>纯复习题，就是按部就班地将input的 四个 8位 二进制数作为 四个 输出的驱动即可<br>可悲的是，本人第一遍竟然把 32 位的 Input写成了input 数组 <span class="github-emoji" alias="point_down" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f447.png?v8">&#x1f447;</span></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs verilog"># wrong<br><span class="hljs-keyword">input</span> A [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>],<br># correct<br><span class="hljs-keyword">input</span> [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>] A，<br></code></pre></td></tr></table></figure><p>果然，还是太久不写verilog，生疏了（<del>其实就是菜</del>）<span class="github-emoji" alias="cry" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8">&#x1f622;</span></p><h1 id="Q2-ALU"><a href="#Q2-ALU" class="headerlink" title="Q2 ALU"></a>Q2 ALU</h1><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>使用 Verilog 搭建一个 32 位六运算 ALU 并提交。具体模块端口定义如下：（看到这个表格才发现我Q1的弱智错误似乎就是因为题目的描述是A [31:0] (<del>疯狂找借口</del>) <span class="github-emoji" alias="stuck_out_tongue_closed_eyes" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f61d.png?v8">&#x1f61d;</span></p><p><img src="/../img_post/p1_1.png" alt=" "></p><p>模块功能如下：</p><p><img src="/../img_post/p1_2.png" alt=" "></p><h2 id="知识复习："><a href="#知识复习：" class="headerlink" title="知识复习："></a>知识复习：</h2><p>在这里我们需要复习逻辑右移和算术右移的区别：</p><p>逻辑右移不考虑符号位，右移一位，左边补零即可。</p><p>算术右移需要考虑符号位，右移一位，若符号位为 1 ，在左边补 1 ；否则，补 0 。</p><p>例如，8 位二进制数 10111101 分别右移2位。</p><ul><li>逻辑右移结果为<strong>00</strong>101111</li><li>算术右移结果为<strong>11</strong>101111</li></ul><p><strong>提示</strong>：可以回Pre中的<a href="http://cscore.buaa.edu.cn/tutorial/verilog/verilog-3/verilog-3-7/">这一小节</a>好好复习了，注意**$signed()**的用法</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>其他的没什么说的，照搬功能描述，需要注意的是最后一个op, <strong>A有符号</strong> ，B无符号</p><p>如果直接三目运算符加$signed(A) &gt;&gt;&gt; B,则根据符号原则（最外层确定，向内传播），会把结果变为无符号数，这显然达不到我们想要的结果</p><p>因此，采取Pre中的一种推荐方案“对于复杂的表达式避免使用 <code>$signed()</code> ，如果希望使用 <code>$signed()</code> 可以将这一部分抽离出来单独作为一个变量”</p><p>意即单独定义一个wire型变量作为$signed(A) &gt;&gt;&gt; B</p><p>注意到这点后，本题应该没什么难度了，最后，贴上俺的代码</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">module</span> alu (<br>    <span class="hljs-keyword">input</span> [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>] A,<br>    <span class="hljs-keyword">input</span> [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>] B,<br>    <span class="hljs-keyword">input</span> [<span class="hljs-number">2</span>:<span class="hljs-number">0</span>] ALUOp,<br>    <span class="hljs-keyword">output</span> [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>] C<br>);<br>    <span class="hljs-keyword">wire</span> [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>] tmp;<br>    <span class="hljs-keyword">assign</span> tmp =  <span class="hljs-built_in">$signed</span>(A) &gt;&gt;&gt; B;<br>    <span class="hljs-keyword">assign</span> C = (ALUOp == <span class="hljs-number">3&#x27;b000</span>) ? (A + B) :<br>               (ALUOp == <span class="hljs-number">3&#x27;b001</span>) ? (A - B) :<br>               (ALUOp == <span class="hljs-number">3&#x27;b010</span>) ? (A &amp; B) :<br>               (ALUOp == <span class="hljs-number">3&#x27;b011</span>) ? (A | B) :<br>               (ALUOp == <span class="hljs-number">3&#x27;b100</span>) ? (A &gt;&gt; B) :<br>               (ALUOp == <span class="hljs-number">3&#x27;b101</span>) ? tmp :<br>                <span class="hljs-number">0</span>;<br>    <br><span class="hljs-keyword">endmodule</span><br></code></pre></td></tr></table></figure><p><strong>结语</strong></p><p>本蒟蒻只做了两道目前，先上传到博客主要是测试一下代码块的风格设置是否成功以及其他功能，后续还会更新本篇 <span class="github-emoji" alias="yum" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60b.png?v8">&#x1f60b;</span></p><hr><p><strong>updated: 10.10</strong></p><h1 id="Q3-EXT"><a href="#Q3-EXT" class="headerlink" title="Q3 EXT"></a>Q3 EXT</h1><h2 id="题面"><a href="#题面" class="headerlink" title="题面"></a>题面</h2><p><img src="/../img_post/p1_3.png"></p><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>verilog中变量位宽以及进制的定义不可偷懒，否则默认为10进制，32位。</p><p>本题直接根据EOp进行位拼接即可，符号位扩展直接拼接“16个最高位”，零扩展直接拼接0即可，<strong>需注意 0 要写做 16’b0</strong>，否则默认为32位的，贴一下本人的代码</p><p>值得注意的是，{16’b0} 也可以写成 {16{1’b0}} ，只是看起来有点“臃肿” <span class="github-emoji" alias="thinking" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f914.png?v8">&#x1f914;</span></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">module</span> ext (<br>    <span class="hljs-keyword">input</span> [<span class="hljs-number">15</span>:<span class="hljs-number">0</span>] imm,<br>    <span class="hljs-keyword">input</span> [<span class="hljs-number">1</span>:<span class="hljs-number">0</span>] Eop,<br>    <span class="hljs-keyword">output</span> [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>] ext<br>);<br>    <span class="hljs-keyword">assign</span> ext = (Eop == <span class="hljs-number">2&#x27;b00</span>) ? &#123;&#123;<span class="hljs-number">16</span>&#123;imm[<span class="hljs-number">15</span>]&#125;&#125;, imm&#125; :<br>                 (Eop == <span class="hljs-number">2&#x27;b01</span>) ? &#123;<span class="hljs-number">16&#x27;b0</span>, imm&#125; : <br>                 (Eop == <span class="hljs-number">2&#x27;b10</span>) ? &#123;imm, <span class="hljs-number">16&#x27;b0</span>&#125; :<br>                 (Eop == <span class="hljs-number">2&#x27;b11</span>) ? &#123;&#123;<span class="hljs-number">14</span>&#123;imm[<span class="hljs-number">15</span>]&#125;&#125;, imm, <span class="hljs-number">2&#x27;b0</span>&#125; :<br>                         <span class="hljs-number">32&#x27;b0</span>;<br>    <br><span class="hljs-keyword">endmodule</span><br></code></pre></td></tr></table></figure><h1 id="Q4-Gray"><a href="#Q4-Gray" class="headerlink" title="Q4 Gray"></a>Q4 Gray</h1><h2 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h2><p>就是设计一个格雷码计数器，具备同步复位功能和使能端，需要了解格雷码的同学可以<a href="https://metaso.cn/search/8532650087478169600?q=%E6%A0%BC%E9%9B%B7%E7%A0%81&bs=8532650003386961920">点击我</a>，顺便给各位推荐一下<a href="https://metaso.cn/">秘塔搜索</a>比较好用的搜索引擎，可以搜论文（选择“学术”搜索模式），最近也提供了搜索图片功能，无广告，在每次搜索下方还会有相关参考文献，可以点击跳转，超丰富。like this:</p><p><img src="/../img_post/metaso.png"></p><p>步入正题，直接附上题目说明</p><p><img src="/../img_post/p1_4_2.png"></p><hr><p>题目还贴心地附上了格雷码转换表，以及波形图（<del>虽然我没有写testbench</del>，但这并不是一个好习惯，各位还是要尽量写一写tb的 <span class="github-emoji" alias="smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">&#x1f604;</span>)</p><p><img src="/../img_post/p1_4.png"></p><p>个人感觉没啥好说的，直接写就是了，附上代码</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">module</span> gray (<br>    <span class="hljs-keyword">input</span> Clk,<br>    <span class="hljs-keyword">input</span> Reset,<br>    <span class="hljs-keyword">input</span> En,<br>    <span class="hljs-keyword">output</span> [<span class="hljs-number">2</span>:<span class="hljs-number">0</span>] Output,<br>    <span class="hljs-keyword">output</span> Overflow<br>);<br>    <span class="hljs-keyword">reg</span> [<span class="hljs-number">2</span>:<span class="hljs-number">0</span>] code = <span class="hljs-number">3&#x27;b0</span>;<br>    <span class="hljs-keyword">reg</span> over = <span class="hljs-number">1&#x27;b0</span>;<br>    <span class="hljs-keyword">always</span> @(<span class="hljs-keyword">posedge</span> Clk) <span class="hljs-keyword">begin</span><br>        <span class="hljs-keyword">if</span>(Reset)<span class="hljs-keyword">begin</span><br>            code &lt;= <span class="hljs-number">3&#x27;b0</span>;<br>            over &lt;= <span class="hljs-number">1&#x27;b0</span>;<br>        <span class="hljs-keyword">end</span><br><br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>            <span class="hljs-keyword">if</span>(En)<span class="hljs-keyword">begin</span><br>                <span class="hljs-keyword">if</span>(code == <span class="hljs-number">3&#x27;b111</span>)<span class="hljs-keyword">begin</span><br>                    over &lt;= <span class="hljs-number">1&#x27;b1</span>;<br>                <span class="hljs-keyword">end</span><br>                code &lt;= code + <span class="hljs-number">3&#x27;b1</span>;<br>            <span class="hljs-keyword">end</span><br><br>            <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                code &lt;= code;<br>            <span class="hljs-keyword">end</span><br>        <span class="hljs-keyword">end</span><br>    <span class="hljs-keyword">end</span><br><br>        <span class="hljs-keyword">assign</span> Output = (code == <span class="hljs-number">3&#x27;b000</span>) ? <span class="hljs-number">3&#x27;b000</span> :<br>                        (code == <span class="hljs-number">3&#x27;b001</span>) ? <span class="hljs-number">3&#x27;b001</span> :<br>                        (code == <span class="hljs-number">3&#x27;b010</span>) ? <span class="hljs-number">3&#x27;b011</span> :<br>                        (code == <span class="hljs-number">3&#x27;b011</span>) ? <span class="hljs-number">3&#x27;b010</span> :<br>                        (code == <span class="hljs-number">3&#x27;b100</span>) ? <span class="hljs-number">3&#x27;b110</span> :<br>                        (code == <span class="hljs-number">3&#x27;b101</span>) ? <span class="hljs-number">3&#x27;b111</span> :<br>                        (code == <span class="hljs-number">3&#x27;b110</span>) ? <span class="hljs-number">3&#x27;b101</span> :<br>                        (code == <span class="hljs-number">3&#x27;b111</span>) ? <span class="hljs-number">3&#x27;b100</span> :<br>                        <span class="hljs-number">3&#x27;b000</span>;<br>        <span class="hljs-keyword">assign</span> Overflow = over;<br><span class="hljs-keyword">endmodule</span><br></code></pre></td></tr></table></figure><h1 id="Q5-Expr"><a href="#Q5-Expr" class="headerlink" title="Q5 Expr"></a>Q5 Expr</h1><h2 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a>题目</h2><p><img src="/../img_post/p1_5.png"></p><p><img src="/../img_post/p1_5_2.png"></p><h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p>题目是几天前写的，<del>忘了（bushi)</del>😢</p><hr><p>定义有点递归的味道emmm……<br>“单个数字[0-9]是F； b. 如果<strong>X</strong>是F，<strong>Y</strong>是F，<strong>X+Y</strong>也是F； c. 如果<strong>X</strong>是F，<strong>Y</strong>是F，<strong>X*Y</strong>也是F”</p><p>虽然题目没有明说，但根据题目描述“每个时钟上升沿，状态机从 in 中读入一个ASCII编码的字符。假设读入的第i个字符为ci，则第n个时钟上升沿时，可以拼出一个字符串: s&#x3D;c1c2….cn  我们需要你此时判断 s 是否符合表达式F的定义”以及波形图，大致可以确定状态机类型：Moore型状态机<br>读懂题目意思后，其实正确状态就是类似 1 + 2 * 3 + 7 * 9 ,即：<br><strong>必须先输入一个数字，然后接着输入一个运算符，连续的数字或者运算符都直接pass掉，合法状态要么是一个数字要么是（数字）（运算符）（数字）</strong>。故设计5种状态，分别为：</p><table><thead><tr><th>状态</th><th>说明</th></tr></thead><tbody><tr><td>state_0</td><td>空状态，初始状态</td></tr><tr><td>state_1</td><td>错误状态，当非法输入后一直保持此状态，除非reset</td></tr><tr><td>state_2</td><td>只输入了一个数字</td></tr><tr><td>state_3</td><td>后缀输入为（数字）（运算符）</td></tr><tr><td>state_4</td><td>后缀输入为（数字）（运算符）（数字）</td></tr></tbody></table><p>后来发现似乎state_2 和 state_4 可以合并，都行<br>代码：</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">module</span> expr (<br>    <span class="hljs-keyword">input</span> clk,<br>    <span class="hljs-keyword">input</span> clr,<br>    <span class="hljs-keyword">input</span> [<span class="hljs-number">7</span>:<span class="hljs-number">0</span>] in,<br>    <span class="hljs-keyword">output</span> out<br>);<br>    <span class="hljs-keyword">reg</span> [<span class="hljs-number">2</span>:<span class="hljs-number">0</span>] state = <span class="hljs-number">3&#x27;b0</span>;<br>    <span class="hljs-keyword">parameter</span> state_0 = <span class="hljs-number">3&#x27;b000</span>,<br>              state_1 = <span class="hljs-number">3&#x27;b001</span>,<br>              state_2 = <span class="hljs-number">3&#x27;b010</span>,<br>              state_3 = <span class="hljs-number">3&#x27;b011</span>,<br>              state_4 = <span class="hljs-number">3&#x27;b100</span>;<br><br>    <span class="hljs-keyword">always</span> @(<span class="hljs-keyword">posedge</span> clk <span class="hljs-keyword">or</span> <span class="hljs-keyword">posedge</span> clr) <span class="hljs-keyword">begin</span><br>            <span class="hljs-keyword">if</span>(clr) <span class="hljs-keyword">begin</span><br>                state &lt;= state_0;<br>            <span class="hljs-keyword">end</span><br><br>            <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                <span class="hljs-keyword">case</span> (state)<br>                    state_0: <span class="hljs-keyword">begin</span><br>                        <span class="hljs-keyword">if</span>(in &gt; <span class="hljs-number">6&#x27;d47</span> &amp;&amp; in &lt; <span class="hljs-number">6&#x27;d58</span>)<span class="hljs-keyword">begin</span><br>                            state &lt;= state_2;<br>                        <span class="hljs-keyword">end</span><br>                        <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span> <span class="hljs-comment">//first input must be number</span><br>                            state &lt;= state_1;<br>                        <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">end</span><br><br>                    state_1: <span class="hljs-keyword">begin</span><br>                            state &lt;= state_1;<br>                    <span class="hljs-keyword">end</span><br><br>                    state_2: <span class="hljs-keyword">begin</span><br>                        <span class="hljs-keyword">if</span>(in == <span class="hljs-number">6&#x27;d42</span> || in == <span class="hljs-number">6&#x27;d43</span>)<span class="hljs-keyword">begin</span><br>                            state &lt;= state_3;<br>                        <span class="hljs-keyword">end</span><br>                        <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                            state &lt;= state_1;<br>                        <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">end</span><br><br>                    state_3: <span class="hljs-keyword">begin</span><br>                        <span class="hljs-keyword">if</span>(in &gt; <span class="hljs-number">6&#x27;d47</span> &amp;&amp; in &lt; <span class="hljs-number">6&#x27;d58</span>) <span class="hljs-keyword">begin</span><br>                            state &lt;= state_4;<br>                        <span class="hljs-keyword">end</span><br>                        <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                            state &lt;= state_1;<br>                        <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">end</span><br><br>                    state_4: <span class="hljs-keyword">begin</span><br>                        <span class="hljs-keyword">if</span>(in == <span class="hljs-number">6&#x27;d42</span> || in == <span class="hljs-number">6&#x27;d43</span>) <span class="hljs-keyword">begin</span><br>                            state &lt;= state_3;<br>                        <span class="hljs-keyword">end</span><br>                        <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                            state &lt;= state_1;<br>                        <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">end</span><br><br>                    <span class="hljs-keyword">default</span>: state &lt;= state_0;<br>                <span class="hljs-keyword">endcase</span><br>            <span class="hljs-keyword">end</span><br>        <span class="hljs-keyword">end</span>    <br>                <br>    <span class="hljs-keyword">assign</span> out = (state == state_4) ? <span class="hljs-number">1&#x27;b1</span> :<br> (state == state_2) ? <span class="hljs-number">1&#x27;b1</span> : <span class="hljs-number">1&#x27;b0</span>;<br><span class="hljs-keyword">endmodule</span><br></code></pre></td></tr></table></figure><h1 id="Q6-BlockChecker"><a href="#Q6-BlockChecker" class="headerlink" title="Q6 BlockChecker"></a>Q6 BlockChecker</h1><h2 id="本题为附加题，通过与否不计入P1课下通过条件"><a href="#本题为附加题，通过与否不计入P1课下通过条件" class="headerlink" title="本题为附加题，通过与否不计入P1课下通过条件"></a>本题为附加题，通过与否不计入P1课下通过条件</h2><h2 id="题目-3"><a href="#题目-3" class="headerlink" title="题目"></a>题目</h2><p><img src="/../img_post/p1_6.png"></p><p><img src="/../img_post/p1_6_2.png"></p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>其实就是把之前数据结构的一道括号匹配题魔改了，<strong>核心思路差不多，通过一个flag来标志匹配状态，读入 Begin（左括号）时 flag 加 1 ，End（右括号）时flag - 1，最后判断 flag 是否为 0 来进行输出</strong>，<strong>同时如果输入 begin 之后，不输入空格，而是输入其他字符，则应该把flag相应地加（减）回去。</strong><br><strong>值得注意的是</strong>：本题是一道本题依旧是 Moore 状态机，因为观察波形输出 result 只在 clk 上升沿变化 ！</p><p>对于输入，如果未匹配的 end 出现在了 begin 前面， 则输入已经非法，此后不管输入如何，都输出 0 （除非reset），那么我们便可以设置一个单独的状态来保存这一状态， 当flag &lt; 0 时直接就“锁”在这个状态里。</p><p>同时，鼓励大家<strong>使用parameter</strong> 来声明_状态_这一常量。</p><p>多的懒得说了，代码里有注释，直接上代码😁😁😁</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">module</span> BlockChecker (<br>    <span class="hljs-keyword">input</span> clk,<br>    <span class="hljs-keyword">input</span> reset,<br>    <span class="hljs-keyword">input</span> [<span class="hljs-number">7</span>:<span class="hljs-number">0</span>] in,<br>    <span class="hljs-keyword">output</span> result<br>);<br>    <span class="hljs-keyword">reg</span> [<span class="hljs-number">3</span>:<span class="hljs-number">0</span>] state = <span class="hljs-number">4&#x27;b0001</span>; <span class="hljs-comment">//store status</span><br>    <span class="hljs-keyword">reg</span> [<span class="hljs-number">31</span>:<span class="hljs-number">0</span>] match = <span class="hljs-number">32&#x27;b0</span>; <span class="hljs-comment">//store if_match</span><br><br>    <span class="hljs-keyword">parameter</span> st_0 = <span class="hljs-number">4&#x27;b0000</span>, <span class="hljs-comment">//null or initial</span><br>              st_1 = <span class="hljs-number">4&#x27;b0001</span>, <span class="hljs-comment">//input &quot; &quot;</span><br>              st_2 = <span class="hljs-number">4&#x27;b0010</span>, <span class="hljs-comment">//input &quot;b&quot;</span><br>              st_3 = <span class="hljs-number">4&#x27;b0011</span>, <span class="hljs-comment">//input &quot;be&quot;</span><br>              st_4 = <span class="hljs-number">4&#x27;b0100</span>, <span class="hljs-comment">//input &quot;beg&quot;</span><br>              st_5 = <span class="hljs-number">4&#x27;b0101</span>, <span class="hljs-comment">//input &quot;begi&quot;</span><br>              st_6 = <span class="hljs-number">4&#x27;b0110</span>, <span class="hljs-comment">//input &quot;begin&quot;</span><br>              st_7 = <span class="hljs-number">4&#x27;b0111</span>, <span class="hljs-comment">//input other words,such as &quot;hello&quot;</span><br>              st_8 = <span class="hljs-number">4&#x27;b1000</span>, <span class="hljs-comment">//input &quot;e&quot;</span><br>              st_9 = <span class="hljs-number">4&#x27;b1001</span>, <span class="hljs-comment">//input &quot;en&quot;</span><br>             st_10 = <span class="hljs-number">4&#x27;b1010</span>; <span class="hljs-comment">//input &quot;end&quot;</span><br>    <br>    <span class="hljs-keyword">always</span> @(<span class="hljs-keyword">posedge</span> clk <span class="hljs-keyword">or</span> <span class="hljs-keyword">posedge</span> reset) <span class="hljs-keyword">begin</span><br>        <span class="hljs-keyword">if</span>(reset) <span class="hljs-keyword">begin</span><br>            state &lt;= st_1;<br>            match &lt;= <span class="hljs-number">32&#x27;b0</span>;<br>        <span class="hljs-keyword">end</span><br>        <br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>            <span class="hljs-keyword">case</span> (state)<br>                st_0: <span class="hljs-keyword">begin</span><br>                    state &lt;= st_0;<br>                <span class="hljs-keyword">end</span><br><br>                st_1: <span class="hljs-keyword">begin</span><br>                    <span class="hljs-keyword">if</span>(<span class="hljs-built_in">$signed</span> (match) &lt; <span class="hljs-built_in">$signed</span>(<span class="hljs-number">32&#x27;b0</span>)) <span class="hljs-keyword">begin</span><br>                        state &lt;= st_0;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                            <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot;b&quot;</span> || in == <span class="hljs-string">&quot;B&quot;</span>) <span class="hljs-keyword">begin</span><br>                            state &lt;= st_2;<br>                        <span class="hljs-keyword">end</span><br>                        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot;e&quot;</span> || in == <span class="hljs-string">&quot;E&quot;</span>) <span class="hljs-keyword">begin</span><br>                            state &lt;= st_8;<br>                        <span class="hljs-keyword">end</span><br>                        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot; &quot;</span>)<span class="hljs-keyword">begin</span><br>                            state &lt;= st_1;<br>                        <span class="hljs-keyword">end</span><br>                        <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                            state &lt;= st_7;<br>                        <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">end</span><br>                    <br>                <span class="hljs-keyword">end</span><br><br>                st_2: <span class="hljs-keyword">begin</span><br>                    <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot;e&quot;</span> || in == <span class="hljs-string">&quot;E&quot;</span>) <span class="hljs-keyword">begin</span><br>                        state &lt;= st_3;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                        state &lt;= st_7;<br>                    <span class="hljs-keyword">end</span><br>                <span class="hljs-keyword">end</span><br><br>                st_3: <span class="hljs-keyword">begin</span><br>                    <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot;g&quot;</span> || in == <span class="hljs-string">&quot;G&quot;</span>) <span class="hljs-keyword">begin</span><br>                        state &lt;= st_4;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot; &quot;</span>)<span class="hljs-keyword">begin</span><br>                        state &lt;= st_1;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                        state &lt;= st_7;<br>                    <span class="hljs-keyword">end</span><br>                <span class="hljs-keyword">end</span><br><br>                st_4: <span class="hljs-keyword">begin</span><br>                    <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot;i&quot;</span> || in == <span class="hljs-string">&quot;I&quot;</span>) <span class="hljs-keyword">begin</span><br>                        state &lt;= st_5;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot; &quot;</span>)<span class="hljs-keyword">begin</span><br>                        state &lt;= st_1;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                        state &lt;= st_7;<br>                    <span class="hljs-keyword">end</span><br>                <span class="hljs-keyword">end</span><br><br>                st_5: <span class="hljs-keyword">begin</span><br>                    <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot;n&quot;</span> || in == <span class="hljs-string">&quot;N&quot;</span>) <span class="hljs-keyword">begin</span><br>                        match &lt;= match + <span class="hljs-number">32&#x27;b1</span>;<br>                        state &lt;= st_6;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot; &quot;</span>)<span class="hljs-keyword">begin</span><br>                        state &lt;= st_1;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                        state &lt;= st_7;<br>                    <span class="hljs-keyword">end</span><br>                <span class="hljs-keyword">end</span><br><br>                st_6: <span class="hljs-keyword">begin</span><br>                    <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot; &quot;</span>) <span class="hljs-keyword">begin</span><br>                        state &lt;= st_1;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                        match &lt;= match - <span class="hljs-number">32&#x27;b1</span>;<br>                        state &lt;= st_7;<br>                    <span class="hljs-keyword">end</span><br>                <span class="hljs-keyword">end</span><br><br>                st_7: <span class="hljs-keyword">begin</span><br>                    <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot; &quot;</span>) <span class="hljs-keyword">begin</span><br>                        state &lt;= st_1;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                        state &lt;= st_7;<br>                    <span class="hljs-keyword">end</span><br>                <span class="hljs-keyword">end</span><br><br>                st_8: <span class="hljs-keyword">begin</span><br>                    <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot;n&quot;</span> || in == <span class="hljs-string">&quot;N&quot;</span>) <span class="hljs-keyword">begin</span><br>                        state &lt;= st_9;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot; &quot;</span>)<span class="hljs-keyword">begin</span><br>                        state &lt;= st_1;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                        state &lt;= st_7;<br>                    <span class="hljs-keyword">end</span><br>                <span class="hljs-keyword">end</span><br><br>                st_9: <span class="hljs-keyword">begin</span><br>                    <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot;d&quot;</span> || in == <span class="hljs-string">&quot;D&quot;</span>) <span class="hljs-keyword">begin</span><br>                        match &lt;= match - <span class="hljs-number">32&#x27;b1</span>;<br>                        state &lt;= st_10;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot; &quot;</span>)<span class="hljs-keyword">begin</span><br>                        state &lt;= st_1;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                        state &lt;= st_7;<br>                    <span class="hljs-keyword">end</span><br>                <span class="hljs-keyword">end</span><br><br>                st_10: <span class="hljs-keyword">begin</span><br>                    <span class="hljs-keyword">if</span>(in == <span class="hljs-string">&quot; &quot;</span>) <span class="hljs-keyword">begin</span><br>                        state &lt;= st_1;<br>                    <span class="hljs-keyword">end</span><br>                    <span class="hljs-keyword">else</span> <span class="hljs-keyword">begin</span><br>                        match &lt;= match + <span class="hljs-number">32&#x27;b1</span>;<br>                        state &lt;= st_7;<br>                    <span class="hljs-keyword">end</span><br>                <span class="hljs-keyword">end</span><br><br>                <span class="hljs-keyword">default</span>: state &lt;= st_0;<br><br>            <span class="hljs-keyword">endcase</span><br>        <span class="hljs-keyword">end</span><br>    <span class="hljs-keyword">end</span><br>    <br>    <span class="hljs-keyword">assign</span> result = (match == <span class="hljs-number">32&#x27;b0</span>) ? <span class="hljs-number">1&#x27;b1</span> : <span class="hljs-number">1&#x27;b0</span>;<br><br><span class="hljs-keyword">endmodule</span><br></code></pre></td></tr></table></figure><p>最后提一嘴，本人一开始不知道verilog中可以直接双引号加字母，like “A”,来表示字母的acii码，，也定义了一堆ascii常量，悲!😭😭😭</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs verilog"><span class="hljs-keyword">parameter</span> in_b = <span class="hljs-number">8&#x27;d66</span>, in_B = <span class="hljs-number">8&#x27;d98</span>,<br>          in_d = <span class="hljs-number">8&#x27;d68</span>, in_D = <span class="hljs-number">8&#x27;d100</span>,<br>          in_e = <span class="hljs-number">8&#x27;d69</span>, in_E = <span class="hljs-number">8&#x27;d101</span>,<br>          in_g = <span class="hljs-number">8&#x27;d71</span>, in_G = <span class="hljs-number">8&#x27;d103</span>,<br>          in_i = <span class="hljs-number">8&#x27;d73</span>, in_I = <span class="hljs-number">8&#x27;d105</span>,<br>          in_n = <span class="hljs-number">8&#x27;d78</span>, in_N = <span class="hljs-number">8&#x27;d110</span>,<br>          in_space = <span class="hljs-number">8&#x27;d32</span>;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>CO</category>
      
      <category>实验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CO</tag>
      
      <tag>Verilog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>My fist blog</title>
    <link href="/2024/10/05/My-fist-blog/"/>
    <url>/2024/10/05/My-fist-blog/</url>
    
    <content type="html"><![CDATA[<h1 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h1><p>经过了大概一天的努力，<strong>个人博客终于搭建好了！</strong><del>虽然很简陋（后续会不断完善）</del> 采用的是hexo架构和fluid主题<br>这一天下来，也不容易啊，走了不少弯路，踩了很多坑，不断地上网查阅资料，也有很多自己摸索的地方，磕磕绊绊的过程中也有不少收获。</p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><h2 id="还好有个国庆假让我有时间花一整天搭建博客（其实是暑假纯玩去了）"><a href="#还好有个国庆假让我有时间花一整天搭建博客（其实是暑假纯玩去了）" class="headerlink" title="还好有个国庆假让我有时间花一整天搭建博客（其实是暑假纯玩去了）"></a>还好有个国庆假让我有时间花一整天搭建博客<del>（其实是暑假纯玩去了）</del></h2><h2 id="接下来还需要学习的地方"><a href="#接下来还需要学习的地方" class="headerlink" title="接下来还需要学习的地方"></a>接下来还需要学习的地方</h2><ul><li>了解git的用法</li><li>尝试将fluid主题的代码块修改为Mac风格<del>纯粹是为了装</del></li><li>写物理笔记并上传博客</li><li>写P1课下并写一篇心得</li></ul><h1 id="鸣谢"><a href="#鸣谢" class="headerlink" title="鸣谢"></a>鸣谢</h1><ol><li><a href="https://hexo.fluid-dev.com/docs/start/">Fluid主题官方介绍文档</a> 全中文文档</li><li><a href="https://zhuanlan.zhihu.com/p/371995929">知乎大神的博客搭建指南</a> 手把手教学</li><li>各位学长学姐，正是看了你们的博客，才想建一个自己的，motivation了算是</li></ol>]]></content>
    
    
    <categories>
      
      <category>日常</category>
      
    </categories>
    
    
    <tags>
      
      <tag>-碎碎念</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>

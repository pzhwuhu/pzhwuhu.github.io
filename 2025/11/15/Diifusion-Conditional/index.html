<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="light"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/avatar_1.jpg"><link rel="icon" href="/img/avatar_1.jpg"><link rel="canonical" href="http://pzhwuhu.github.io/2025/11/15/Diifusion-Conditional/"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="pzhwuhu"><meta name="keywords" content=""><meta name="description" content="è¿™æ˜¯æˆ‘å­¦ä¹ huggingfaceä¸Šçš„diffusion courseæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š             IntroductionUnconditional models donâ€™t give much control over what is generated. We can train a conditional model that"><meta property="og:type" content="article"><meta property="og:title" content="Diifusion-Conditional"><meta property="og:url" content="http://pzhwuhu.github.io/2025/11/15/Diifusion-Conditional/index.html"><meta property="og:site_name" content="é¹å“¥"><meta property="og:description" content="è¿™æ˜¯æˆ‘å­¦ä¹ huggingfaceä¸Šçš„diffusion courseæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š             IntroductionUnconditional models donâ€™t give much control over what is generated. We can train a conditional model that"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://pzhwuhu.github.io/myimg/%E5%8D%83%E4%B8%8E%E5%8D%83%E5%AF%BB.jpg"><meta property="article:published_time" content="2025-11-15T04:23:00.000Z"><meta property="article:modified_time" content="2025-11-15T05:14:15.708Z"><meta property="article:author" content="pzhwuhu"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://pzhwuhu.github.io/myimg/%E5%8D%83%E4%B8%8E%E5%8D%83%E5%AF%BB.jpg"><title>Diifusion-Conditional - é¹å“¥</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/custom_dark_mode.css"><link rel="stylesheet" href="/css/scroll_animation.css"><link rel="stylesheet" href="/css/mac.css"><link rel="stylesheet" href="/css/loading.css"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/gradient.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"pzhwuhu.github.io",root:"/",version:"1.9.8",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!1,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:{key:null},google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"cp3oK6SJdvk7QCx0TiMpNSdI-gzGzoHsz",app_key:"B9u5rSWtXwzrQl56fuCE0o9M",server_url:"https://cp3ok6sj.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2024-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>if(!Fluid.ctx.dnt){var _hmt=_hmt||[];!function(){var t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?[object Object]";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()}</script><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:90vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>èº«å¦‚èŠ¥å­ï¼Œå¿ƒè—é¡»å¼¥</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>é¦–é¡µ</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>å½’æ¡£</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>åˆ†ç±»</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>æ ‡ç­¾</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>å…³äº</span></a></li><li class="nav-item"><a class="nav-link" href="/links/" target="_self"><i class="iconfont icon-link-fill"></i> <span>å‹é“¾</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url('/myimg/%E5%B9%BD%E7%81%B5%E5%85%AC%E4%B8%BB_4.jpg') no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="Diifusion-Conditional"></span></div><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> pzhwuhu </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-11-15 12:23" pubdate>2025å¹´11æœˆ15æ—¥ ä¸­åˆ</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.1k å­— </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 10 åˆ†é’Ÿ </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> æ¬¡</span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="ML&amp;DL" id="heading-07cc572b8beca73017be9b3680716d88" role="tab" data-toggle="collapse" href="#collapse-07cc572b8beca73017be9b3680716d88" aria-expanded="true">ML&amp;DL <span class="list-group-count">(11)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-07cc572b8beca73017be9b3680716d88" role="tabpanel" aria-labelledby="heading-07cc572b8beca73017be9b3680716d88"><div class="category-post-list"><a href="/2025/11/13/Diffusion-%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/" title="Diffusion-åŸç†è§£æ" class="list-group-item list-group-item-action"><span class="category-post">Diffusion-åŸç†è§£æ</span> </a><a href="/2025/11/15/Diifusion-Conditional/" title="Diifusion-Conditional" class="list-group-item list-group-item-action active"><span class="category-post">Diifusion-Conditional</span> </a><a href="/2025/09/15/Matplotlib%20%E7%94%BB%E5%9B%BE/" title="Matplotlib ç”»å›¾" class="list-group-item list-group-item-action"><span class="category-post">Matplotlib ç”»å›¾</span> </a><a href="/2025/08/19/CNN/" title="æå®æ¯…æœºå™¨å­¦ä¹ -CNN" class="list-group-item list-group-item-action"><span class="category-post">æå®æ¯…æœºå™¨å­¦ä¹ -CNN</span> </a><a href="/2025/09/13/GAN/" title="æå®æ¯…æœºå™¨å­¦ä¹ -GAN" class="list-group-item list-group-item-action"><span class="category-post">æå®æ¯…æœºå™¨å­¦ä¹ -GAN</span> </a><a href="/2025/08/16/HW1%20Notebook/" title="æå®æ¯…æœºå™¨å­¦ä¹ -HW1 Notebook" class="list-group-item list-group-item-action"><span class="category-post">æå®æ¯…æœºå™¨å­¦ä¹ -HW1 Notebook</span> </a><a href="/2025/08/22/HW3%20Notebook/" title="æå®æ¯…æœºå™¨å­¦ä¹ -HW3 Notebook" class="list-group-item list-group-item-action"><span class="category-post">æå®æ¯…æœºå™¨å­¦ä¹ -HW3 Notebook</span> </a><a href="/2025/08/21/Self-Attention/" title="æå®æ¯…æœºå™¨å­¦ä¹ -Self-Attention" class="list-group-item list-group-item-action"><span class="category-post">æå®æ¯…æœºå™¨å­¦ä¹ -Self-Attention</span> </a><a href="/2025/08/26/Transformer/" title="æå®æ¯…æœºå™¨å­¦ä¹ -Transformer" class="list-group-item list-group-item-action"><span class="category-post">æå®æ¯…æœºå™¨å­¦ä¹ -Transformer</span> </a><a href="/2025/10/16/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/" title="æå®æ¯…æœºå™¨å­¦ä¹ -å¤§æ¨¡å‹è®­ç»ƒ" class="list-group-item list-group-item-action"><span class="category-post">æå®æ¯…æœºå™¨å­¦ä¹ -å¤§æ¨¡å‹è®­ç»ƒ</span> </a><a href="/2025/08/16/%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%B3%A8%E6%84%8F/" title="æå®æ¯…æœºå™¨å­¦ä¹ -ç±»ç¥ç»ç½‘ç»œè®­ç»ƒ" class="list-group-item list-group-item-action"><span class="category-post">æå®æ¯…æœºå™¨å­¦ä¹ -ç±»ç¥ç»ç½‘ç»œè®­ç»ƒ</span></a></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">Diifusion-Conditional</h1><div class="markdown-body"><div class="note note-info"><p>è¿™æ˜¯æˆ‘å­¦ä¹ huggingfaceä¸Šçš„diffusion courseæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š</p></div><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Unconditional models donâ€™t give much control over what is generated. We can train a conditional model that takes additional inputs to help steer the generation process, but what if we already have a trained unconditional model weâ€™dÂ like to use? <strong>Enter guidance, a process by which the model predictions at each step in the generation process are evaluated against some guidance function and modified such that the final generated image is more to our liking</strong></p><p>There are a number of ways to pass in this conditioning information, such as</p><ul><li><strong>Feeding it in as additional channels in the input to the UNet.</strong> This is often used when the conditioning information is the same shape as the image, such as a segmentation mask, a depth map or a blurry version of the image (in the case of a restoration&#x2F;superresolution model). It does work for other types of conditioning too. For example, in the notebook, <strong>the class label is mapped to an embedding</strong> and then expanded to be the same width and height as the input image so that it can be fed in as additional channels.</li><li><strong>Creating an embedding and then projecting it down to aÂ size</strong>Â that matches the number of channels at the output of one or more internal layers of the UNet, and then adding it to those outputs. This is how the timestep conditioning is handled, for example. <strong>The output of each Resnet block has a projected timestep embedding added to it.</strong> This is useful when you have a vector such as a CLIP image embedding as your conditioning information. A notable example is theÂ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/lambdalabs/stable-diffusion-image-variations">â€˜Image Variationsâ€™ version of Stable Diffusion</a>Â which does exactly this.</li><li><strong>Adding cross-attention layers that can â€˜attendâ€™ to a sequence passed in as conditioning.</strong> This is most useful when the conditioning is in the form of some text - <strong>the text is mapped to a sequence of embeddings using a transformer model</strong>, and then cross-attention layers in the UNet are used to incorporate this information into the denoising path. Weâ€™ll see this in action in Unit 3 as we examine how Stable Diffusion handles text conditioning.</li></ul><p>ä¹Ÿå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„ Conditional Generation</p><h1 id="Guidance"><a href="#Guidance" class="headerlink" title="Guidance"></a>Guidance</h1><p>For example, say we wanted to bias the generated images to be <strong>a specific color</strong>. How would we go about that? Enter <strong>guidance</strong>, a technique for <strong>adding additional control</strong> to the sampling process.</p><p>Step one is to create our c<strong>onditioning function</strong>: some measure (loss) which weâ€™d like to minimize. Hereâ€™s one for the color example, which compares the pixels of an image to a target color (by default a sort of light teal) and returns the average error:</p><h2 id="Color"><a href="#Color" class="headerlink" title="Color"></a>Color</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">color_loss</span>(<span class="hljs-params">images, target_color=(<span class="hljs-params"><span class="hljs-number">0.1</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">0.5</span></span>)</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Given a target color (R, G, B) return a loss for how far away on average</span><br><span class="hljs-string">    the images&#x27; pixels are from that color. Defaults to a light teal: (0.1, 0.9, 0.5)&quot;&quot;&quot;</span><br>    target = (<br>        torch.tensor(target_color).to(images.device) * <span class="hljs-number">2</span> - <span class="hljs-number">1</span><br>    )  <span class="hljs-comment"># Map target color to (-1, 1)</span><br>    target = target[<br>        <span class="hljs-literal">None</span>, :, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    ]  <span class="hljs-comment"># Get shape right to work with the images (b, c, h, w)</span><br>    error = torch.<span class="hljs-built_in">abs</span>(<br>        images - target<br>    ).mean()  <span class="hljs-comment"># Mean absolute difference between the image pixels and the target color</span><br>    <span class="hljs-keyword">return</span> error<br></code></pre></td></tr></table></figure><p>Next, weâ€™ll make <strong>a modified version of the sampling loop</strong> where, at each step, we do the following:</p><ul><li>Create a new version of x that has <code>requires_grad = True</code></li><li>Calculate the denoised version (x0)</li><li>Feed the predicted x0 through our loss function</li><li>Find the <strong>gradient</strong> of this loss function <strong>with respect to x</strong></li><li>Use this <strong>conditioning gradient to modify x</strong> before we step with the scheduler, hopefully pushing x in a direction that will lead to lower loss according to our guidance function</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Variant 2: setting x.requires_grad before calculating the model predictions</span><br>guidance_loss_scale = <span class="hljs-number">40</span><br>x = torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>).to(device)<br><br><span class="hljs-keyword">for</span> i, t <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">enumerate</span>(scheduler.timesteps)):<br><br>    <span class="hljs-comment"># Set requires_grad before the model forward pass</span><br>    x = x.detach().requires_grad_()<br>    model_input = scheduler.scale_model_input(x, t)<br><br>    <span class="hljs-comment"># predict (with grad this time)</span><br>    noise_pred = image_pipe.unet(model_input, t)[<span class="hljs-string">&quot;sample&quot;</span>]<br><br>    <span class="hljs-comment"># Get the predicted x0:</span><br>    x0 = scheduler.step(noise_pred, t, x).pred_original_sample<br><br>    <span class="hljs-comment"># Calculate loss</span><br>    loss = color_loss(x0) * guidance_loss_scale<br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(i, <span class="hljs-string">&quot;loss:&quot;</span>, loss.item())<br><br>    <span class="hljs-comment"># Get gradient</span><br>    cond_grad = -torch.autograd.grad(loss, x)[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-comment"># Modify x based on this gradient</span><br>    x = x.detach() + cond_grad<br><br>    <span class="hljs-comment"># Now step with scheduler</span><br>    x = scheduler.step(noise_pred, t, x).prev_sample<br><br><br>grid = torchvision.utils.make_grid(x, nrow=<span class="hljs-number">4</span>)<br>im = grid.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>).cpu().clip(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) * <span class="hljs-number">0.5</span> + <span class="hljs-number">0.5</span><br>Image.fromarray(np.array(im * <span class="hljs-number">255</span>).astype(np.uint8))<br></code></pre></td></tr></table></figure><h2 id="CLIP-Guidance"><a href="#CLIP-Guidance" class="headerlink" title="CLIP Guidance"></a>CLIP Guidance</h2><p><a target="_blank" rel="noopener" href="https://openai.com/blog/clip/">CLIP</a> is a model created by OpenAI that allows us to <strong>compare images to text captions</strong>. This is extremely powerful, since it allows us to quantify how well an image matches a prompt. And since the process is differentiable å¯å¾®, we can use this as a loss function to guide our diffusion model!</p><p>We wonâ€™t go too much into the details here. The basic approach is as follows:</p><ul><li>Embed the text prompt to get a <strong>512-dimensional CLIP embedding</strong> of the text</li><li>For every step in the diffusion model process:<ul><li>Make several variants of the predicted denoised image (having multiple variations gives a cleaner loss signal)</li><li>For each one, embed the image with CLIP and compare this embedding with the text embedding of the prompt (using a measure called â€˜Great Circle Distance Squaredâ€™)</li></ul></li><li>Calculate the gradient of this loss with respect to the current noisy x and use this gradient to modify x before updating it with the scheduler.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># @markdown load a CLIP model and define the loss function</span><br><span class="hljs-keyword">import</span> open_clip<br><br>clip_model, _, preprocess = open_clip.create_model_and_transforms(<span class="hljs-string">&quot;ViT-B-32&quot;</span>, pretrained=<span class="hljs-string">&quot;openai&quot;</span>)<br>clip_model.to(device)<br><br><span class="hljs-comment"># Transforms to resize and augment an image + normalize to match CLIP&#x27;s training data</span><br>tfms = torchvision.transforms.Compose(<br>    [<br>        torchvision.transforms.RandomResizedCrop(<span class="hljs-number">224</span>),  <span class="hljs-comment"># Random CROP each time</span><br>        torchvision.transforms.RandomAffine(<span class="hljs-number">5</span>),  <span class="hljs-comment"># One possible random augmentation: skews the image</span><br>        torchvision.transforms.RandomHorizontalFlip(),  <span class="hljs-comment"># You can add additional augmentations if you like</span><br>        torchvision.transforms.Normalize(<br>            mean=(<span class="hljs-number">0.48145466</span>, <span class="hljs-number">0.4578275</span>, <span class="hljs-number">0.40821073</span>),<br>            std=(<span class="hljs-number">0.26862954</span>, <span class="hljs-number">0.26130258</span>, <span class="hljs-number">0.27577711</span>),<br>        ),<br>    ]<br>)<br><br><br><span class="hljs-comment"># And define a loss function that takes an image, embeds it and compares with</span><br><span class="hljs-comment"># the text features of the prompt</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">clip_loss</span>(<span class="hljs-params">image, text_features</span>):<br>    image_features = clip_model.encode_image(tfms(image))  <span class="hljs-comment"># Note: applies the above transforms</span><br>    input_normed = torch.nn.functional.normalize(image_features.unsqueeze(<span class="hljs-number">1</span>), dim=<span class="hljs-number">2</span>)<br>    embed_normed = torch.nn.functional.normalize(text_features.unsqueeze(<span class="hljs-number">0</span>), dim=<span class="hljs-number">2</span>)<br>    dists = input_normed.sub(embed_normed).norm(dim=<span class="hljs-number">2</span>).div(<span class="hljs-number">2</span>).arcsin().<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mul(<span class="hljs-number">2</span>)  <span class="hljs-comment"># Squared Great Circle Distance</span><br>    <span class="hljs-keyword">return</span> dists.mean()<br>    <br><span class="hljs-comment"># ========================================</span><br><br>prompt = <span class="hljs-string">&quot;Red Rose (still life), red flower painting&quot;</span>  <span class="hljs-comment"># @param</span><br><br><span class="hljs-comment"># Explore changing this</span><br>guidance_scale = <span class="hljs-number">8</span>  <span class="hljs-comment"># @param</span><br>n_cuts = <span class="hljs-number">4</span>  <span class="hljs-comment"># @param</span><br><br><span class="hljs-comment"># More steps -&gt; more time for the guidance to have an effect</span><br>scheduler.set_timesteps(<span class="hljs-number">50</span>)<br><br><span class="hljs-comment"># We embed a prompt with CLIP as our target</span><br>text = open_clip.tokenize([prompt]).to(device)<br><span class="hljs-keyword">with</span> torch.no_grad(), torch.cuda.amp.autocast():<br>    text_features = clip_model.encode_text(text)<br><br><br>x = torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>).to(device)  <span class="hljs-comment"># RAM usage is high, you may want only 1 image at a time</span><br><br><span class="hljs-keyword">for</span> i, t <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">enumerate</span>(scheduler.timesteps)):<br><br>    model_input = scheduler.scale_model_input(x, t)<br><br>    <span class="hljs-comment"># predict the noise residual</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        noise_pred = image_pipe.unet(model_input, t)[<span class="hljs-string">&quot;sample&quot;</span>]<br><br>    cond_grad = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">for</span> cut <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_cuts):<br><br>        <span class="hljs-comment"># Set requires grad on x</span><br>        x = x.detach().requires_grad_()<br><br>        <span class="hljs-comment"># Get the predicted x0:</span><br>        x0 = scheduler.step(noise_pred, t, x).pred_original_sample<br><br>        <span class="hljs-comment"># Calculate loss</span><br>        loss = clip_loss(x0, text_features) * guidance_scale<br><br>        <span class="hljs-comment"># Get gradient (scale by n_cuts since we want the average)</span><br>        cond_grad -= torch.autograd.grad(loss, x)[<span class="hljs-number">0</span>] / n_cuts<br><br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">25</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step:&quot;</span>, i, <span class="hljs-string">&quot;, Guidance loss:&quot;</span>, loss.item())<br><br>    <span class="hljs-comment"># Modify x based on this gradient</span><br>    alpha_bar = scheduler.alphas_cumprod[i]<br>    x = x.detach() + cond_grad * alpha_bar.sqrt()  <span class="hljs-comment"># Note the additional scaling factor here!</span><br><br>    <span class="hljs-comment"># Now step with scheduler</span><br>    x = scheduler.step(noise_pred, t, x).prev_sample<br><br><br>grid = torchvision.utils.make_grid(x.detach(), nrow=<span class="hljs-number">4</span>)<br>im = grid.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>).cpu().clip(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) * <span class="hljs-number">0.5</span> + <span class="hljs-number">0.5</span><br>Image.fromarray(np.array(im * <span class="hljs-number">255</span>).astype(np.uint8))<br></code></pre></td></tr></table></figure><ul><li>If you check out <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/EleutherAI/clip-guided-diffusion/blob/main/app.py">some code for CLIP-guided diffusion in practice</a>, youâ€™ll see a more complex approach with a better class for picking random cutouts from the images and lots of additional tweaks to the loss function for better performance.</li><li>Before text-conditioned diffusion models came along, this was the best text-to-image system there was! <strong>Our little toy version here has lots of room to improve, but it captures the core idea: thanks to guidance plus the amazing capabilities of CLIP, we can add text control to an unconditional diffusion model</strong> ğŸ¨.</li></ul><h1 id="Class-Conditioned-DM"><a href="#Class-Conditioned-DM" class="headerlink" title="Class-Conditioned DM"></a>Class-Conditioned DM</h1><p>As mentioned in the introduction to this unit, this is just one of many ways we could add additional conditioning information to a diffusion model, and has been chosen for its relative simplicity.</p><h2 id="Creating-a-Class-Conditioned-UNet"><a href="#Creating-a-Class-Conditioned-UNet" class="headerlink" title="Creating a Class-Conditioned UNet"></a>Creating a Class-Conditioned UNet</h2><p>The way weâ€™ll feed in the class conditioning is as follows:</p><ul><li>Create a standardÂ <code>UNet2DModel</code>Â with <strong>some additional input channels</strong></li><li>Map the class label to a learned vector of shapeÂ <code>(class_emb_size)</code>via an <strong>embedding layer</strong></li><li>Concatenate this information as extra channels for the internal UNet input withÂ <code>net_input = torch.cat((x, class_cond), 1)</code></li><li>Feed thisÂ <code>net_input</code>Â (which has (<code>class_emb_size+1</code>) channels in total) into the UNet to get the final prediction</li></ul><p>In this exampleÂ Iâ€™veÂ set the class_emb_size to 4, but this is completely arbitrary and you could explore having itÂ sizeÂ 1Â (to see if it still works),Â sizeÂ 10 (to match the number of classes), or replacing the learned nn.Embedding with a simple one-hot encoding of the class label directly.</p><p>This is what the implementation looks like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ClassConditionedUnet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">10</span>, class_emb_size=<span class="hljs-number">4</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-comment"># The embedding layer will map the class label to a vector of size class_emb_size</span><br>        <span class="hljs-variable language_">self</span>.class_emb = nn.Embedding(num_classes, class_emb_size)<br><br>        <span class="hljs-comment"># Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)</span><br>        <span class="hljs-variable language_">self</span>.model = UNet2DModel(<br>            sample_size=<span class="hljs-number">28</span>,  <span class="hljs-comment"># the target image resolution</span><br>            in_channels=<span class="hljs-number">1</span> + class_emb_size,  <span class="hljs-comment"># Additional input channels for class cond.</span><br>            out_channels=<span class="hljs-number">1</span>,  <span class="hljs-comment"># the number of output channels</span><br>            layers_per_block=<span class="hljs-number">2</span>,  <span class="hljs-comment"># how many ResNet layers to use per UNet block</span><br>            block_out_channels=(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>),<br>            down_block_types=(<br>                <span class="hljs-string">&quot;DownBlock2D&quot;</span>,  <span class="hljs-comment"># a regular ResNet downsampling block</span><br>                <span class="hljs-string">&quot;AttnDownBlock2D&quot;</span>,  <span class="hljs-comment"># a ResNet downsampling block with spatial self-attention</span><br>                <span class="hljs-string">&quot;AttnDownBlock2D&quot;</span>,<br>            ),<br>            up_block_types=(<br>                <span class="hljs-string">&quot;AttnUpBlock2D&quot;</span>,<br>                <span class="hljs-string">&quot;AttnUpBlock2D&quot;</span>,  <span class="hljs-comment"># a ResNet upsampling block with spatial self-attention</span><br>                <span class="hljs-string">&quot;UpBlock2D&quot;</span>,  <span class="hljs-comment"># a regular ResNet upsampling block</span><br>            ),<br>        )<br><br>    <span class="hljs-comment"># Our forward method now takes the class labels as an additional argument</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, t, class_labels</span>):<br>        <span class="hljs-comment"># Shape of x:</span><br>        bs, ch, w, h = x.shape<br><br>        <span class="hljs-comment"># class conditioning in right shape to add as additional input channels</span><br>        class_cond = <span class="hljs-variable language_">self</span>.class_emb(class_labels) <span class="hljs-comment"># å¾—åˆ°çš„å½¢çŠ¶ä¸º(bs,emb_size)</span><br>        class_cond = class_cond.view(bs, class_cond.shape[<span class="hljs-number">1</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>).expand(bs, class_cond.shape[<span class="hljs-number">1</span>], w, h)<br>        <span class="hljs-comment"># x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)</span><br><br>        <span class="hljs-comment"># Net input is now x and class cond concatenated together along dimension 1</span><br>        net_input = torch.cat((x, class_cond), <span class="hljs-number">1</span>)  <span class="hljs-comment"># (bs, 5, 28, 28)</span><br><br>        <span class="hljs-comment"># Feed this to the UNet alongside the timestep and return the prediction</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.model(net_input, t).sample  <span class="hljs-comment"># (bs, 1, 28, 28)</span><br></code></pre></td></tr></table></figure><h2 id="Training-and-Sampling"><a href="#Training-and-Sampling" class="headerlink" title="Training and Sampling"></a>Training and Sampling</h2><p>Where previously weâ€™dÂ do something likeÂ <code>prediction = unet(x, t)</code>Â weâ€™ll now add the correct labels as a third argument (<code>prediction = unet(x, t, y)</code>) during training, and <strong>at inference we can pass whatever labels we want</strong> and if all goes well the model should generate images that match.Â <code>y</code>Â in this case is the labels of the MNIST digits, with values from 0 to 9.</p><p>The training loop is very similar to theÂ <a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusion-models-class/blob/unit2/unit1/02_diffusion_models_from_scratch.ipynb">example from UnitÂ 1</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># @markdown Training loop (10 Epochs):</span><br><br><span class="hljs-comment"># Redefining the dataloader to set the batch size higher than the demo of 8</span><br>train_dataloader = DataLoader(dataset, batch_size=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># How many runs through the data should we do?</span><br>n_epochs = <span class="hljs-number">10</span><br><br><span class="hljs-comment"># Our network</span><br>net = ClassConditionedUnet().to(device)<br><br><span class="hljs-comment"># Our loss function</span><br>loss_fn = nn.MSELoss()<br><br><span class="hljs-comment"># The optimizer</span><br>opt = torch.optim.Adam(net.parameters(), lr=<span class="hljs-number">1e-3</span>)<br><br><span class="hljs-comment"># Keeping a record of the losses for later viewing</span><br>losses = []<br><br><span class="hljs-comment"># The training loop</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> tqdm(train_dataloader):<br><br>        <span class="hljs-comment"># Get some data and prepare the corrupted version</span><br>        x = x.to(device) * <span class="hljs-number">2</span> - <span class="hljs-number">1</span>  <span class="hljs-comment"># Data on the GPU (mapped to (-1, 1))</span><br>        y = y.to(device)<br>        noise = torch.randn_like(x)<br>        timesteps = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">999</span>, (x.shape[<span class="hljs-number">0</span>],)).long().to(device)<br>        noisy_x = noise_scheduler.add_noise(x, noise, timesteps)<br><br>        <span class="hljs-comment"># Get the model prediction</span><br>        pred = net(noisy_x, timesteps, y)  <span class="hljs-comment"># åˆ«çš„éƒ½ä¸€æ ·ï¼Œåªæ˜¯è¿™é‡ŒæŠŠç±»åˆ«å€¼ä¼ è¿›äº†unet</span><br><br>        <span class="hljs-comment"># Calculate the loss</span><br>        loss = loss_fn(pred, noise)  <span class="hljs-comment"># How close is the output to the noise</span><br><br>        <span class="hljs-comment"># Backprop and update the params:</span><br>        opt.zero_grad()<br>        loss.backward()<br>        opt.step()<br><br>        <span class="hljs-comment"># Store the loss for later</span><br>        losses.append(loss.item())<br><br>    <span class="hljs-comment"># Print out the average of the last 100 loss values to get an idea of progress:</span><br>    avg_loss = <span class="hljs-built_in">sum</span>(losses[-<span class="hljs-number">100</span>:]) / <span class="hljs-number">100</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Finished epoch <span class="hljs-subst">&#123;epoch&#125;</span>. Average of the last 100 loss values: <span class="hljs-subst">&#123;avg_loss:05f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># View the loss curve</span><br>plt.plot(losses)<br></code></pre></td></tr></table></figure><h1 id="Stable-Diffusion-æ¦‚è§ˆ"><a href="#Stable-Diffusion-æ¦‚è§ˆ" class="headerlink" title="Stable Diffusion æ¦‚è§ˆ"></a>Stable Diffusion æ¦‚è§ˆ</h1><p>In this unit you will meet a powerful diffusion model called <strong>Stable Diffusion (SD)</strong> and explore what it can do. Stable Diffusion is a powerful <strong>text-conditioned latent diffusion model</strong>.</p><h2 id="Latent-Diffusion"><a href="#Latent-Diffusion" class="headerlink" title="Latent Diffusion"></a>Latent Diffusion</h2><p>As imageÂ sizeÂ grows, so does the <strong>computational power</strong> required to work with those images. This is especially pronounced in an operation called <strong>self-attention, where the amount of operations grows quadratically with the number of inputs</strong>. A 128px square image has 4x as many pixels as a 64px square image, and so requires 16x (i.e. 42) the memory and compute in a self-attention layer. This is a problem for anyone whoâ€™dÂ like to generate high-resolution images!</p><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251115130119810.png" srcset="/img/loading.gif" lazyload alt="image.png"></p><p>Latent diffusion helps to mitigate this issue by <strong>using a separate model called a Variational Auto-Encoder</strong> (<code>VAE</code>) toÂ <strong>compress</strong>Â images to <strong>a smaller spatial dimension</strong>. The rationale behind this is that images tend to contain a large amount of redundant information - given enough training data, <strong>a VAE can hopefully learn to produce a much smaller representation of an input image and then reconstruct the image based on this smallÂ latentÂ representation with a high degree of fidelity.</strong> The VAE used in SD takes in 3-channel images and produces a 4-channel latent representation with a reduction factor of 8 for each spatial dimension. That is, a 512px square input image will be compressed down to a 4x64x64 latent.</p><p>By applying the diffusion process on theseÂ <strong>latent representations</strong>Â rather than on full-resolution images, we can get many of the benefits that would come from using smaller images (<strong>lower memory usage, fewer layers needed in the UNet, faster generation timesâ€¦</strong>) and still decode the result back to a high-resolution image once weâ€™re ready to view the final result. <strong>This innovation dramatically lowers the cost to train and run these models.</strong></p><h2 id="Text-Conditioning"><a href="#Text-Conditioning" class="headerlink" title="Text Conditioning"></a><a target="_blank" rel="noopener" href="https://huggingface.co/learn/diffusion-course/en/unit3/1#text-conditioning"></a>Text Conditioning</h2><p>In Unit 2 we showed how feeding additional information to the UNet allows us to have some additional control over the types of images generated. We call this conditioning. Given a noisy version of an image, the model is tasked with predicting the denoised versionÂ <strong>based on additional clues</strong>Â such as a class label or, in the case of Stable Diffusion, a text description of the image. At inference time, we can feed in the description of an image weâ€™dÂ like to see and some pure noise as a starting point, and the model does its best to â€˜denoiseâ€™ the random input into something that matches the caption.</p><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251115130440936.png" srcset="/img/loading.gif" lazyload alt="image.png"><br><em>Diagram showing the text encoding process which transforms the input prompt into a set of text embeddings (the encoder_hidden_states) which can then be fed in as conditioning to the UNet.</em></p><p>For this to work, we need to <strong>create a numeric representation of the text</strong> that captures relevant information about what it describes. To do this, SD <strong>leverages a pre-trained transformer model based on something called CLIP.</strong><br><strong>CLIPâ€™sÂ text encoder was designed to process image captions into a form that could be used to compare images and text</strong>, so it is well suited to the task of creating useful representations from image descriptions. An input prompt is first tokenized (based on a large <strong>vocabulary</strong> where each word or sub-word is assigned a specific token) and then fed through the CLIP text encoder, producing a 768-dimensional (in the case of SDÂ 1.X) or <strong>1024-dimensional</strong> (SD 2.X) vector for each token. To keep things consistent <strong>prompts are always padded&#x2F;truncated to be 77 tokens long</strong>, and so the final representation which we use as <strong>conditioning is a tensor of shape 77x1024 per prompt</strong>.</p><p><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20251115130734519.png" srcset="/img/loading.gif" lazyload alt="image.png"></p><p>OK, so how do we actually <strong>feed this conditioning information into the UNet</strong> for it to use as it makes predictions? The answer is something called &#x3D;&#x3D;cross-attention&#x3D;&#x3D;. Scattered throughout the UNet are cross-attention layers. Each spatial location in the UNet can â€˜attendâ€™ to different tokens in the text conditioning, bringing in relevant information from the prompt. <strong>The diagram above shows how this text conditioning (as well as timestep-based conditioning) is fed in at different points</strong>. As you can see, at every level the UNet has ample opportunity to make use of this conditioning!</p><h2 id="Classifier-free-Guidance"><a href="#Classifier-free-Guidance" class="headerlink" title="Classifier-free Guidance"></a>Classifier-free Guidance</h2><p>ç ”ç©¶å‘ç°ï¼Œå³ä¾¿æˆ‘ä»¬ç«­å°½å…¨åŠ›ä¼˜åŒ–æ–‡æœ¬æ¡ä»¶çš„æ•ˆæœï¼Œæ¨¡å‹åœ¨é¢„æµ‹æ—¶ä»å€¾å‘äº<strong>ä¸»è¦ä¾èµ–å«å™ªè¾“å…¥å›¾åƒè€Œéæç¤ºè¯</strong>ã€‚æŸç§ç¨‹åº¦ä¸Šè¿™å¯ä»¥ç†è§£â€”â€”è®¸å¤šå›¾åƒè¯´æ˜æ–‡å­—ä¸å…¶å¯¹åº”å›¾ç‰‡å…³è”æ€§è¾ƒå¼±ï¼Œå› æ­¤æ¨¡å‹å­¦ä¼šäº†ä¸è¿‡åº¦ä¾èµ–æ–‡å­—æè¿°ï¼ä½†åœ¨ç”Ÿæˆæ–°å›¾åƒæ—¶ï¼Œè¿™ç§è¡Œä¸ºå°±ä¸å¯å–äº†â€”â€”å¦‚æœæ¨¡å‹ä¸éµå¾ªæç¤ºè¯ï¼Œæˆ‘ä»¬å¯èƒ½å¾—åˆ°ä¸æè¿°å®Œå…¨æ— å…³çš„å›¾åƒã€‚</p><p>To fix this, we use a trick called <strong>Classifier-Free Guidance</strong> (<code>CGF</code>). <strong>During training, text conditioning is sometimes kept blank, forcing the model to learn to denoise images with no text information</strong> whatsoever (unconditional generation). Then at inference time, we make two separate predictions: one with the text prompt as conditioning and one without. We can then use the difference between these two predictions to create a final combined prediction that pushesÂ <strong>even further</strong>Â in the direction indicated by the text-conditioned prediction according to <strong>some scaling factor (the guidance scale)</strong>, hopefully resulting in an image that better matches the prompt. The image above shows the outputs for a prompt at different guidance scales - as you can see, higher values result in images that better match the description.</p><ul><li>ç±»ä¼¼ dropout ,ç„¶åé€šè¿‡ä¸€ä¸ªç³»æ•°æ¥æ§åˆ¶guidanceçš„å¼ºåº¦</li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/ML-DL/" class="category-chain-item">ML&amp;DL</a></span></span></div></div><div class="license-box my-3"><div class="license-title"><div>Diifusion-Conditional</div><div>http://pzhwuhu.github.io/2025/11/15/Diifusion-Conditional/</div></div><div class="license-meta"><div class="license-meta-item"><div>æœ¬æ–‡ä½œè€…</div><div>pzhwuhu</div></div><div class="license-meta-item license-meta-date"><div>å‘å¸ƒäº</div><div>2025å¹´11æœˆ15æ—¥</div></div><div class="license-meta-item license-meta-date"><div>æ›´æ–°äº</div><div>2025å¹´11æœˆ15æ—¥</div></div><div class="license-meta-item"><div>è®¸å¯åè®®</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - ç½²å"><i class="iconfont icon-cc-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - éå•†ä¸šæ€§ä½¿ç”¨"><i class="iconfont icon-cc-nc"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - ç›¸åŒæ–¹å¼å…±äº«"><i class="iconfont icon-cc-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/2025/11/13/Diffusion-%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/" title="Diffusion-åŸç†è§£æ"><span class="hidden-mobile">Diffusion-åŸç†è§£æ</span> <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script>Fluid.utils.loadComments("#valine",(function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",(function(){var i=Object.assign({appId:"cp3oK6SJdvk7QCx0TiMpNSdI-gzGzoHsz",appKey:"B9u5rSWtXwzrQl56fuCE0o9M",path:"window.location.pathname",placeholder:"ç•™ä¸‹ä½ çš„è¶³è¿¹å§ï¼Œè¶…å¤šemojiå¯ç”¨å™¢ï¼Œgravataré‚®ç®±æˆ–qqé‚®ç®±è‡ªåŠ¨è·å–å¤´åƒ",avatar:"retro",meta:["nick","mail","link"],requiredFields:["nick"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://valine-emoji.bili33.top/",emojiMaps:{bilibilitv2:"bilibilitv/[tv_doge].png",bilibilitv3:"bilibilitv/[tv_äº²äº²].png",bilibilitv4:"bilibilitv/[tv_å·ç¬‘].png",bilibilitv5:"bilibilitv/[tv_å†è§].png",bilibilitv6:"bilibilitv/[tv_å†·æ¼ ].png",bilibilitv7:"bilibilitv/[tv_å‘æ€’].png",bilibilitv8:"bilibilitv/[tv_å‘è´¢].png",bilibilitv9:"bilibilitv/[tv_å¯çˆ±].png",bilibilitv10:"bilibilitv/[tv_åè¡€].png",bilibilitv11:"bilibilitv/[tv_å‘†].png",bilibilitv12:"bilibilitv/[tv_å‘•å].png",bilibilitv13:"bilibilitv/[tv_å›°].png",bilibilitv14:"bilibilitv/[tv_åç¬‘].png",bilibilitv15:"bilibilitv/[tv_å¤§ä½¬].png",bilibilitv16:"bilibilitv/[tv_å¤§å“­].png",bilibilitv17:"bilibilitv/[tv_å§”å±ˆ].png",bilibilitv18:"bilibilitv/[tv_å®³ç¾].png",bilibilitv19:"bilibilitv/[tv_å°´å°¬].png",bilibilitv20:"bilibilitv/[tv_å¾®ç¬‘].png",bilibilitv21:"bilibilitv/[tv_æ€è€ƒ].png",bilibilitv22:"bilibilitv/[tv_æƒŠå“].png",bilibilitv23:"bilibilitv/[tv_æ‰“è„¸].png",bilibilitv24:"bilibilitv/[tv_æŠ“ç‹‚].png",bilibilitv25:"bilibilitv/[tv_æŠ é¼»].png",bilibilitv26:"bilibilitv/[tv_æ–œçœ¼ç¬‘].png",bilibilitv27:"bilibilitv/[tv_æ— å¥ˆ].png",bilibilitv28:"bilibilitv/[tv_æ™•].png",bilibilitv29:"bilibilitv/[tv_æµæ±—].png",bilibilitv30:"bilibilitv/[tv_æµæ³ª].png",bilibilitv31:"bilibilitv/[tv_æµé¼»è¡€].png",bilibilitv32:"bilibilitv/[tv_ç‚¹èµ].png",bilibilitv33:"bilibilitv/[tv_ç”Ÿæ°”].png",bilibilitv34:"bilibilitv/[tv_ç”Ÿç—…].png",bilibilitv35:"bilibilitv/[tv_ç–‘é—®].png",bilibilitv36:"bilibilitv/[tv_ç™½çœ¼].png",bilibilitv37:"bilibilitv/[tv_çš±çœ‰].png",bilibilitv38:"bilibilitv/[tv_ç›®çªå£å‘†].png",bilibilitv39:"bilibilitv/[tv_ç¡ç€].png",bilibilitv40:"bilibilitv/[tv_ç¬‘å“­].png",bilibilitv41:"bilibilitv/[tv_è…¼è…†].png",bilibilitv42:"bilibilitv/[tv_è‰²].png",bilibilitv43:"bilibilitv/[tv_è°ƒä¾ƒ].png",bilibilitv44:"bilibilitv/[tv_è°ƒçš®].png",bilibilitv45:"bilibilitv/[tv_é„™è§†].png",bilibilitv46:"bilibilitv/[tv_é—­å˜´].png",bilibilitv47:"bilibilitv/[tv_éš¾è¿‡].png",bilibilitv48:"bilibilitv/[tv_é¦‹].png",bilibilitv49:"bilibilitv/[tv_é¬¼è„¸].png",bilibilitv50:"bilibilitv/[tv_é»‘äººé—®å·].png",bilibilitv51:"bilibilitv/[tv_é¼“æŒ].png",bilibili22332:"bilibili2233/[2233å¨˜_å–èŒ].png",bilibili22333:"bilibili2233/[2233å¨˜_åƒæƒŠ].png",bilibili22334:"bilibili2233/[2233å¨˜_åé­‚].png",bilibili22335:"bilibili2233/[2233å¨˜_å–æ°´].png",bilibili22336:"bilibili2233/[2233å¨˜_å›°æƒ‘].png",bilibili22337:"bilibili2233/[2233å¨˜_å¤§å“­].png",bilibili22338:"bilibili2233/[2233å¨˜_å¤§ç¬‘].png",bilibili22339:"bilibili2233/[2233å¨˜_å§”å±ˆ].png",bilibili223310:"bilibili2233/[2233å¨˜_æ€’].png",bilibili223311:"bilibili2233/[2233å¨˜_æ— è¨€].png",bilibili223312:"bilibili2233/[2233å¨˜_æ±—].png",bilibili223313:"bilibili2233/[2233å¨˜_ç–‘é—®].png",bilibili223314:"bilibili2233/[2233å¨˜_ç¬¬ä¸€].png",bilibili223315:"bilibili2233/[2233å¨˜_è€¶].png",bilibili223316:"bilibili2233/[2233å¨˜_éƒé—·].png","Tieba-New2":"Tieba-New/image_emoticon.png","Tieba-New3":"Tieba-New/image_emoticon10.png","Tieba-New4":"Tieba-New/image_emoticon100.png","Tieba-New14":"Tieba-New/image_emoticon11.png","Tieba-New31":"Tieba-New/image_emoticon13.png","Tieba-New32":"Tieba-New/image_emoticon14.png","Tieba-New33":"Tieba-New/image_emoticon15.png","Tieba-New34":"Tieba-New/image_emoticon16.png","Tieba-New35":"Tieba-New/image_emoticon17.png","Tieba-New36":"Tieba-New/image_emoticon18.png","Tieba-New37":"Tieba-New/image_emoticon19.png","Tieba-New38":"Tieba-New/image_emoticon2.png","Tieba-New39":"Tieba-New/image_emoticon20.png","Tieba-New40":"Tieba-New/image_emoticon21.png","Tieba-New41":"Tieba-New/image_emoticon22.png","Tieba-New42":"Tieba-New/image_emoticon23.png","Tieba-New43":"Tieba-New/image_emoticon24.png","Tieba-New44":"Tieba-New/image_emoticon25.png","Tieba-New45":"Tieba-New/image_emoticon26.png","Tieba-New46":"Tieba-New/image_emoticon27.png","Tieba-New47":"Tieba-New/image_emoticon28.png","Tieba-New48":"Tieba-New/image_emoticon29.png","Tieba-New49":"Tieba-New/image_emoticon3.png","Tieba-New50":"Tieba-New/image_emoticon30.png","Tieba-New51":"Tieba-New/image_emoticon31.png","Tieba-New52":"Tieba-New/image_emoticon32.png","Tieba-New53":"Tieba-New/image_emoticon33.png","Tieba-New54":"Tieba-New/image_emoticon34.png","Tieba-New55":"Tieba-New/image_emoticon35.png","Tieba-New56":"Tieba-New/image_emoticon36.png","Tieba-New57":"Tieba-New/image_emoticon37.png","Tieba-New58":"Tieba-New/image_emoticon38.png","Tieba-New59":"Tieba-New/image_emoticon39.png","Tieba-New60":"Tieba-New/image_emoticon4.png","Tieba-New61":"Tieba-New/image_emoticon40.png","Tieba-New62":"Tieba-New/image_emoticon41.png","Tieba-New63":"Tieba-New/image_emoticon42.png","Tieba-New64":"Tieba-New/image_emoticon43.png","Tieba-New65":"Tieba-New/image_emoticon44.png","Tieba-New66":"Tieba-New/image_emoticon45.png","Tieba-New67":"Tieba-New/image_emoticon46.png","Tieba-New68":"Tieba-New/image_emoticon47.png","Tieba-New69":"Tieba-New/image_emoticon48.png","Tieba-New70":"Tieba-New/image_emoticon49.png","Tieba-New71":"Tieba-New/image_emoticon5.png","Tieba-New72":"Tieba-New/image_emoticon50.png","Tieba-New73":"Tieba-New/image_emoticon6.png","Tieba-New74":"Tieba-New/image_emoticon66.png","Tieba-New75":"Tieba-New/image_emoticon67.png","Tieba-New76":"Tieba-New/image_emoticon68.png","Tieba-New77":"Tieba-New/image_emoticon69.png","Tieba-New78":"Tieba-New/image_emoticon7.png","Tieba-New79":"Tieba-New/image_emoticon70.png","Tieba-New80":"Tieba-New/image_emoticon71.png","Tieba-New81":"Tieba-New/image_emoticon72.png","Tieba-New82":"Tieba-New/image_emoticon73.png","Tieba-New83":"Tieba-New/image_emoticon74.png","Tieba-New84":"Tieba-New/image_emoticon75.png","Tieba-New85":"Tieba-New/image_emoticon76.png","Tieba-New86":"Tieba-New/image_emoticon77.png","Tieba-New87":"Tieba-New/image_emoticon78.png","Tieba-New88":"Tieba-New/image_emoticon79.png","Tieba-New89":"Tieba-New/image_emoticon8.png","Tieba-New90":"Tieba-New/image_emoticon80.png","Tieba-New91":"Tieba-New/image_emoticon81.png","Tieba-New92":"Tieba-New/image_emoticon82.png","Tieba-New93":"Tieba-New/image_emoticon83.png","Tieba-New94":"Tieba-New/image_emoticon84.png","Tieba-New95":"Tieba-New/image_emoticon85.png","Tieba-New96":"Tieba-New/image_emoticon86.png","Tieba-New97":"Tieba-New/image_emoticon87.png","Tieba-New98":"Tieba-New/image_emoticon88.png","Tieba-New99":"Tieba-New/image_emoticon89.png","Tieba-New100":"Tieba-New/image_emoticon9.png","Tieba-New101":"Tieba-New/image_emoticon90.png","Tieba-New102":"Tieba-New/image_emoticon91.png","Tieba-New103":"Tieba-New/image_emoticon92.png","Tieba-New104":"Tieba-New/image_emoticon93.png","Tieba-New105":"Tieba-New/image_emoticon94.png","Tieba-New106":"Tieba-New/image_emoticon95.png","Tieba-New107":"Tieba-New/image_emoticon96.png","Tieba-New108":"Tieba-New/image_emoticon97.png","Tieba-New109":"Tieba-New/image_emoticon98.png","Tieba-New110":"Tieba-New/image_emoticon99.png",weibo2:"weibo/d_aoteman.png",weibo15:"weibo/d_doge.png",weibo16:"weibo/d_erha.png",weibo40:"weibo/d_miao.png",weibo49:"weibo/d_shenshou.png",weibo65:"weibo/d_xiongmao.png",weibo74:"weibo/d_zhutou.png",weibo75:"weibo/d_zuiyou.png",weibo76:"weibo/emoji_0x1f31f.png",weibo77:"weibo/emoji_0x1f349.png",weibo78:"weibo/emoji_0x1f357.png",weibo79:"weibo/emoji_0x1f384.png",weibo80:"weibo/emoji_0x1f44f.png",weibo81:"weibo/emoji_0x1f47b.png",weibo82:"weibo/emoji_0x1f47f.png",weibo83:"weibo/emoji_0x1f48a.png",weibo84:"weibo/emoji_0x1f4a3.png",weibo85:"weibo/emoji_0x1f4a9.png",weibo86:"weibo/emoji_0x1f631.png",weibo87:"weibo/emoji_0x1f643.png",weibo88:"weibo/emoji_0x1f645.png",weibo89:"weibo/emoji_0x1f648.png",weibo90:"weibo/emoji_0x1f649.png",weibo91:"weibo/emoji_0x1f64a.png",weibo92:"weibo/emoji_0x1f64b.png",weibo93:"weibo/emoji_0x1f64f.png",weibo94:"weibo/emoji_0x1f913.png",weibo95:"weibo/emoji_0x1f917.png",weibo96:"weibo/emoji_0x26a1.png",weibo97:"weibo/h_buyao.png",weibo98:"weibo/h_good.png",weibo99:"weibo/h_haha.png",weibo100:"weibo/h_jiayou.png",weibo101:"weibo/h_lai.png",weibo102:"weibo/h_ok.png",weibo103:"weibo/h_quantou.png",weibo105:"weibo/h_woshou.png",weibo106:"weibo/h_ye.png",weibo107:"weibo/h_zan.png",weibo108:"weibo/h_zuoyi.png","HONKAI3-Star1":"HONKAI3-Star/1.gif","HONKAI3-Star2":"HONKAI3-Star/10.gif","HONKAI3-Star3":"HONKAI3-Star/11.gif","HONKAI3-Star4":"HONKAI3-Star/12.gif","HONKAI3-Star5":"HONKAI3-Star/13.gif","HONKAI3-Star6":"HONKAI3-Star/14.gif","HONKAI3-Star7":"HONKAI3-Star/15.gif","HONKAI3-Star8":"HONKAI3-Star/16.gif","HONKAI3-Star9":"HONKAI3-Star/2.gif","HONKAI3-Star10":"HONKAI3-Star/3.gif","HONKAI3-Star11":"HONKAI3-Star/4.gif","HONKAI3-Star12":"HONKAI3-Star/5.gif","HONKAI3-Star13":"HONKAI3-Star/6.gif","HONKAI3-Star14":"HONKAI3-Star/7.gif","HONKAI3-Star15":"HONKAI3-Star/8.gif","HONKAI3-Star16":"HONKAI3-Star/9.gif","HONKAI3-Daily1":"HONKAI3-Daily/1.gif","HONKAI3-Daily2":"HONKAI3-Daily/10.gif","HONKAI3-Daily3":"HONKAI3-Daily/11.gif","HONKAI3-Daily4":"HONKAI3-Daily/12.gif","HONKAI3-Daily5":"HONKAI3-Daily/13.gif","HONKAI3-Daily6":"HONKAI3-Daily/14.gif","HONKAI3-Daily7":"HONKAI3-Daily/15.gif","HONKAI3-Daily8":"HONKAI3-Daily/16.gif","HONKAI3-Daily9":"HONKAI3-Daily/2.gif","HONKAI3-Daily10":"HONKAI3-Daily/3.gif","HONKAI3-Daily11":"HONKAI3-Daily/4.gif","HONKAI3-Daily12":"HONKAI3-Daily/5.gif","HONKAI3-Daily13":"HONKAI3-Daily/6.gif","HONKAI3-Daily14":"HONKAI3-Daily/7.gif","HONKAI3-Daily15":"HONKAI3-Daily/8.gif","HONKAI3-Daily16":"HONKAI3-Daily/9.gif","Tsuri-me-ju_mimi1":"Tsuri-me-ju_mimi/10753776_key@2x.png","Tsuri-me-ju_mimi2":"Tsuri-me-ju_mimi/10753777_key@2x.png","Tsuri-me-ju_mimi3":"Tsuri-me-ju_mimi/10753778_key@2x.png","Tsuri-me-ju_mimi12":"Tsuri-me-ju_mimi/10753787_key@2x.png","Tsuri-me-ju_mimi13":"Tsuri-me-ju_mimi/10753788_key@2x.png","Tsuri-me-ju_mimi14":"Tsuri-me-ju_mimi/10753789_key@2x.png","Tsuri-me-ju_mimi15":"Tsuri-me-ju_mimi/10753790_key@2x.png","Tsuri-me-ju_mimi16":"Tsuri-me-ju_mimi/10753791_key@2x.png","Tsuri-me-ju_mimi36":"Tsuri-me-ju_mimi/10753811_key@2x.png","Tsuri-me-ju_mimi37":"Tsuri-me-ju_mimi/10753812_key@2x.png","Tsuri-me-ju_mimi38":"Tsuri-me-ju_mimi/10753813_key@2x.png","Tsuri-me-ju_mimi40":"Tsuri-me-ju_mimi/10753815_key@2x.png"},enableQQ:!0,avatar_cdn:"https://cravatar.cn/avatar/",visitor:!0},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vemoji",(()=>{const i=document.createElement("style");i.innerHTML="\n            #valine .vemoji {\n              width: 40px !important;  /* æ ¹æ®éœ€è¦è°ƒæ•´å®½åº¦ */\n              height: 40px !important; /* æ ¹æ®éœ€è¦è°ƒæ•´é«˜åº¦ */\n            }\n          ",document.head.appendChild(i)})),Fluid.utils.waitElementVisible("#valine .vcontent",(()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)}))}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>ç›®å½•</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a><div style="font-size:.85rem"><span id="timeDate">è½½å…¥å¤©æ•°...</span> <span id="times">è½½å…¥æ—¶åˆ†ç§’...</span><script src="/js/duration.js"></script></div></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">æ€»è®¿é—®é‡ <span id="leancloud-site-pv"></span> æ¬¡ </span><span id="leancloud-site-uv-container" style="display:none">æ€»è®¿å®¢æ•° <span id="leancloud-site-uv"></span> äºº</span></div><span>85k</span></div></footer><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach((t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")}))},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicRibbon.min.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/star.min.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/love.min.js"></script><script src="//cdn.jsdelivr.net/npm/highlight.js@11.5.1/styles/monokai.min.css.js"></script><script src="/js/scrollAnimation.js"></script><script src="/js/loading.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div></noscript><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>
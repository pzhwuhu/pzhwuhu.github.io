<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="light"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/avatar_1.jpg"><link rel="icon" href="/img/avatar_1.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="pzhwuhu"><meta name="keywords" content=""><meta name="description" content="è¿™æ˜¯æˆ‘å­¦ä¹ Pytorchæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š               You will sometimes see aÂ 1-dimensional tensor called aÂ vector. Likewise, a 2-dimensional tensor is often referred to as aÂ matrix. Anything"><meta property="og:type" content="article"><meta property="og:title" content="Pytorch Tutorial-Tensors"><meta property="og:url" content="http://pzhwuhu.github.io/2025/08/03/Tensors/index.html"><meta property="og:site_name" content="é¹å“¥"><meta property="og:description" content="è¿™æ˜¯æˆ‘å­¦ä¹ Pytorchæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š               You will sometimes see aÂ 1-dimensional tensor called aÂ vector. Likewise, a 2-dimensional tensor is often referred to as aÂ matrix. Anything"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://pzhwuhu.github.io/myimg/%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%9711.jpg"><meta property="article:published_time" content="2025-08-02T17:35:00.000Z"><meta property="article:modified_time" content="2025-08-16T02:05:01.844Z"><meta property="article:author" content="pzhwuhu"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Python"><meta property="article:tag" content="ML"><meta property="article:tag" content="Tensor"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://pzhwuhu.github.io/myimg/%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%9711.jpg"><title>Pytorch Tutorial-Tensors - é¹å“¥</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/custom_dark_mode.css"><link rel="stylesheet" href="/css/scroll_animation.css"><link rel="stylesheet" href="/css/mac.css"><link rel="stylesheet" href="/css/loading.css"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/gradient.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"pzhwuhu.github.io",root:"/",version:"1.9.8",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!1,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:{key:null},google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"cp3oK6SJdvk7QCx0TiMpNSdI-gzGzoHsz",app_key:"B9u5rSWtXwzrQl56fuCE0o9M",server_url:"https://cp3ok6sj.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2024-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>if(!Fluid.ctx.dnt){var _hmt=_hmt||[];!function(){var t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?[object Object]";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()}</script><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:90vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>èº«å¦‚èŠ¥å­ï¼Œå¿ƒè—é¡»å¼¥</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>é¦–é¡µ</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>å½’æ¡£</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>åˆ†ç±»</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>æ ‡ç­¾</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>å…³äº</span></a></li><li class="nav-item"><a class="nav-link" href="/links/" target="_self"><i class="iconfont icon-link-fill"></i> <span>å‹é“¾</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url('/myimg/%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%974.jpg') no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="Pytorch Tutorial-Tensors"></span></div><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> pzhwuhu </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-08-03 01:35" pubdate>2025å¹´8æœˆ3æ—¥ å‡Œæ™¨</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2k å­— </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 17 åˆ†é’Ÿ </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> æ¬¡</span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Pytorch" id="heading-edf7dd0dd9dbc3b022a1ef1fdb134ea1" role="tab" data-toggle="collapse" href="#collapse-edf7dd0dd9dbc3b022a1ef1fdb134ea1" aria-expanded="true">Pytorch <span class="list-group-count">(5)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-edf7dd0dd9dbc3b022a1ef1fdb134ea1" role="tabpanel" aria-labelledby="heading-edf7dd0dd9dbc3b022a1ef1fdb134ea1"><div class="category-post-list"><a href="/2025/08/12/Autograd/" title="Pytorch Tutorial-Autograd" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-Autograd</span> </a><a href="/2025/08/12/Building%20models/" title="Pytorch Tutorial-Building models" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-Building models</span> </a><a href="/2025/08/12/TensorBoard/" title="Pytorch Tutorial-TensorBoard" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-TensorBoard</span> </a><a href="/2025/08/03/Tensors/" title="Pytorch Tutorial-Tensors" class="list-group-item list-group-item-action active"><span class="category-post">Pytorch Tutorial-Tensors</span> </a><a href="/2025/08/12/Training%20Models/" title="Pytorch Tutorial-Training Models" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-Training Models</span></a></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">Pytorch Tutorial-Tensors</h1><div class="markdown-body"><div class="note note-info"><p>è¿™æ˜¯æˆ‘å­¦ä¹ Pytorchæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š</p></div><ul><li>You will sometimes see aÂ 1-dimensional tensor called aÂ <em>vector.</em></li><li>Likewise, a 2-dimensional tensor is often referred to as aÂ <em>matrix.</em></li><li>Anything with more than two dimensions is generally just called a tensor</li></ul><h1 id="Creating-Tensors"><a href="#Creating-Tensors" class="headerlink" title="Creating Tensors"></a>Creating Tensors</h1><ul><li>using factory methods<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(x)) <span class="hljs-comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><br>zeros = torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(zeros)<br><br>ones = torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(ones)<br><br>torch.manual_seed(<span class="hljs-number">1729</span>)<br>random = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(random)<br><br><span class="hljs-comment"># The output</span><br>tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]])<br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br>tensor([[<span class="hljs-number">0.3126</span>, <span class="hljs-number">0.3791</span>, <span class="hljs-number">0.3087</span>],<br>        [<span class="hljs-number">0.0736</span>, <span class="hljs-number">0.4216</span>, <span class="hljs-number">0.0691</span>]])<br></code></pre></td></tr></table></figure></li></ul><h2 id="Random-Tensors-and-Seeding"><a href="#Random-Tensors-and-Seeding" class="headerlink" title="Random Tensors and Seeding"></a>Random Tensors and Seeding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">1729</span>)<br>random1 = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(random1)<br><br>random2 = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(random2)<br><br>torch.manual_seed(<span class="hljs-number">1729</span>)<br>random3 = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(random3)<br><br>random4 = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(random4)<br><br>tensor([[<span class="hljs-number">0.3126</span>, <span class="hljs-number">0.3791</span>, <span class="hljs-number">0.3087</span>],<br>        [<span class="hljs-number">0.0736</span>, <span class="hljs-number">0.4216</span>, <span class="hljs-number">0.0691</span>]])<br>tensor([[<span class="hljs-number">0.2332</span>, <span class="hljs-number">0.4047</span>, <span class="hljs-number">0.2162</span>],<br>        [<span class="hljs-number">0.9927</span>, <span class="hljs-number">0.4128</span>, <span class="hljs-number">0.5938</span>]])<br>tensor([[<span class="hljs-number">0.3126</span>, <span class="hljs-number">0.3791</span>, <span class="hljs-number">0.3087</span>],<br>        [<span class="hljs-number">0.0736</span>, <span class="hljs-number">0.4216</span>, <span class="hljs-number">0.0691</span>]])<br>tensor([[<span class="hljs-number">0.2332</span>, <span class="hljs-number">0.4047</span>, <span class="hljs-number">0.2162</span>],<br>        [<span class="hljs-number">0.9927</span>, <span class="hljs-number">0.4128</span>, <span class="hljs-number">0.5938</span>]])<br></code></pre></td></tr></table></figure><ul><li>å¯ä»¥å‘ç°random1,2 ä¸ 3,4å®Œå…¨ä¸€è‡´ï¼Œéšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­éƒ½æ˜¯1729ï¼Œè¿™æ ·å¯ä»¥ä¿è¯â€œç»“æœå¯å¤ç°â€</li><li>r1ä¸r2çš„å€¼ä¸åŒï¼Œå› ä¸ºç”Ÿæˆr1åéšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€æ›´æ–°äº†ï¼Œè€Œr3ç›¸å½“äºé‡ç½®äº†ï¼Œç±»ä¼¼äºexecve</li></ul><h2 id="Tensor-Shapes"><a href="#Tensor-Shapes" class="headerlink" title="Tensor Shapes"></a>Tensor Shapes</h2><ul><li>Â when youâ€™re performing operations on two or more tensors, they will <strong>need to be of the sameÂ <em>shape</em></strong>Â - that is, having the same number of dimensions and the same number of cells in each dimension. For that, we have theÂ <code>torch.*_like()</code>Â methods:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.empty(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(x.shape)<br><span class="hljs-built_in">print</span>(x)<br><br>empty_like_x = torch.empty_like(x)<br><span class="hljs-built_in">print</span>(empty_like_x.shape)<br><span class="hljs-built_in">print</span>(empty_like_x)<br><br>zeros_like_x = torch.zeros_like(x)<br><span class="hljs-built_in">print</span>(zeros_like_x.shape)<br><span class="hljs-built_in">print</span>(zeros_like_x)<br><br>rand_like_x = torch.rand_like(x)<br><span class="hljs-built_in">print</span>(rand_like_x.shape)<br><span class="hljs-built_in">print</span>(rand_like_x)<br><br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>tensor([[[ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">1.0842e-19</span>,  <span class="hljs-number">4.9628e-26</span>],<br>         [-<span class="hljs-number">2.5250e-29</span>,  <span class="hljs-number">9.8091e-45</span>,  <span class="hljs-number">0.0000e+00</span>]],<br><br>        [[ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>],<br>         [ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>]]])<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>tensor([[[ <span class="hljs-number">2.5776e-33</span>,  <span class="hljs-number">1.4013e-45</span>,         nan],<br>         [ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">1.4013e-45</span>,  <span class="hljs-number">0.0000e+00</span>]],<br><br>        [[ <span class="hljs-number">4.9477e-26</span>, -<span class="hljs-number">3.6902e+19</span>,  <span class="hljs-number">2.6082e-33</span>],<br>         [ <span class="hljs-number">1.4013e-45</span>,  <span class="hljs-number">4.9633e-26</span>, -<span class="hljs-number">8.5920e+09</span>]]])<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>tensor([[[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>         [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]],<br><br>        [[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>         [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]]])<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>tensor([[[<span class="hljs-number">0.6128</span>, <span class="hljs-number">0.1519</span>, <span class="hljs-number">0.0453</span>],<br>         [<span class="hljs-number">0.5035</span>, <span class="hljs-number">0.9978</span>, <span class="hljs-number">0.3884</span>]],<br><br>        [[<span class="hljs-number">0.6929</span>, <span class="hljs-number">0.1703</span>, <span class="hljs-number">0.1384</span>],<br>         [<span class="hljs-number">0.4759</span>, <span class="hljs-number">0.7481</span>, <span class="hljs-number">0.0361</span>]]])<br></code></pre></td></tr></table></figure><ul><li>Below that, we call theÂ <code>.empty_like()</code>,Â <code>.zeros_like()</code>,Â  andÂ <code>.rand_like()</code>Â methods. Using theÂ <code>.shape</code>Â property, we can verify that <strong>each of these methods returns a tensor of identical dimensionality and extent</strong>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">some_integers = torch.tensor((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">13</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>))<br><span class="hljs-built_in">print</span>(some_integers)<br><br>more_integers = torch.tensor(((<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>), [<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>]))<br><span class="hljs-built_in">print</span>(more_integers)<br><br>tensor([ <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">13</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>])<br>tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>],<br>        [<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>]])<br></code></pre></td></tr></table></figure><ul><li>UsingÂ <code>torch.tensor()</code>Â is the most straightforward way to create a tensor if you already have data in a Python tuple or list. As shown above, nesting the collections will result in a multi-dimensional tensor.(åµŒå¥—é›†åˆå°†äº§ç”Ÿå¤šç»´å¼ é‡)<br><em>Note:Â <code>torch.tensor()</code>Â creates a copy of the data.</em></li></ul><h2 id="Tensor-Data-Types"><a href="#Tensor-Data-Types" class="headerlink" title="Tensor Data Types"></a>Tensor Data Types</h2><p>é»˜è®¤æ˜¯<code>float32,</code> Available data types include:</p><ul><li><code>torch.bool</code></li><li><code>torch.int8</code></li><li><code>torch.uint8</code></li><li><code>torch.int16</code></li><li><code>torch.int32</code></li><li><code>torch.int64</code></li><li><code>torch.half</code></li><li><code>torch.float</code></li><li><code>torch.double</code></li><li><code>torch.bfloat</code></li></ul><p>è®¾ç½®æ•°æ®ç±»å‹æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€æ˜¯åœ¨åˆ›å»ºæ—¶è®¾ç½®ï¼ŒäºŒæ˜¯ä½¿ç”¨<code>.to()</code>å‡½æ•°</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=torch.int16)<br><span class="hljs-built_in">print</span>(a)<br><br>b = torch.rand((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=torch.float64) * <span class="hljs-number">20.</span><br><span class="hljs-built_in">print</span>(b)<br><br>c = b.to(torch.int32)<br><span class="hljs-built_in">print</span>(c)<br><br>tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.int16) <span class="hljs-comment"># è¿™é‡Œå¦‚æœä¸æŒ‡å®šæ•°æ®ç±»å‹ä¼šæ˜¾ç¤º 1.</span><br>tensor([[ <span class="hljs-number">0.9956</span>,  <span class="hljs-number">1.4148</span>,  <span class="hljs-number">5.8364</span>],<br>        [<span class="hljs-number">11.2406</span>, <span class="hljs-number">11.2083</span>, <span class="hljs-number">11.6692</span>]], dtype=torch.float64)<br>tensor([[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">5</span>],<br>        [<span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>]], dtype=torch.int32)<br></code></pre></td></tr></table></figure><h1 id="Math-Logic-with-Tensors"><a href="#Math-Logic-with-Tensors" class="headerlink" title="Math &amp; Logic with Tensors"></a>Math &amp; Logic with Tensors</h1><ul><li>å¤§éƒ¨åˆ†è¿ç®—éƒ½å’Œä½ çš„ç›´è§‰å·®ä¸å¤šï¼Œè€Œä¸”æ¯”è¾ƒâ€œèªæ˜â€<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">ones = torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) + <span class="hljs-number">1</span><br>twos = torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) * <span class="hljs-number">2</span><br>threes = (torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) * <span class="hljs-number">7</span> - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span><br>fours = twos ** <span class="hljs-number">2</span><br>sqrt2s = twos ** <span class="hljs-number">0.5</span><br><br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br>tensor([[<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>],<br>        [<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>]])<br>tensor([[<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>],<br>        [<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>]])<br>tensor([[<span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>],<br>        [<span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>]])<br>tensor([[<span class="hljs-number">1.4142</span>, <span class="hljs-number">1.4142</span>],<br>        [<span class="hljs-number">1.4142</span>, <span class="hljs-number">1.4142</span>]])<br>        <br>powers2 = twos ** torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-built_in">print</span>(powers2)<br><br>fives = ones + fours<br><span class="hljs-built_in">print</span>(fives)<br><br>dozens = threes * fours<br><span class="hljs-built_in">print</span>(dozens)<br>tensor([[ <span class="hljs-number">2.</span>,  <span class="hljs-number">4.</span>],<br>        [ <span class="hljs-number">8.</span>, <span class="hljs-number">16.</span>]])<br>tensor([[<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>],<br>        [<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>]])<br>tensor([[<span class="hljs-number">12.</span>, <span class="hljs-number">12.</span>],<br>        [<span class="hljs-number">12.</span>, <span class="hljs-number">12.</span>]])<br></code></pre></td></tr></table></figure></li></ul><h2 id="In-Brief-Tensor-Broadcasting"><a href="#In-Brief-Tensor-Broadcasting" class="headerlink" title="In Brief: Tensor Broadcasting"></a>In Brief: Tensor Broadcasting</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">rand = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>)<br>doubled = rand * (torch.ones(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>) * <span class="hljs-number">2</span>)<br><br><span class="hljs-built_in">print</span>(rand)<br><span class="hljs-built_in">print</span>(doubled)<br><br>tensor([[<span class="hljs-number">0.2024</span>, <span class="hljs-number">0.5731</span>, <span class="hljs-number">0.7191</span>, <span class="hljs-number">0.4067</span>],<br>        [<span class="hljs-number">0.7301</span>, <span class="hljs-number">0.6276</span>, <span class="hljs-number">0.7357</span>, <span class="hljs-number">0.0381</span>]])<br>tensor([[<span class="hljs-number">0.4049</span>, <span class="hljs-number">1.1461</span>, <span class="hljs-number">1.4382</span>, <span class="hljs-number">0.8134</span>],<br>        [<span class="hljs-number">1.4602</span>, <span class="hljs-number">1.2551</span>, <span class="hljs-number">1.4715</span>, <span class="hljs-number">0.0762</span>]])<br></code></pre></td></tr></table></figure><p><strong>The rules for broadcasting are:</strong></p><ul><li>Each tensor must have at least one dimension - no empty tensors.</li><li>Comparing the dimensionÂ sizesÂ of the two tensors,Â <em>going from last to first ä»åå¾€å‰:</em><ul><li>Each dimension must be equal,Â <em>or</em></li><li>One of the dimensions must be ofÂ sizeÂ 1,Â <em>or</em></li><li>The dimension does not exist in one of the tensors</li></ul></li><li>some correct examples:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">a =     torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br><br>b = a * torch.rand(   <span class="hljs-number">3</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># 3rd &amp; 2nd dims identical to a, dim 1 absent</span><br><span class="hljs-built_in">print</span>(b)<br><br>c = a * torch.rand(   <span class="hljs-number">3</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># 3rd dim = 1, 2nd dim identical to a</span><br><span class="hljs-built_in">print</span>(c)<br><br>d = a * torch.rand(   <span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># 3rd dim identical to a, 2nd dim = 1</span><br><span class="hljs-built_in">print</span>(d)<br></code></pre></td></tr></table></figure>For more information on broadcasting, see theÂ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/broadcasting.html">PyTorch documentation</a>Â on the topic.</li></ul><h2 id="More-Math-with-Tensors"><a href="#More-Math-with-Tensors" class="headerlink" title="More Math with Tensors"></a>More Math with Tensors</h2><p>This is a small sample of For more details and the full inventory of math functions, have a look at theÂ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#math-operations">documentation</a>.<br>ç»™å‡ºä¸€äº›ç»å…¸ç¤ºä¾‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># common functions</span><br>a = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>) * <span class="hljs-number">2</span> - <span class="hljs-number">1</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Common functions:&#x27;</span>)<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">abs</span>(a))<br><span class="hljs-built_in">print</span>(torch.ceil(a))<br><span class="hljs-built_in">print</span>(torch.floor(a))<br><span class="hljs-built_in">print</span>(torch.clamp(a, -<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>)) <span class="hljs-comment"># `torch.clamp()` æ˜¯ä¸€ä¸ªå¼ é‡è£å‰ªå‡½æ•°ï¼Œç”¨äºé™åˆ¶å¼ é‡ `a` ä¸­çš„å…ƒç´ å€¼åœ¨æŒ‡å®šçš„èŒƒå›´å†… `[min, max]`</span><br><br><span class="hljs-comment"># trigonometric functions and their inverses</span><br>angles = torch.tensor([<span class="hljs-number">0</span>, math.pi / <span class="hljs-number">4</span>, math.pi / <span class="hljs-number">2</span>, <span class="hljs-number">3</span> * math.pi / <span class="hljs-number">4</span>])<br>sines = torch.sin(angles)<br>inverses = torch.asin(sines)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nSine and arcsine:&#x27;</span>)<br><span class="hljs-built_in">print</span>(angles)<br><span class="hljs-built_in">print</span>(sines)<br><span class="hljs-built_in">print</span>(inverses)<br><br><span class="hljs-comment"># bitwise operations</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nBitwise XOR:&#x27;</span>)<br>b = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">11</span>])<br>c = torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">7</span>, <span class="hljs-number">10</span>])<br><span class="hljs-built_in">print</span>(torch.bitwise_xor(b, c))<br><br><span class="hljs-comment"># comparisons:</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nBroadcasted, element-wise equality comparison:&#x27;</span>)<br>d = torch.tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>], [<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>]])<br>e = torch.ones(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># many comparison ops support broadcasting!</span><br><span class="hljs-built_in">print</span>(torch.eq(d, e)) <span class="hljs-comment"># returns a tensor of type bool</span><br><br><span class="hljs-comment"># reductions:</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nReduction ops:&#x27;</span>)<br><span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">max</span>(d))        <span class="hljs-comment"># returns a single-element tensor</span><br><span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">max</span>(d).item()) <span class="hljs-comment"># extracts the value from the returned tensor</span><br><span class="hljs-built_in">print</span>(torch.mean(d))       <span class="hljs-comment"># average</span><br><span class="hljs-built_in">print</span>(torch.std(d))        <span class="hljs-comment"># standard deviation æ ‡å‡†å·®</span><br><span class="hljs-built_in">print</span>(torch.prod(d))       <span class="hljs-comment"># product of all numbers æ‰€æœ‰å…ƒç´ ç›¸ä¹˜</span><br><span class="hljs-built_in">print</span>(torch.unique(torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]))) <span class="hljs-comment"># filter unique elements</span><br><br><span class="hljs-comment"># vector and linear algebra operations</span><br>v1 = torch.tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>])         <span class="hljs-comment"># x unit vector</span><br>v2 = torch.tensor([<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>])         <span class="hljs-comment"># y unit vector</span><br>m1 = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)                   <span class="hljs-comment"># random matrix</span><br>m2 = torch.tensor([[<span class="hljs-number">3.</span>, <span class="hljs-number">0.</span>], [<span class="hljs-number">0.</span>, <span class="hljs-number">3.</span>]]) <span class="hljs-comment"># three times identity matrix</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nVectors &amp; Matrices:&#x27;</span>)<br><span class="hljs-built_in">print</span>(torch.cross(v2, v1)) <span class="hljs-comment"># negative of z unit vector å‰ä¹˜</span><br><span class="hljs-built_in">print</span>(m1)<br>m3 = torch.matmul(m1, m2)  <span class="hljs-comment"># mm: matrix multiplication</span><br><span class="hljs-built_in">print</span>(m3)                  <span class="hljs-comment"># 3 times m1</span><br><span class="hljs-built_in">print</span>(torch.svd(m3))       <span class="hljs-comment"># singular value decomposition,å¥‡å¼‚å€¼åˆ†è§£</span><br><br><br>Common functions:<br>tensor([[ <span class="hljs-number">0.2719</span>,  <span class="hljs-number">0.1760</span>,  <span class="hljs-number">0.8550</span>,  <span class="hljs-number">0.0404</span>],<br>        [-<span class="hljs-number">0.5709</span>,  <span class="hljs-number">0.4227</span>, -<span class="hljs-number">0.9504</span>, -<span class="hljs-number">0.8692</span>]])<br>tensor([[<span class="hljs-number">0.2719</span>, <span class="hljs-number">0.1760</span>, <span class="hljs-number">0.8550</span>, <span class="hljs-number">0.0404</span>],<br>        [<span class="hljs-number">0.5709</span>, <span class="hljs-number">0.4227</span>, <span class="hljs-number">0.9504</span>, <span class="hljs-number">0.8692</span>]])<br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [-<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, -<span class="hljs-number">0.</span>, -<span class="hljs-number">0.</span>]])<br>tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>],<br>        [-<span class="hljs-number">1.</span>,  <span class="hljs-number">0.</span>, -<span class="hljs-number">1.</span>, -<span class="hljs-number">1.</span>]])<br>tensor([[ <span class="hljs-number">0.2719</span>,  <span class="hljs-number">0.1760</span>,  <span class="hljs-number">0.5000</span>,  <span class="hljs-number">0.0404</span>],<br>        [-<span class="hljs-number">0.5000</span>,  <span class="hljs-number">0.4227</span>, -<span class="hljs-number">0.5000</span>, -<span class="hljs-number">0.5000</span>]])<br><br>Sine <span class="hljs-keyword">and</span> arcsine:<br>tensor([<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.7854</span>, <span class="hljs-number">1.5708</span>, <span class="hljs-number">2.3562</span>])<br>tensor([<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.7071</span>, <span class="hljs-number">1.0000</span>, <span class="hljs-number">0.7071</span>]) <span class="hljs-comment"># xæ¯”è¾ƒå°çš„æ—¶å€™å’Œsinxå·®ä¸å¤š</span><br>tensor([<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.7854</span>, <span class="hljs-number">1.5708</span>, <span class="hljs-number">0.7854</span>])<br><br>Bitwise XOR:<br>tensor([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br><br>Broadcasted, element-wise equality comparison:<br>tensor([[ <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>],<br>        [<span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>]])<br><br>Reduction ops:<br>tensor(<span class="hljs-number">4.</span>)<br><span class="hljs-number">4.0</span><br>tensor(<span class="hljs-number">2.5000</span>)<br>tensor(<span class="hljs-number">1.2910</span>)<br>tensor(<span class="hljs-number">24.</span>)<br>tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><br>Vectors &amp; Matrices:<br>tensor([ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>, -<span class="hljs-number">1.</span>])<br>tensor([[<span class="hljs-number">0.8629</span>, <span class="hljs-number">0.6615</span>],<br>        [<span class="hljs-number">0.8881</span>, <span class="hljs-number">0.5365</span>]])<br>tensor([[<span class="hljs-number">2.5888</span>, <span class="hljs-number">1.9844</span>],<br>        [<span class="hljs-number">2.6643</span>, <span class="hljs-number">1.6095</span>]])<br>torch.return_types.svd(<br>U=tensor([[-<span class="hljs-number">0.7236</span>, -<span class="hljs-number">0.6903</span>],<br>        [-<span class="hljs-number">0.6903</span>,  <span class="hljs-number">0.7236</span>]]),<br>S=tensor([<span class="hljs-number">4.5019</span>, <span class="hljs-number">0.2488</span>]),<br>V=tensor([[-<span class="hljs-number">0.8246</span>,  <span class="hljs-number">0.5657</span>],<br>        [-<span class="hljs-number">0.5657</span>, -<span class="hljs-number">0.8246</span>]]))<br></code></pre></td></tr></table></figure><h1 id="Copying-Tensors"><a href="#Copying-Tensors" class="headerlink" title="Copying Tensors"></a>Copying Tensors</h1><ul><li>clone()<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>b = a.clone()<br><br><span class="hljs-keyword">assert</span> b <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> a      <span class="hljs-comment"># different objects in memory...</span><br><span class="hljs-built_in">print</span>(torch.eq(a, b))  <span class="hljs-comment"># ...but still with the same contents!</span><br><br>a[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] = <span class="hljs-number">561</span>          <span class="hljs-comment"># a changes...</span><br><span class="hljs-built_in">print</span>(b)               <span class="hljs-comment"># ...but b is still all ones</span><br></code></pre></td></tr></table></figure></li><li><strong>There is an important thing to be aware of when usingÂ <code>clone()</code>.</strong>Â If your source tensor has autograd, enabled then so will the clone.Â <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, requires_grad=<span class="hljs-literal">True</span>) <span class="hljs-comment"># turn on autograd</span><br><span class="hljs-built_in">print</span>(a)<br><br>b = a.clone()<br><span class="hljs-built_in">print</span>(b)<br><br>c = a.detach().clone()<br><span class="hljs-built_in">print</span>(c)<br><br><span class="hljs-built_in">print</span>(a)<br><br>tensor([[<span class="hljs-number">0.5461</span>, <span class="hljs-number">0.5396</span>],<br>        [<span class="hljs-number">0.3053</span>, <span class="hljs-number">0.1973</span>]], requires_grad=<span class="hljs-literal">True</span>)<br>tensor([[<span class="hljs-number">0.5461</span>, <span class="hljs-number">0.5396</span>],<br>        [<span class="hljs-number">0.3053</span>, <span class="hljs-number">0.1973</span>]], grad_fn=&lt;CloneBackward&gt;)<br>tensor([[<span class="hljs-number">0.5461</span>, <span class="hljs-number">0.5396</span>],<br>        [<span class="hljs-number">0.3053</span>, <span class="hljs-number">0.1973</span>]])<br>tensor([[<span class="hljs-number">0.5461</span>, <span class="hljs-number">0.5396</span>],<br>        [<span class="hljs-number">0.3053</span>, <span class="hljs-number">0.1973</span>]], requires_grad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure></li><li>TheÂ <code>detach()</code>Â methodÂ <em>detaches the tensor from its computation history.</em>Â It says, â€œdo whatever comes next as if autograd was off.â€ It does thisÂ <em>without</em>Â changingÂ <code>a</code>Â - you can see that when we printÂ <code>a</code>Â again at the end, it retains itsÂ <code>requires_grad=True</code>Â property.</li></ul><h1 id="Moving-to-GPU"><a href="#Moving-to-GPU" class="headerlink" title="Moving to GPU"></a>Moving to GPU</h1><p>å¼•å…¥ï¼šOne of the major advantages of PyTorch is its robust acceleration on CUDA-compatible Nvidia GPUs. (â€œCUDAâ€ stands forÂ <em>Compute Unified Device Architecture</em>, which is Nvidiaâ€™s platform for parallel computing.) So far, everything weâ€™ve done has been on CPU. How do we move to the faster hardware?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;We have a GPU!&#x27;</span>)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Sorry, CPU only.&#x27;</span>)<br></code></pre></td></tr></table></figure><p>CPUåœ¨RAMä¸­è¿›è¡Œè®¡ç®—ï¼Œè€ŒGPUæä¾›äº†é™„åŠ å†…å­˜ï¼Œç”¨GPUè®¡ç®—æ—¶éœ€è¦å°†æ•°æ®è¿ç§»åˆ°GPU</p><h2 id="åˆ›å»ºæ—¶è¿ç§»"><a href="#åˆ›å»ºæ—¶è¿ç§»" class="headerlink" title="åˆ›å»ºæ—¶è¿ç§»"></a>åˆ›å»ºæ—¶è¿ç§»</h2><ul><li><p>å¼ é‡é»˜è®¤åˆ›å»ºåœ¨CPUä¸Šï¼Œä½ å¯ä»¥ä½¿ç”¨<code>device</code>æ¥åˆ¶å®šè®¾å¤‡</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    gpu_rand = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(gpu_rand)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Sorry, CPU only.&#x27;</span>)<br></code></pre></td></tr></table></figure></li><li><p>ä¸€ä¸ªå¥½çš„ä¹ æƒ¯æ˜¯ä½¿ç”¨<code>my_device</code>æ¥è¡¨å¾è‡ªå·±çš„è®¾å¤‡ï¼Œè€Œä¸ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    my_device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br><span class="hljs-keyword">else</span>:<br>    my_device = torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Device: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(my_device))<br></code></pre></td></tr></table></figure></li></ul><h2 id="to-è¿ç§»"><a href="#to-è¿ç§»" class="headerlink" title="to()è¿ç§»"></a>to()è¿ç§»</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">y = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>y = y.to(my_device)<br></code></pre></td></tr></table></figure><p><strong>It is important to know that</strong> in order to do computation involving two or more tensors,Â <em>all of the tensors must be on the same device</em>.</p><h1 id="Manipulating-Tensor-Shapes"><a href="#Manipulating-Tensor-Shapes" class="headerlink" title="Manipulating Tensor Shapes"></a>Manipulating Tensor Shapes</h1><p>PyTorch models generally expectÂ <em>batches</em>Â of input.For example, imagine having a model that works on 3 x 226 x 226 images - a 226-pixel square with 3 color channels. When you load and transform it, youâ€™ll get a tensor of shapeÂ <code>(3, 226, 226)</code>. Your model, though, is expecting input of shapeÂ <code>(N, 3, 226, 226)</code>, whereÂ <code>N</code>Â is the number of images in the batch. So how do you make a batch of one?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.rand(<span class="hljs-number">3</span>, <span class="hljs-number">226</span>, <span class="hljs-number">226</span>)<br>b = a.unsqueeze(<span class="hljs-number">0</span>)<br><br><span class="hljs-built_in">print</span>(a.shape)<br><span class="hljs-built_in">print</span>(b.shape)<br><br>torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">226</span>, <span class="hljs-number">226</span>])<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">226</span>, <span class="hljs-number">226</span>])<br></code></pre></td></tr></table></figure><ul><li>TheÂ <code>unsqueeze()</code>Â method adds a dimension of extentÂ 1.Â <code>unsqueeze(0)</code>Â adds it as a new zeroth dimension - now you have a batch of one!</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>)<br><span class="hljs-built_in">print</span>(a.shape)<br><span class="hljs-built_in">print</span>(a)<br><br>b = a.squeeze(<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(b.shape)<br><span class="hljs-built_in">print</span>(b)<br><br>c = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(c.shape)<br><br>d = c.squeeze(<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(d.shape)<br><br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">20</span>])<br>tensor([[<span class="hljs-number">0.4887</span>, <span class="hljs-number">0.8625</span>, <span class="hljs-number">0.6191</span>, <span class="hljs-number">0.9935</span>, <span class="hljs-number">0.1844</span>, <span class="hljs-number">0.6138</span>, <span class="hljs-number">0.6854</span>, <span class="hljs-number">0.0438</span>, <span class="hljs-number">0.0636</span>,<br>         <span class="hljs-number">0.2884</span>, <span class="hljs-number">0.4362</span>, <span class="hljs-number">0.2368</span>, <span class="hljs-number">0.1394</span>, <span class="hljs-number">0.1721</span>, <span class="hljs-number">0.1751</span>, <span class="hljs-number">0.3851</span>, <span class="hljs-number">0.0732</span>, <span class="hljs-number">0.3118</span>,<br>         <span class="hljs-number">0.9180</span>, <span class="hljs-number">0.7293</span>]])<br>torch.Size([<span class="hljs-number">20</span>])<br>tensor([<span class="hljs-number">0.4887</span>, <span class="hljs-number">0.8625</span>, <span class="hljs-number">0.6191</span>, <span class="hljs-number">0.9935</span>, <span class="hljs-number">0.1844</span>, <span class="hljs-number">0.6138</span>, <span class="hljs-number">0.6854</span>, <span class="hljs-number">0.0438</span>, <span class="hljs-number">0.0636</span>,<br>        <span class="hljs-number">0.2884</span>, <span class="hljs-number">0.4362</span>, <span class="hljs-number">0.2368</span>, <span class="hljs-number">0.1394</span>, <span class="hljs-number">0.1721</span>, <span class="hljs-number">0.1751</span>, <span class="hljs-number">0.3851</span>, <span class="hljs-number">0.0732</span>, <span class="hljs-number">0.3118</span>,<br>        <span class="hljs-number">0.9180</span>, <span class="hljs-number">0.7293</span>])<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br></code></pre></td></tr></table></figure><ul><li>å…¶å®å¢åŠ æˆ–å‡å°‘ä¸€ä¸ªèŒƒå›´ä¸º1çš„ç»´åº¦ï¼Œå…¶å®å¯¹æ•°æ®æœ¬èº«å¹¶æ²¡æœ‰å½±å“ï¼Œè¿™ä¹Ÿèƒ½æç¤ºæˆ‘ä»¬ï¼šÂ Calls toÂ <code>squeeze()</code>Â andÂ <code>unsqueeze()</code>Â can only act on dimensions of extentÂ 1Â because to do otherwise would change the number of elements in the tensor.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>b = torch.rand(   <span class="hljs-number">3</span>)     <span class="hljs-comment"># trying to multiply a * b will give a runtime error</span><br>c = b.unsqueeze(<span class="hljs-number">1</span>)       <span class="hljs-comment"># change to a 2-dimensional tensor, adding new dim at the end</span><br><span class="hljs-built_in">print</span>(c.shape)<br><span class="hljs-built_in">print</span>(a * c)             <span class="hljs-comment"># broadcasting works again!</span><br></code></pre></td></tr></table></figure><p>Sometimes youâ€™ll want to change the shape of a tensor more radically, while still preserving the number of elements and their contents. One case where this happens is at the interface between a convolutional layer of a model and a linear layer of the model - this is common in image classification models. A convolution kernel will yield an output tensor of shapeÂ <em>features x width x height,</em>Â but the following linear layer expects aÂ 1-dimensional input.Â <code>reshape()</code>Â will do this for you, provided that the dimensions you request yield the same number of elements as the input tensor has:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">output3d = torch.rand(<span class="hljs-number">6</span>, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>)<br><span class="hljs-built_in">print</span>(output3d.shape)<br><br>input1d = output3d.reshape(<span class="hljs-number">6</span> * <span class="hljs-number">20</span> * <span class="hljs-number">20</span>)<br><span class="hljs-built_in">print</span>(input1d.shape)<br><br><span class="hljs-comment"># can also call it as a method on the torch module:</span><br><span class="hljs-built_in">print</span>(torch.reshape(output3d, (<span class="hljs-number">6</span> * <span class="hljs-number">20</span> * <span class="hljs-number">20</span>,)).shape)<br><br>torch.Size([<span class="hljs-number">6</span>, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>])<br>torch.Size([<span class="hljs-number">2400</span>])<br>torch.Size([<span class="hljs-number">2400</span>])<br></code></pre></td></tr></table></figure><h1 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h1><p>NumPy æœ‰å’Œå¼ é‡æ¯”è¾ƒæƒ³é€šçš„åœ°æ–¹ï¼Œæ¯”å¦‚<code>Broadcast</code>ï¼Œæ‰€ä»¥åœ¨äºŒè€…ä¹‹é—´è½¬æ¢å¾ˆå®¹æ˜“</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>numpy_array = np.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br><span class="hljs-built_in">print</span>(numpy_array)<br><br>pytorch_tensor = torch.from_numpy(numpy_array)<br><span class="hljs-built_in">print</span>(pytorch_tensor)<br><br>[[<span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span>]<br> [<span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span>]]<br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]], dtype=torch.float64)<br></code></pre></td></tr></table></figure><ul><li>PyTorch åˆ›å»ºäº†ä¸€ä¸ªä¸ NumPy æ•°ç»„å…·æœ‰ç›¸åŒå½¢çŠ¶å¹¶åŒ…å«ç›¸åŒæ•°æ®çš„å¼ é‡ï¼Œç”šè‡³ä¿ç•™äº† NumPy é»˜è®¤çš„ 64 ä½æµ®ç‚¹æ•°æ®ç±»å‹ã€‚</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">pytorch_rand = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(pytorch_rand)<br><br>numpy_rand = pytorch_rand.numpy()<br><span class="hljs-built_in">print</span>(numpy_rand)<br></code></pre></td></tr></table></figure><p>è¿™äº›éƒ½ä¸é‡è¦ï¼Œé‡è¦çš„æ˜¯è¦çŸ¥é“<strong>è¿™äº›è½¬æ¢åçš„å¯¹è±¡ä½¿ç”¨ä¸å…¶æºå¯¹è±¡æœ‰ç›¸åŒçš„åº•å±‚å†…å­˜_Â ï¼Œè¿™æ„å‘³ç€å¯¹ä¸€ä¸ªå¯¹è±¡çš„æ›´æ”¹ä¼šåæ˜ åœ¨å¦ä¸€ä¸ªå¯¹è±¡ä¸­</strong>ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy_array[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>] = <span class="hljs-number">23</span><br><span class="hljs-built_in">print</span>(pytorch_tensor)<br><br>pytorch_rand[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>] = <span class="hljs-number">17</span><br><span class="hljs-built_in">print</span>(numpy_rand)<br><br>tensor([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>],<br>        [ <span class="hljs-number">1.</span>, <span class="hljs-number">23.</span>,  <span class="hljs-number">1.</span>]], dtype=torch.float64)<br>[[ <span class="hljs-number">0.5646949</span>   <span class="hljs-number">0.91600937</span>  <span class="hljs-number">0.77828014</span>]<br> [ <span class="hljs-number">0.82769746</span> <span class="hljs-number">17.</span>          <span class="hljs-number">0.6381657</span> ]]<br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Pytorch/" class="category-chain-item">Pytorch</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Pytorch/" class="print-no-link">#Pytorch</a> <a href="/tags/Python/" class="print-no-link">#Python</a> <a href="/tags/ML/" class="print-no-link">#ML</a> <a href="/tags/Tensor/" class="print-no-link">#Tensor</a></div></div><div class="license-box my-3"><div class="license-title"><div>Pytorch Tutorial-Tensors</div><div>http://pzhwuhu.github.io/2025/08/03/Tensors/</div></div><div class="license-meta"><div class="license-meta-item"><div>æœ¬æ–‡ä½œè€…</div><div>pzhwuhu</div></div><div class="license-meta-item license-meta-date"><div>å‘å¸ƒäº</div><div>2025å¹´8æœˆ3æ—¥</div></div><div class="license-meta-item license-meta-date"><div>æ›´æ–°äº</div><div>2025å¹´8æœˆ16æ—¥</div></div><div class="license-meta-item"><div>è®¸å¯åè®®</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - ç½²å"><i class="iconfont icon-cc-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - éå•†ä¸šæ€§ä½¿ç”¨"><i class="iconfont icon-cc-nc"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - ç›¸åŒæ–¹å¼å…±äº«"><i class="iconfont icon-cc-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2025/08/12/Autograd/" title="Pytorch Tutorial-Autograd"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Pytorch Tutorial-Autograd</span> <span class="visible-mobile">ä¸Šä¸€ç¯‡</span></a></article><article class="post-next col-6"><a href="/2025/07/30/TUH%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/" title="TUHæ•°æ®é›†ä¸‹è½½"><span class="hidden-mobile">TUHæ•°æ®é›†ä¸‹è½½</span> <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script>Fluid.utils.loadComments("#valine",(function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",(function(){var i=Object.assign({appId:"cp3oK6SJdvk7QCx0TiMpNSdI-gzGzoHsz",appKey:"B9u5rSWtXwzrQl56fuCE0o9M",path:"window.location.pathname",placeholder:"ç•™ä¸‹ä½ çš„è¶³è¿¹å§ï¼Œè¶…å¤šemojiå¯ç”¨å™¢ï¼Œgravataré‚®ç®±æˆ–qqé‚®ç®±è‡ªåŠ¨è·å–å¤´åƒ",avatar:"retro",meta:["nick","mail","link"],requiredFields:["nick"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://valine-emoji.bili33.top/",emojiMaps:{bilibilitv2:"bilibilitv/[tv_doge].png",bilibilitv3:"bilibilitv/[tv_äº²äº²].png",bilibilitv4:"bilibilitv/[tv_å·ç¬‘].png",bilibilitv5:"bilibilitv/[tv_å†è§].png",bilibilitv6:"bilibilitv/[tv_å†·æ¼ ].png",bilibilitv7:"bilibilitv/[tv_å‘æ€’].png",bilibilitv8:"bilibilitv/[tv_å‘è´¢].png",bilibilitv9:"bilibilitv/[tv_å¯çˆ±].png",bilibilitv10:"bilibilitv/[tv_åè¡€].png",bilibilitv11:"bilibilitv/[tv_å‘†].png",bilibilitv12:"bilibilitv/[tv_å‘•å].png",bilibilitv13:"bilibilitv/[tv_å›°].png",bilibilitv14:"bilibilitv/[tv_åç¬‘].png",bilibilitv15:"bilibilitv/[tv_å¤§ä½¬].png",bilibilitv16:"bilibilitv/[tv_å¤§å“­].png",bilibilitv17:"bilibilitv/[tv_å§”å±ˆ].png",bilibilitv18:"bilibilitv/[tv_å®³ç¾].png",bilibilitv19:"bilibilitv/[tv_å°´å°¬].png",bilibilitv20:"bilibilitv/[tv_å¾®ç¬‘].png",bilibilitv21:"bilibilitv/[tv_æ€è€ƒ].png",bilibilitv22:"bilibilitv/[tv_æƒŠå“].png",bilibilitv23:"bilibilitv/[tv_æ‰“è„¸].png",bilibilitv24:"bilibilitv/[tv_æŠ“ç‹‚].png",bilibilitv25:"bilibilitv/[tv_æŠ é¼»].png",bilibilitv26:"bilibilitv/[tv_æ–œçœ¼ç¬‘].png",bilibilitv27:"bilibilitv/[tv_æ— å¥ˆ].png",bilibilitv28:"bilibilitv/[tv_æ™•].png",bilibilitv29:"bilibilitv/[tv_æµæ±—].png",bilibilitv30:"bilibilitv/[tv_æµæ³ª].png",bilibilitv31:"bilibilitv/[tv_æµé¼»è¡€].png",bilibilitv32:"bilibilitv/[tv_ç‚¹èµ].png",bilibilitv33:"bilibilitv/[tv_ç”Ÿæ°”].png",bilibilitv34:"bilibilitv/[tv_ç”Ÿç—…].png",bilibilitv35:"bilibilitv/[tv_ç–‘é—®].png",bilibilitv36:"bilibilitv/[tv_ç™½çœ¼].png",bilibilitv37:"bilibilitv/[tv_çš±çœ‰].png",bilibilitv38:"bilibilitv/[tv_ç›®çªå£å‘†].png",bilibilitv39:"bilibilitv/[tv_ç¡ç€].png",bilibilitv40:"bilibilitv/[tv_ç¬‘å“­].png",bilibilitv41:"bilibilitv/[tv_è…¼è…†].png",bilibilitv42:"bilibilitv/[tv_è‰²].png",bilibilitv43:"bilibilitv/[tv_è°ƒä¾ƒ].png",bilibilitv44:"bilibilitv/[tv_è°ƒçš®].png",bilibilitv45:"bilibilitv/[tv_é„™è§†].png",bilibilitv46:"bilibilitv/[tv_é—­å˜´].png",bilibilitv47:"bilibilitv/[tv_éš¾è¿‡].png",bilibilitv48:"bilibilitv/[tv_é¦‹].png",bilibilitv49:"bilibilitv/[tv_é¬¼è„¸].png",bilibilitv50:"bilibilitv/[tv_é»‘äººé—®å·].png",bilibilitv51:"bilibilitv/[tv_é¼“æŒ].png",bilibili22332:"bilibili2233/[2233å¨˜_å–èŒ].png",bilibili22333:"bilibili2233/[2233å¨˜_åƒæƒŠ].png",bilibili22334:"bilibili2233/[2233å¨˜_åé­‚].png",bilibili22335:"bilibili2233/[2233å¨˜_å–æ°´].png",bilibili22336:"bilibili2233/[2233å¨˜_å›°æƒ‘].png",bilibili22337:"bilibili2233/[2233å¨˜_å¤§å“­].png",bilibili22338:"bilibili2233/[2233å¨˜_å¤§ç¬‘].png",bilibili22339:"bilibili2233/[2233å¨˜_å§”å±ˆ].png",bilibili223310:"bilibili2233/[2233å¨˜_æ€’].png",bilibili223311:"bilibili2233/[2233å¨˜_æ— è¨€].png",bilibili223312:"bilibili2233/[2233å¨˜_æ±—].png",bilibili223313:"bilibili2233/[2233å¨˜_ç–‘é—®].png",bilibili223314:"bilibili2233/[2233å¨˜_ç¬¬ä¸€].png",bilibili223315:"bilibili2233/[2233å¨˜_è€¶].png",bilibili223316:"bilibili2233/[2233å¨˜_éƒé—·].png","Tieba-New2":"Tieba-New/image_emoticon.png","Tieba-New3":"Tieba-New/image_emoticon10.png","Tieba-New4":"Tieba-New/image_emoticon100.png","Tieba-New14":"Tieba-New/image_emoticon11.png","Tieba-New31":"Tieba-New/image_emoticon13.png","Tieba-New32":"Tieba-New/image_emoticon14.png","Tieba-New33":"Tieba-New/image_emoticon15.png","Tieba-New34":"Tieba-New/image_emoticon16.png","Tieba-New35":"Tieba-New/image_emoticon17.png","Tieba-New36":"Tieba-New/image_emoticon18.png","Tieba-New37":"Tieba-New/image_emoticon19.png","Tieba-New38":"Tieba-New/image_emoticon2.png","Tieba-New39":"Tieba-New/image_emoticon20.png","Tieba-New40":"Tieba-New/image_emoticon21.png","Tieba-New41":"Tieba-New/image_emoticon22.png","Tieba-New42":"Tieba-New/image_emoticon23.png","Tieba-New43":"Tieba-New/image_emoticon24.png","Tieba-New44":"Tieba-New/image_emoticon25.png","Tieba-New45":"Tieba-New/image_emoticon26.png","Tieba-New46":"Tieba-New/image_emoticon27.png","Tieba-New47":"Tieba-New/image_emoticon28.png","Tieba-New48":"Tieba-New/image_emoticon29.png","Tieba-New49":"Tieba-New/image_emoticon3.png","Tieba-New50":"Tieba-New/image_emoticon30.png","Tieba-New51":"Tieba-New/image_emoticon31.png","Tieba-New52":"Tieba-New/image_emoticon32.png","Tieba-New53":"Tieba-New/image_emoticon33.png","Tieba-New54":"Tieba-New/image_emoticon34.png","Tieba-New55":"Tieba-New/image_emoticon35.png","Tieba-New56":"Tieba-New/image_emoticon36.png","Tieba-New57":"Tieba-New/image_emoticon37.png","Tieba-New58":"Tieba-New/image_emoticon38.png","Tieba-New59":"Tieba-New/image_emoticon39.png","Tieba-New60":"Tieba-New/image_emoticon4.png","Tieba-New61":"Tieba-New/image_emoticon40.png","Tieba-New62":"Tieba-New/image_emoticon41.png","Tieba-New63":"Tieba-New/image_emoticon42.png","Tieba-New64":"Tieba-New/image_emoticon43.png","Tieba-New65":"Tieba-New/image_emoticon44.png","Tieba-New66":"Tieba-New/image_emoticon45.png","Tieba-New67":"Tieba-New/image_emoticon46.png","Tieba-New68":"Tieba-New/image_emoticon47.png","Tieba-New69":"Tieba-New/image_emoticon48.png","Tieba-New70":"Tieba-New/image_emoticon49.png","Tieba-New71":"Tieba-New/image_emoticon5.png","Tieba-New72":"Tieba-New/image_emoticon50.png","Tieba-New73":"Tieba-New/image_emoticon6.png","Tieba-New74":"Tieba-New/image_emoticon66.png","Tieba-New75":"Tieba-New/image_emoticon67.png","Tieba-New76":"Tieba-New/image_emoticon68.png","Tieba-New77":"Tieba-New/image_emoticon69.png","Tieba-New78":"Tieba-New/image_emoticon7.png","Tieba-New79":"Tieba-New/image_emoticon70.png","Tieba-New80":"Tieba-New/image_emoticon71.png","Tieba-New81":"Tieba-New/image_emoticon72.png","Tieba-New82":"Tieba-New/image_emoticon73.png","Tieba-New83":"Tieba-New/image_emoticon74.png","Tieba-New84":"Tieba-New/image_emoticon75.png","Tieba-New85":"Tieba-New/image_emoticon76.png","Tieba-New86":"Tieba-New/image_emoticon77.png","Tieba-New87":"Tieba-New/image_emoticon78.png","Tieba-New88":"Tieba-New/image_emoticon79.png","Tieba-New89":"Tieba-New/image_emoticon8.png","Tieba-New90":"Tieba-New/image_emoticon80.png","Tieba-New91":"Tieba-New/image_emoticon81.png","Tieba-New92":"Tieba-New/image_emoticon82.png","Tieba-New93":"Tieba-New/image_emoticon83.png","Tieba-New94":"Tieba-New/image_emoticon84.png","Tieba-New95":"Tieba-New/image_emoticon85.png","Tieba-New96":"Tieba-New/image_emoticon86.png","Tieba-New97":"Tieba-New/image_emoticon87.png","Tieba-New98":"Tieba-New/image_emoticon88.png","Tieba-New99":"Tieba-New/image_emoticon89.png","Tieba-New100":"Tieba-New/image_emoticon9.png","Tieba-New101":"Tieba-New/image_emoticon90.png","Tieba-New102":"Tieba-New/image_emoticon91.png","Tieba-New103":"Tieba-New/image_emoticon92.png","Tieba-New104":"Tieba-New/image_emoticon93.png","Tieba-New105":"Tieba-New/image_emoticon94.png","Tieba-New106":"Tieba-New/image_emoticon95.png","Tieba-New107":"Tieba-New/image_emoticon96.png","Tieba-New108":"Tieba-New/image_emoticon97.png","Tieba-New109":"Tieba-New/image_emoticon98.png","Tieba-New110":"Tieba-New/image_emoticon99.png",weibo2:"weibo/d_aoteman.png",weibo15:"weibo/d_doge.png",weibo16:"weibo/d_erha.png",weibo40:"weibo/d_miao.png",weibo49:"weibo/d_shenshou.png",weibo65:"weibo/d_xiongmao.png",weibo74:"weibo/d_zhutou.png",weibo75:"weibo/d_zuiyou.png",weibo76:"weibo/emoji_0x1f31f.png",weibo77:"weibo/emoji_0x1f349.png",weibo78:"weibo/emoji_0x1f357.png",weibo79:"weibo/emoji_0x1f384.png",weibo80:"weibo/emoji_0x1f44f.png",weibo81:"weibo/emoji_0x1f47b.png",weibo82:"weibo/emoji_0x1f47f.png",weibo83:"weibo/emoji_0x1f48a.png",weibo84:"weibo/emoji_0x1f4a3.png",weibo85:"weibo/emoji_0x1f4a9.png",weibo86:"weibo/emoji_0x1f631.png",weibo87:"weibo/emoji_0x1f643.png",weibo88:"weibo/emoji_0x1f645.png",weibo89:"weibo/emoji_0x1f648.png",weibo90:"weibo/emoji_0x1f649.png",weibo91:"weibo/emoji_0x1f64a.png",weibo92:"weibo/emoji_0x1f64b.png",weibo93:"weibo/emoji_0x1f64f.png",weibo94:"weibo/emoji_0x1f913.png",weibo95:"weibo/emoji_0x1f917.png",weibo96:"weibo/emoji_0x26a1.png",weibo97:"weibo/h_buyao.png",weibo98:"weibo/h_good.png",weibo99:"weibo/h_haha.png",weibo100:"weibo/h_jiayou.png",weibo101:"weibo/h_lai.png",weibo102:"weibo/h_ok.png",weibo103:"weibo/h_quantou.png",weibo105:"weibo/h_woshou.png",weibo106:"weibo/h_ye.png",weibo107:"weibo/h_zan.png",weibo108:"weibo/h_zuoyi.png","HONKAI3-Star1":"HONKAI3-Star/1.gif","HONKAI3-Star2":"HONKAI3-Star/10.gif","HONKAI3-Star3":"HONKAI3-Star/11.gif","HONKAI3-Star4":"HONKAI3-Star/12.gif","HONKAI3-Star5":"HONKAI3-Star/13.gif","HONKAI3-Star6":"HONKAI3-Star/14.gif","HONKAI3-Star7":"HONKAI3-Star/15.gif","HONKAI3-Star8":"HONKAI3-Star/16.gif","HONKAI3-Star9":"HONKAI3-Star/2.gif","HONKAI3-Star10":"HONKAI3-Star/3.gif","HONKAI3-Star11":"HONKAI3-Star/4.gif","HONKAI3-Star12":"HONKAI3-Star/5.gif","HONKAI3-Star13":"HONKAI3-Star/6.gif","HONKAI3-Star14":"HONKAI3-Star/7.gif","HONKAI3-Star15":"HONKAI3-Star/8.gif","HONKAI3-Star16":"HONKAI3-Star/9.gif","HONKAI3-Daily1":"HONKAI3-Daily/1.gif","HONKAI3-Daily2":"HONKAI3-Daily/10.gif","HONKAI3-Daily3":"HONKAI3-Daily/11.gif","HONKAI3-Daily4":"HONKAI3-Daily/12.gif","HONKAI3-Daily5":"HONKAI3-Daily/13.gif","HONKAI3-Daily6":"HONKAI3-Daily/14.gif","HONKAI3-Daily7":"HONKAI3-Daily/15.gif","HONKAI3-Daily8":"HONKAI3-Daily/16.gif","HONKAI3-Daily9":"HONKAI3-Daily/2.gif","HONKAI3-Daily10":"HONKAI3-Daily/3.gif","HONKAI3-Daily11":"HONKAI3-Daily/4.gif","HONKAI3-Daily12":"HONKAI3-Daily/5.gif","HONKAI3-Daily13":"HONKAI3-Daily/6.gif","HONKAI3-Daily14":"HONKAI3-Daily/7.gif","HONKAI3-Daily15":"HONKAI3-Daily/8.gif","HONKAI3-Daily16":"HONKAI3-Daily/9.gif","Tsuri-me-ju_mimi1":"Tsuri-me-ju_mimi/10753776_key@2x.png","Tsuri-me-ju_mimi2":"Tsuri-me-ju_mimi/10753777_key@2x.png","Tsuri-me-ju_mimi3":"Tsuri-me-ju_mimi/10753778_key@2x.png","Tsuri-me-ju_mimi12":"Tsuri-me-ju_mimi/10753787_key@2x.png","Tsuri-me-ju_mimi13":"Tsuri-me-ju_mimi/10753788_key@2x.png","Tsuri-me-ju_mimi14":"Tsuri-me-ju_mimi/10753789_key@2x.png","Tsuri-me-ju_mimi15":"Tsuri-me-ju_mimi/10753790_key@2x.png","Tsuri-me-ju_mimi16":"Tsuri-me-ju_mimi/10753791_key@2x.png","Tsuri-me-ju_mimi36":"Tsuri-me-ju_mimi/10753811_key@2x.png","Tsuri-me-ju_mimi37":"Tsuri-me-ju_mimi/10753812_key@2x.png","Tsuri-me-ju_mimi38":"Tsuri-me-ju_mimi/10753813_key@2x.png","Tsuri-me-ju_mimi40":"Tsuri-me-ju_mimi/10753815_key@2x.png"},enableQQ:!0,avatar_cdn:"https://cravatar.cn/avatar/",visitor:!0},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vemoji",(()=>{const i=document.createElement("style");i.innerHTML="\n            #valine .vemoji {\n              width: 40px !important;  /* æ ¹æ®éœ€è¦è°ƒæ•´å®½åº¦ */\n              height: 40px !important; /* æ ¹æ®éœ€è¦è°ƒæ•´é«˜åº¦ */\n            }\n          ",document.head.appendChild(i)})),Fluid.utils.waitElementVisible("#valine .vcontent",(()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)}))}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>ç›®å½•</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a><div style="font-size:.85rem"><span id="timeDate">è½½å…¥å¤©æ•°...</span> <span id="times">è½½å…¥æ—¶åˆ†ç§’...</span><script src="/js/duration.js"></script></div></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">æ€»è®¿é—®é‡ <span id="leancloud-site-pv"></span> æ¬¡ </span><span id="leancloud-site-uv-container" style="display:none">æ€»è®¿å®¢æ•° <span id="leancloud-site-uv"></span> äºº</span></div><span>83k</span></div></footer><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach((t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")}))},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicRibbon.min.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/star.min.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/love.min.js"></script><script src="//cdn.jsdelivr.net/npm/highlight.js@11.5.1/styles/monokai.min.css.js"></script><script src="/js/scrollAnimation.js"></script><script src="/js/loading.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div></noscript><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html>
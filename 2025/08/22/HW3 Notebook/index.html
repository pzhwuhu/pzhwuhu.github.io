<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="light"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/avatar_1.jpg"><link rel="icon" href="/img/avatar_1.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="pzhwuhu"><meta name="keywords" content=""><meta name="description" content="这是我完成李宏毅机器学习2021作业3时记录的一些笔记 ，希望能对你有所帮助😊             😆 作业2没整出来什么，直接看SOTA 方法了，也就没什么笔记 作业要求Homework 3 - Convolutional Neural NetworkThis is the example code of homework 3 of the machine le"><meta property="og:type" content="article"><meta property="og:title" content="HW3 Notebook"><meta property="og:url" content="http://pzhwuhu.github.io/2025/08/22/HW3%20Notebook/index.html"><meta property="og:site_name" content="鹏哥"><meta property="og:description" content="这是我完成李宏毅机器学习2021作业3时记录的一些笔记 ，希望能对你有所帮助😊             😆 作业2没整出来什么，直接看SOTA 方法了，也就没什么笔记 作业要求Homework 3 - Convolutional Neural NetworkThis is the example code of homework 3 of the machine le"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://pzhwuhu.github.io/myimg/%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%97.png"><meta property="article:published_time" content="2025-08-22T15:11:00.000Z"><meta property="article:modified_time" content="2025-08-23T10:17:16.076Z"><meta property="article:author" content="pzhwuhu"><meta property="article:tag" content="CNN"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://pzhwuhu.github.io/myimg/%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%97.png"><title>HW3 Notebook - 鹏哥</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/custom_dark_mode.css"><link rel="stylesheet" href="/css/scroll_animation.css"><link rel="stylesheet" href="/css/mac.css"><link rel="stylesheet" href="/css/loading.css"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/gradient.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"pzhwuhu.github.io",root:"/",version:"1.9.8",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!1,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:{key:null},google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"cp3oK6SJdvk7QCx0TiMpNSdI-gzGzoHsz",app_key:"B9u5rSWtXwzrQl56fuCE0o9M",server_url:"https://cp3ok6sj.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2024-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>if(!Fluid.ctx.dnt){var _hmt=_hmt||[];!function(){var t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?[object Object]";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()}</script><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:90vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>身如芥子，心藏须弥</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item"><a class="nav-link" href="/links/" target="_self"><i class="iconfont icon-link-fill"></i> <span>友链</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url('/myimg/%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%979.jpg') no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="HW3 Notebook"></span></div><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> pzhwuhu </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-08-22 23:11" pubdate>2025年8月22日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2.1k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 18 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="ML&amp;DL" id="heading-07cc572b8beca73017be9b3680716d88" role="tab" data-toggle="collapse" href="#collapse-07cc572b8beca73017be9b3680716d88" aria-expanded="true">ML&amp;DL <span class="list-group-count">(6)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-07cc572b8beca73017be9b3680716d88" role="tabpanel" aria-labelledby="heading-07cc572b8beca73017be9b3680716d88"><div class="category-post-list"><a href="/2025/08/22/HW3%20Notebook/" title="HW3 Notebook" class="list-group-item list-group-item-action active"><span class="category-post">HW3 Notebook</span> </a><a href="/2025/08/26/Transformer/" title="Transformer" class="list-group-item list-group-item-action"><span class="category-post">Transformer</span> </a><a href="/2025/08/19/CNN/" title="李宏毅机器学习-CNN" class="list-group-item list-group-item-action"><span class="category-post">李宏毅机器学习-CNN</span> </a><a href="/2025/08/16/HW1%20Notebook/" title="李宏毅机器学习-HW1 Notebook" class="list-group-item list-group-item-action"><span class="category-post">李宏毅机器学习-HW1 Notebook</span> </a><a href="/2025/08/21/Self-Attention/" title="李宏毅机器学习-Self-Attention" class="list-group-item list-group-item-action"><span class="category-post">李宏毅机器学习-Self-Attention</span> </a><a href="/2025/08/16/%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%B3%A8%E6%84%8F/" title="李宏毅机器学习-类神经网络训练" class="list-group-item list-group-item-action"><span class="category-post">李宏毅机器学习-类神经网络训练</span></a></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">HW3 Notebook</h1><div class="markdown-body"><div class="note note-info"><p>这是我完成李宏毅机器学习2021作业3时记录的一些笔记 ，希望能对你有所帮助😊</p></div><p>😆 作业2没整出来什么，直接看SOTA 方法了，也就没什么笔记</p><h1 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h1><h2 id="Homework-3-Convolutional-Neural-Network"><a href="#Homework-3-Convolutional-Neural-Network" class="headerlink" title="Homework 3 - Convolutional Neural Network"></a>Homework 3 - Convolutional Neural Network</h2><p>This is the example code of homework 3 of the machine learning course by Prof. Hung-yi Lee.<br>In this homework, you are required to build a <strong>convolutional neural network for image classification</strong>, possibly with some advanced training tips.</p><p>There are three levels here:<br><strong>Easy</strong>: Build a simple convolutional neural network as the baseline. (2 pts)<br><strong>Medium</strong>: Design a better architecture or adopt different data augmentations to improve the performance. (2 pts)<br><strong>Hard</strong>: Utilize provided unlabeled data to obtain better results. (2 pts)</p><h1 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h1><p>直接跑原始代码，会发现<strong>train的acc显著高于val的acc</strong>，我们的模型<strong>在train_data上过拟合了</strong>，过分依赖训练数据甚至是某些局部特征，因此，需要采取一些措施来防止过拟合</p><p>我大致修改了这些地方：</p><ul><li>使用数据增强</li><li>修改模型结构</li><li>超参数调整</li></ul><h1 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h1><p>对于一张食物图片，以我们人眼看来，将图片旋转少许角度，将图片横向翻转，改变图片的尺寸、亮度、对比度等，我们用肉眼依然能够分辨，但model就不一定了，因此，我们可以用数据增强来提升模型的泛化能力、鲁棒性，减轻一定程度的过拟合。</p><p>Data Augmentation并不直接修改原始数据，而是<strong>在样本加载时动态随机生成另一个新样本</strong></p><p>问了下deepseek-v3<br>本指南将详细解析 <code>torchvision.transforms</code> 中各种数据增强方法的参数，并介绍如何使用 <strong>AutoAugment</strong> 和 <strong>RandAugment</strong> 这两种高级增强策略。</p><hr><h2 id="1-基础几何变换方法"><a href="#1-基础几何变换方法" class="headerlink" title="1. 基础几何变换方法"></a>1. 基础几何变换方法</h2><h3 id="1-RandomHorizontalFlip-RandomVerticalFlip"><a href="#1-RandomHorizontalFlip-RandomVerticalFlip" class="headerlink" title="(1) RandomHorizontalFlip &#x2F; RandomVerticalFlip"></a>(1) <code>RandomHorizontalFlip</code> &#x2F; <code>RandomVerticalFlip</code></h3><ul><li><strong>作用</strong>：随机水平或垂直翻转图像。</li><li><strong>参数</strong>： <code>p</code> (float): 翻转概率（默认 <code>0.5</code>）。</li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.RandomHorizontalFlip(p=<span class="hljs-number">0.7</span>)  <span class="hljs-comment"># 70%概率水平翻转</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="2-RandomRotation"><a href="#2-RandomRotation" class="headerlink" title="(2) RandomRotation"></a>(2) <code>RandomRotation</code></h3><ul><li><strong>作用</strong>：随机旋转图像。</li><li><strong>参数</strong>：<ul><li><code>degrees</code> (float or tuple): 旋转角度范围（如 <code>30</code> 表示 <code>±30°</code>，<code>(10, 30)</code> 表示 <code>10°~30°</code>）。</li><li><code>expand</code> (bool): 是否扩大图像尺寸以容纳旋转后的内容（默认 <code>False</code>）。</li><li><code>fill</code> (tuple): 旋转后空白区域的填充值（RGB格式，如 <code>(255, 0, 0)</code> 表示红色）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.RandomRotation(degrees=(-<span class="hljs-number">15</span>, <span class="hljs-number">15</span>), fill=(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 旋转±15°，空白填黑色</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="3-RandomResizedCrop"><a href="#3-RandomResizedCrop" class="headerlink" title="(3) RandomResizedCrop"></a>(3) <code>RandomResizedCrop</code></h3><ul><li><strong>作用</strong>：随机裁剪并缩放到指定尺寸。</li><li><strong>参数</strong>：<ul><li><code>size</code> (int or tuple): 目标尺寸（如 <code>224</code> 或 <code>(224, 224)</code>）。</li><li><code>scale</code> (tuple): 裁剪面积比例范围（如 <code>(0.08, 1.0)</code> 表示裁剪原图的 <code>8%~100%</code>）。</li><li><code>ratio</code> (tuple): 宽高比范围（如 <code>(0.75, 1.33)</code>）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.RandomResizedCrop(size=<span class="hljs-number">224</span>, scale=(<span class="hljs-number">0.8</span>, <span class="hljs-number">1.0</span>), ratio=(<span class="hljs-number">0.9</span>, <span class="hljs-number">1.1</span>))<br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="2-颜色空间变换方法"><a href="#2-颜色空间变换方法" class="headerlink" title="2. 颜色空间变换方法"></a>2. 颜色空间变换方法</h2><h3 id="1-ColorJitter"><a href="#1-ColorJitter" class="headerlink" title="(1) ColorJitter"></a>(1) <code>ColorJitter</code></h3><ul><li><strong>作用</strong>：随机调整亮度、对比度、饱和度和色调。</li><li><strong>参数</strong>：<ul><li><code>brightness</code> (float or tuple): 亮度调整范围（如 <code>0.2</code> 表示 <code>±20%</code>）。</li><li><code>contrast</code> &#x2F; <code>saturation</code> &#x2F; <code>hue</code>: 类似亮度，但 <code>hue</code> 范围是 <code>[-0.5, 0.5]</code>。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.ColorJitter(brightness=<span class="hljs-number">0.2</span>, contrast=<span class="hljs-number">0.2</span>, saturation=<span class="hljs-number">0.2</span>, hue=<span class="hljs-number">0.1</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-RandomGrayscale"><a href="#2-RandomGrayscale" class="headerlink" title="(2) RandomGrayscale"></a>(2) <code>RandomGrayscale</code></h3><ul><li><strong>作用</strong>：以概率 <code>p</code> 将图像转为灰度。</li><li><strong>参数</strong>：<ul><li><code>p</code> (float): 灰度化概率（默认 <code>0.1</code>）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.RandomGrayscale(p=<span class="hljs-number">0.3</span>)  <span class="hljs-comment"># 30%概率灰度化</span><br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="3-高级增强方法"><a href="#3-高级增强方法" class="headerlink" title="3. 高级增强方法"></a>3. 高级增强方法</h2><h3 id="1-AutoAugment"><a href="#1-AutoAugment" class="headerlink" title="(1) AutoAugment"></a>(1) <code>AutoAugment</code></h3><ul><li><strong>作用</strong>：基于强化学习搜索的增强策略（针对特定数据集如 ImageNet、CIFAR10 优化）。</li><li><strong>参数</strong>：<ul><li><code>policy</code> (str): 预定义策略（<code>imagenet</code> &#x2F; <code>cifar10</code> &#x2F; <code>svhn</code>）。</li><li><code>interpolation</code> (InterpolationMode): 插值方法（如 <code>BILINEAR</code>）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> AutoAugment, AutoAugmentPolicy<br>transform = transforms.Compose([<br>    AutoAugment(policy=AutoAugmentPolicy.IMAGENET),  <span class="hljs-comment"># 使用ImageNet策略</span><br>    transforms.ToTensor()<br>])<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-RandAugment"><a href="#2-RandAugment" class="headerlink" title="(2) RandAugment"></a>(2) <code>RandAugment</code></h3><ul><li><strong>作用</strong>：简化版 AutoAugment，随机选择 <code>N</code> 种变换并统一强度 <code>M</code>。</li><li><strong>参数</strong>：<ul><li><code>num_ops</code> (int): 每次增强应用的变换数量（默认 <code>2</code>）。</li><li><code>magnitude</code> (int): 强度值（0~30，默认 <code>9</code>）。</li><li><code>num_magnitude_bins</code> (int): 强度离散化级别（默认 <code>31</code>）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> RandAugment<br>transform = transforms.Compose([<br>    RandAugment(num_ops=<span class="hljs-number">3</span>, magnitude=<span class="hljs-number">9</span>),  <span class="hljs-comment"># 随机选3种变换，强度9</span><br>    transforms.ToTensor()<br>])<br></code></pre></td></tr></table></figure></li></ul><h3 id="3-TrivialAugmentWide"><a href="#3-TrivialAugmentWide" class="headerlink" title="(3) TrivialAugmentWide"></a>(3) <code>TrivialAugmentWide</code></h3><ul><li><strong>作用</strong>：更简单的 RandAugment 变体，<strong>无需调参</strong>。</li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> TrivialAugmentWide<br>transform = transforms.Compose([<br>    TrivialAugmentWide(),  <span class="hljs-comment"># 自动选择变换和强度</span><br>    transforms.ToTensor()<br>])<br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="4-其他实用方法"><a href="#4-其他实用方法" class="headerlink" title="4. 其他实用方法"></a>4. 其他实用方法</h2><h3 id="1-GaussianBlur"><a href="#1-GaussianBlur" class="headerlink" title="(1) GaussianBlur"></a>(1) <code>GaussianBlur</code></h3><ul><li><strong>作用</strong>：高斯模糊（模拟焦距变化）。</li><li><strong>参数</strong>：<ul><li><code>kernel_size</code> (int or tuple): 卷积核大小（必须为奇数）。</li><li><code>sigma</code> (float or tuple): 标准差范围（如 <code>(0.1, 2.0)</code>）。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.GaussianBlur(kernel_size=<span class="hljs-number">5</span>, sigma=(<span class="hljs-number">0.1</span>, <span class="hljs-number">2.0</span>))<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-RandomErasing"><a href="#2-RandomErasing" class="headerlink" title="(2) RandomErasing"></a>(2) <code>RandomErasing</code></h3><ul><li><strong>作用</strong>：随机擦除图像区域（类似 Cutout）。</li><li><strong>参数</strong>：<ul><li><code>p</code> (float): 擦除概率。</li><li><code>scale</code> &#x2F; <code>ratio</code>: 擦除区域的面积和宽高比范围。</li></ul></li><li><strong>示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.RandomErasing(p=<span class="hljs-number">0.5</span>, scale=(<span class="hljs-number">0.02</span>, <span class="hljs-number">0.2</span>), ratio=(<span class="hljs-number">0.3</span>, <span class="hljs-number">3.3</span>))<br></code></pre></td></tr></table></figure></li></ul><hr><h2 id="5-完整示例代码"><a href="#5-完整示例代码" class="headerlink" title="5. 完整示例代码"></a>5. 完整示例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> AutoAugment, AutoAugmentPolicy, RandAugment<br><br><span class="hljs-comment"># 基础增强组合</span><br>basic_transform = transforms.Compose([<br>    transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<br>    transforms.RandomHorizontalFlip(),<br>    transforms.ColorJitter(brightness=<span class="hljs-number">0.2</span>, contrast=<span class="hljs-number">0.2</span>),<br>    transforms.ToTensor(),<br>])<br><br><span class="hljs-comment"># AutoAugment（ImageNet优化策略）</span><br>auto_transform = transforms.Compose([<br>    AutoAugment(policy=AutoAugmentPolicy.IMAGENET),<br>    transforms.ToTensor(),<br>])<br><br><span class="hljs-comment"># RandAugment（自定义强度和变换数量）</span><br>rand_transform = transforms.Compose([<br>    RandAugment(num_ops=<span class="hljs-number">3</span>, magnitude=<span class="hljs-number">9</span>),<br>    transforms.ToTensor(),<br>])<br></code></pre></td></tr></table></figure><hr><h2 id="6-选择建议"><a href="#6-选择建议" class="headerlink" title="6. 选择建议"></a>6. 选择建议</h2><ul><li><strong>简单任务</strong>：基础几何变换 + <code>ColorJitter</code>。</li><li><strong>复杂任务</strong>：优先尝试 <code>AutoAugment</code>（预训练策略）或 <code>RandAugment</code>（需调参）。</li><li><strong>资源有限</strong>：使用 <code>TrivialAugmentWide</code>（免调参）。</li></ul><p>通过合理组合这些方法，可以显著提升图像分类模型的泛化能力！ 🚀</p><p>反正我试了一下几种方法，也包括自己diy的，竟然<code>TrivialAugmentWide</code>的效果是最好的，看来它必然有它的道理</p><h1 id="CNN架构"><a href="#CNN架构" class="headerlink" title="CNN架构"></a>CNN架构</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Classifier</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Classifier, <span class="hljs-variable language_">self</span>).__init__()<br><br>        <span class="hljs-comment"># The arguments for commonly used modules:</span><br><br>        <span class="hljs-comment"># torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)</span><br>        <span class="hljs-comment"># torch.nn.MaxPool2d(kernel_size, stride, padding)</span><br><br>        <span class="hljs-comment"># input image size: [3, 128, 128]</span><br><br>        <span class="hljs-variable language_">self</span>.cnn_layers = nn.Sequential(<br><br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>),<br><br>            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">128</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>),<br><br>            nn.Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">256</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>),<br><br>        )<br><br>        <span class="hljs-variable language_">self</span>.fc_layers = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">256</span> * <span class="hljs-number">8</span> * <span class="hljs-number">8</span>, <span class="hljs-number">256</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">11</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># input (x): [batch_size, 3, 128, 128]</span><br>        <span class="hljs-comment"># output: [batch_size, 11]</span><br>        <br>        <span class="hljs-comment"># Extract features by convolutional layers.</span><br>        x = <span class="hljs-variable language_">self</span>.cnn_layers(x)<br><br>        <span class="hljs-comment"># The extracted feature map must be flatten before going to fully-connected layers.</span><br>        x = x.flatten(<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># The features are transformed by fully-connected layers to obtain the final logits.</span><br>        x = <span class="hljs-variable language_">self</span>.fc_layers(x)<br><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h2><p>先解释下<code>nn.Conv2d(3, 64, 3, 1, 1)</code>,分别对应 in_channels, out_channels, kernel_size, stride, padding</p><ul><li><strong>输入通道（<code>in_channels</code>）</strong>：输入数据的特征层数（如RGB图像的3通道、前一层的输出通道数）。</li><li><strong>输出通道（<code>out_channels</code>）</strong>：卷积后生成的特征图数量（即卷积核的数量）。</li><li><strong>每个输出通道</strong>由<strong>一个独立的卷积核</strong>生成，该卷积核会扫描所有输入通道并加权求和。</li></ul><h3 id="输入输出通道的设计逻辑"><a href="#输入输出通道的设计逻辑" class="headerlink" title="输入输出通道的设计逻辑"></a>输入输出通道的设计逻辑</h3><h4 id="1-通道数的变化规律"><a href="#1-通道数的变化规律" class="headerlink" title="(1) 通道数的变化规律"></a>(1) 通道数的变化规律</h4><ul><li><strong>经典CNN模式</strong>：<ul><li>随着网络加深，空间尺寸（<code>H, W</code>）逐渐减小（通过池化或大步长卷积）。</li><li>通道数逐渐增加（如 <code>3 → 64 → 128 → 256</code>），以保留更多高阶特征信息。</li><li><strong>原因</strong>：空间信息压缩后，需增加通道维度来维持信息容量。</li></ul></li></ul><h4 id="2-为什么需要多输出通道？"><a href="#2-为什么需要多输出通道？" class="headerlink" title="(2) 为什么需要多输出通道？"></a>(2) 为什么需要多输出通道？</h4><ul><li><strong>多样性特征提取</strong>：<strong>不同卷积核检测不同模式（如边缘、颜色、纹理</strong>）。<ul><li>示例：第一层的输出通道可能分别对应“水平边缘”、“垂直边缘”、“红色区域”等特征。</li></ul></li><li><strong>特征组合</strong>：深层卷积通过混合多通道输入，生成更复杂的特征（如“眼睛+鼻子”组合为人脸）。</li></ul><h4 id="3-输入输出通道的约束"><a href="#3-输入输出通道的约束" class="headerlink" title="(3) 输入输出通道的约束"></a>(3) 输入输出通道的约束</h4><ul><li><strong>输入通道必须匹配</strong>：<ul><li>前一层的 <code>out_channels</code> 必须等于后一层的 <code>in_channels</code>。</li><li>例如：<code>Conv2d(64, 128)</code> 的下一层必须是 <code>Conv2d(128, ...)</code>。</li></ul></li><li><strong>输出通道自由选择</strong>：<ul><li>通常取2的幂次（如64、128、256），方便GPU内存对齐优化。</li></ul></li></ul><h3 id="卷积过程"><a href="#卷积过程" class="headerlink" title="卷积过程"></a>卷积过程</h3><p>可视化图：<br><img src="https://cdn.jsdelivr.net/gh/pzhwuhu/Image-Hosting/Posts%20insert/20250823164942361.gif" srcset="/img/loading.gif" lazyload></p><h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><ul><li>原始数据集中有很大一部分是没有<code>label</code>的，如果想要提高模型准确性与泛化能力，必须利用好这部分数据，这就涉及到半监督学习 <code>semi-supervised learning</code></li><li>具体就是先用已有模型去预测数据，置信度大于<code>threshold</code>就认为这个<code>label</code>是正确的，将其加入训练集，进入下一轮训练</li><li>具体需要我们做的就是<code>filter</code>一下即可</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_pseudo_labels</span>(<span class="hljs-params">dataset, model, threshold=<span class="hljs-number">0.7</span></span>):<br>    <span class="hljs-comment"># This functions generates pseudo-labels of a dataset using given model.</span><br>    <span class="hljs-comment"># It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.</span><br>    <span class="hljs-comment"># You are NOT allowed to use any models trained on external data for pseudo-labeling.</span><br>    device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br><br>    <span class="hljs-comment"># Construct a data loader.</span><br>    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-comment"># Make sure the model is in eval mode.</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-comment"># Define softmax function.</span><br>    softmax = nn.Softmax(dim=-<span class="hljs-number">1</span>)<br>    new_dataset = []<br><br>    <span class="hljs-comment"># Iterate over the dataset by batches.</span><br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> tqdm(data_loader):<br>        img, _ = batch<br><br>        <span class="hljs-comment"># Forward the data</span><br>        <span class="hljs-comment"># Using torch.no_grad() accelerates the forward process.</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            logits = model(img.to(device))<br><br>        <span class="hljs-comment"># Obtain the probability distributions by applying softmax on logits.</span><br>        probs = softmax(logits)<br><br>        <span class="hljs-comment"># ---------- TODO ----------</span><br>        <span class="hljs-comment"># Filter the data and construct a new dataset.</span><br>        max_probs, preds = torch.<span class="hljs-built_in">max</span>(probs, dim=-<span class="hljs-number">1</span>)<br>        mask = max_probs &gt;= threshold<br>        selected_imgs = img[mask]<br>        selected_labels = preds[mask]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(selected_imgs.size(<span class="hljs-number">0</span>)):<br>            new_dataset.append((selected_imgs[i], selected_labels[i].item()))<br>            <br>        dataset = torch.utils.data.DatasetFolder(new_dataset, loader=image_loader, extensions=<span class="hljs-string">&quot;jpg&quot;</span>, transform = train_tfm)  <span class="hljs-comment"># Dummy loader and extensions</span><br>	    <span class="hljs-comment"># -------------------------- </span><br><br>    <span class="hljs-comment"># # Turn off the eval mode.</span><br>    model.train()<br>    <span class="hljs-keyword">return</span> dataset<br></code></pre></td></tr></table></figure><h2 id="mask"><a href="#mask" class="headerlink" title="mask"></a>mask</h2><ul><li>这里比较有趣的是<code>mask</code>，为什么能直接索引数据了，这不是一个判断条件么？其实这是一种<strong>高效张量操作方式</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">mask = max_probs &gt;= threshold<br>      selected_imgs = img[mask]<br>      selected_labels = preds[mask]<br></code></pre></td></tr></table></figure></li></ul><h3 id="本质：布尔掩码索引（Boolean-Masking）"><a href="#本质：布尔掩码索引（Boolean-Masking）" class="headerlink" title="本质：布尔掩码索引（Boolean Masking）"></a>本质：布尔掩码索引（Boolean Masking）</h3><p>PyTorch 的张量（<code>Tensor</code>）支持类似 NumPy 的 <strong>高级索引（Advanced Indexing）</strong>，其中布尔掩码索引是核心特性之一：<br><strong>关键点</strong></p><ul><li><code>max_probs &gt;= threshold</code> 会生成一个与原张量形状相同的布尔张量（<code>Tensor</code> of <code>bool</code>）</li><li><code>img[mask]</code> 会返回所有 <code>mask</code> 为 <code>True</code> 位置对应的数据子集<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">mask = max_probs &gt;= threshold  <span class="hljs-comment"># 生成布尔张量（True/False）</span><br>selected_imgs = img[mask]      <span class="hljs-comment"># 用布尔张量筛选数据</span><br><br>max_probs = tensor([<span class="hljs-number">0.9</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.3</span>])<br>mask = max_probs &gt;= <span class="hljs-number">0.7</span>  <span class="hljs-comment"># tensor([ True, False,  True, False])</span><br></code></pre></td></tr></table></figure></li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/ML-DL/" class="category-chain-item">ML&amp;DL</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/CNN/" class="print-no-link">#CNN</a></div></div><div class="license-box my-3"><div class="license-title"><div>HW3 Notebook</div><div>http://pzhwuhu.github.io/2025/08/22/HW3 Notebook/</div></div><div class="license-meta"><div class="license-meta-item"><div>本文作者</div><div>pzhwuhu</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2025年8月22日</div></div><div class="license-meta-item license-meta-date"><div>更新于</div><div>2025年8月23日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-cc-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - 非商业性使用"><i class="iconfont icon-cc-nc"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - 相同方式共享"><i class="iconfont icon-cc-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2025/08/26/Transformer/" title="Transformer"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Transformer</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2025/08/21/Self-Attention/" title="李宏毅机器学习-Self-Attention"><span class="hidden-mobile">李宏毅机器学习-Self-Attention</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script>Fluid.utils.loadComments("#valine",(function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",(function(){var i=Object.assign({appId:"cp3oK6SJdvk7QCx0TiMpNSdI-gzGzoHsz",appKey:"B9u5rSWtXwzrQl56fuCE0o9M",path:"window.location.pathname",placeholder:"留下你的足迹吧，超多emoji可用噢，gravatar邮箱或qq邮箱自动获取头像",avatar:"retro",meta:["nick","mail","link"],requiredFields:["nick"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://valine-emoji.bili33.top/",emojiMaps:{bilibilitv2:"bilibilitv/[tv_doge].png",bilibilitv3:"bilibilitv/[tv_亲亲].png",bilibilitv4:"bilibilitv/[tv_偷笑].png",bilibilitv5:"bilibilitv/[tv_再见].png",bilibilitv6:"bilibilitv/[tv_冷漠].png",bilibilitv7:"bilibilitv/[tv_发怒].png",bilibilitv8:"bilibilitv/[tv_发财].png",bilibilitv9:"bilibilitv/[tv_可爱].png",bilibilitv10:"bilibilitv/[tv_吐血].png",bilibilitv11:"bilibilitv/[tv_呆].png",bilibilitv12:"bilibilitv/[tv_呕吐].png",bilibilitv13:"bilibilitv/[tv_困].png",bilibilitv14:"bilibilitv/[tv_坏笑].png",bilibilitv15:"bilibilitv/[tv_大佬].png",bilibilitv16:"bilibilitv/[tv_大哭].png",bilibilitv17:"bilibilitv/[tv_委屈].png",bilibilitv18:"bilibilitv/[tv_害羞].png",bilibilitv19:"bilibilitv/[tv_尴尬].png",bilibilitv20:"bilibilitv/[tv_微笑].png",bilibilitv21:"bilibilitv/[tv_思考].png",bilibilitv22:"bilibilitv/[tv_惊吓].png",bilibilitv23:"bilibilitv/[tv_打脸].png",bilibilitv24:"bilibilitv/[tv_抓狂].png",bilibilitv25:"bilibilitv/[tv_抠鼻].png",bilibilitv26:"bilibilitv/[tv_斜眼笑].png",bilibilitv27:"bilibilitv/[tv_无奈].png",bilibilitv28:"bilibilitv/[tv_晕].png",bilibilitv29:"bilibilitv/[tv_流汗].png",bilibilitv30:"bilibilitv/[tv_流泪].png",bilibilitv31:"bilibilitv/[tv_流鼻血].png",bilibilitv32:"bilibilitv/[tv_点赞].png",bilibilitv33:"bilibilitv/[tv_生气].png",bilibilitv34:"bilibilitv/[tv_生病].png",bilibilitv35:"bilibilitv/[tv_疑问].png",bilibilitv36:"bilibilitv/[tv_白眼].png",bilibilitv37:"bilibilitv/[tv_皱眉].png",bilibilitv38:"bilibilitv/[tv_目瞪口呆].png",bilibilitv39:"bilibilitv/[tv_睡着].png",bilibilitv40:"bilibilitv/[tv_笑哭].png",bilibilitv41:"bilibilitv/[tv_腼腆].png",bilibilitv42:"bilibilitv/[tv_色].png",bilibilitv43:"bilibilitv/[tv_调侃].png",bilibilitv44:"bilibilitv/[tv_调皮].png",bilibilitv45:"bilibilitv/[tv_鄙视].png",bilibilitv46:"bilibilitv/[tv_闭嘴].png",bilibilitv47:"bilibilitv/[tv_难过].png",bilibilitv48:"bilibilitv/[tv_馋].png",bilibilitv49:"bilibilitv/[tv_鬼脸].png",bilibilitv50:"bilibilitv/[tv_黑人问号].png",bilibilitv51:"bilibilitv/[tv_鼓掌].png",bilibili22332:"bilibili2233/[2233娘_卖萌].png",bilibili22333:"bilibili2233/[2233娘_吃惊].png",bilibili22334:"bilibili2233/[2233娘_吐魂].png",bilibili22335:"bilibili2233/[2233娘_喝水].png",bilibili22336:"bilibili2233/[2233娘_困惑].png",bilibili22337:"bilibili2233/[2233娘_大哭].png",bilibili22338:"bilibili2233/[2233娘_大笑].png",bilibili22339:"bilibili2233/[2233娘_委屈].png",bilibili223310:"bilibili2233/[2233娘_怒].png",bilibili223311:"bilibili2233/[2233娘_无言].png",bilibili223312:"bilibili2233/[2233娘_汗].png",bilibili223313:"bilibili2233/[2233娘_疑问].png",bilibili223314:"bilibili2233/[2233娘_第一].png",bilibili223315:"bilibili2233/[2233娘_耶].png",bilibili223316:"bilibili2233/[2233娘_郁闷].png","Tieba-New2":"Tieba-New/image_emoticon.png","Tieba-New3":"Tieba-New/image_emoticon10.png","Tieba-New4":"Tieba-New/image_emoticon100.png","Tieba-New14":"Tieba-New/image_emoticon11.png","Tieba-New31":"Tieba-New/image_emoticon13.png","Tieba-New32":"Tieba-New/image_emoticon14.png","Tieba-New33":"Tieba-New/image_emoticon15.png","Tieba-New34":"Tieba-New/image_emoticon16.png","Tieba-New35":"Tieba-New/image_emoticon17.png","Tieba-New36":"Tieba-New/image_emoticon18.png","Tieba-New37":"Tieba-New/image_emoticon19.png","Tieba-New38":"Tieba-New/image_emoticon2.png","Tieba-New39":"Tieba-New/image_emoticon20.png","Tieba-New40":"Tieba-New/image_emoticon21.png","Tieba-New41":"Tieba-New/image_emoticon22.png","Tieba-New42":"Tieba-New/image_emoticon23.png","Tieba-New43":"Tieba-New/image_emoticon24.png","Tieba-New44":"Tieba-New/image_emoticon25.png","Tieba-New45":"Tieba-New/image_emoticon26.png","Tieba-New46":"Tieba-New/image_emoticon27.png","Tieba-New47":"Tieba-New/image_emoticon28.png","Tieba-New48":"Tieba-New/image_emoticon29.png","Tieba-New49":"Tieba-New/image_emoticon3.png","Tieba-New50":"Tieba-New/image_emoticon30.png","Tieba-New51":"Tieba-New/image_emoticon31.png","Tieba-New52":"Tieba-New/image_emoticon32.png","Tieba-New53":"Tieba-New/image_emoticon33.png","Tieba-New54":"Tieba-New/image_emoticon34.png","Tieba-New55":"Tieba-New/image_emoticon35.png","Tieba-New56":"Tieba-New/image_emoticon36.png","Tieba-New57":"Tieba-New/image_emoticon37.png","Tieba-New58":"Tieba-New/image_emoticon38.png","Tieba-New59":"Tieba-New/image_emoticon39.png","Tieba-New60":"Tieba-New/image_emoticon4.png","Tieba-New61":"Tieba-New/image_emoticon40.png","Tieba-New62":"Tieba-New/image_emoticon41.png","Tieba-New63":"Tieba-New/image_emoticon42.png","Tieba-New64":"Tieba-New/image_emoticon43.png","Tieba-New65":"Tieba-New/image_emoticon44.png","Tieba-New66":"Tieba-New/image_emoticon45.png","Tieba-New67":"Tieba-New/image_emoticon46.png","Tieba-New68":"Tieba-New/image_emoticon47.png","Tieba-New69":"Tieba-New/image_emoticon48.png","Tieba-New70":"Tieba-New/image_emoticon49.png","Tieba-New71":"Tieba-New/image_emoticon5.png","Tieba-New72":"Tieba-New/image_emoticon50.png","Tieba-New73":"Tieba-New/image_emoticon6.png","Tieba-New74":"Tieba-New/image_emoticon66.png","Tieba-New75":"Tieba-New/image_emoticon67.png","Tieba-New76":"Tieba-New/image_emoticon68.png","Tieba-New77":"Tieba-New/image_emoticon69.png","Tieba-New78":"Tieba-New/image_emoticon7.png","Tieba-New79":"Tieba-New/image_emoticon70.png","Tieba-New80":"Tieba-New/image_emoticon71.png","Tieba-New81":"Tieba-New/image_emoticon72.png","Tieba-New82":"Tieba-New/image_emoticon73.png","Tieba-New83":"Tieba-New/image_emoticon74.png","Tieba-New84":"Tieba-New/image_emoticon75.png","Tieba-New85":"Tieba-New/image_emoticon76.png","Tieba-New86":"Tieba-New/image_emoticon77.png","Tieba-New87":"Tieba-New/image_emoticon78.png","Tieba-New88":"Tieba-New/image_emoticon79.png","Tieba-New89":"Tieba-New/image_emoticon8.png","Tieba-New90":"Tieba-New/image_emoticon80.png","Tieba-New91":"Tieba-New/image_emoticon81.png","Tieba-New92":"Tieba-New/image_emoticon82.png","Tieba-New93":"Tieba-New/image_emoticon83.png","Tieba-New94":"Tieba-New/image_emoticon84.png","Tieba-New95":"Tieba-New/image_emoticon85.png","Tieba-New96":"Tieba-New/image_emoticon86.png","Tieba-New97":"Tieba-New/image_emoticon87.png","Tieba-New98":"Tieba-New/image_emoticon88.png","Tieba-New99":"Tieba-New/image_emoticon89.png","Tieba-New100":"Tieba-New/image_emoticon9.png","Tieba-New101":"Tieba-New/image_emoticon90.png","Tieba-New102":"Tieba-New/image_emoticon91.png","Tieba-New103":"Tieba-New/image_emoticon92.png","Tieba-New104":"Tieba-New/image_emoticon93.png","Tieba-New105":"Tieba-New/image_emoticon94.png","Tieba-New106":"Tieba-New/image_emoticon95.png","Tieba-New107":"Tieba-New/image_emoticon96.png","Tieba-New108":"Tieba-New/image_emoticon97.png","Tieba-New109":"Tieba-New/image_emoticon98.png","Tieba-New110":"Tieba-New/image_emoticon99.png",weibo2:"weibo/d_aoteman.png",weibo15:"weibo/d_doge.png",weibo16:"weibo/d_erha.png",weibo40:"weibo/d_miao.png",weibo49:"weibo/d_shenshou.png",weibo65:"weibo/d_xiongmao.png",weibo74:"weibo/d_zhutou.png",weibo75:"weibo/d_zuiyou.png",weibo76:"weibo/emoji_0x1f31f.png",weibo77:"weibo/emoji_0x1f349.png",weibo78:"weibo/emoji_0x1f357.png",weibo79:"weibo/emoji_0x1f384.png",weibo80:"weibo/emoji_0x1f44f.png",weibo81:"weibo/emoji_0x1f47b.png",weibo82:"weibo/emoji_0x1f47f.png",weibo83:"weibo/emoji_0x1f48a.png",weibo84:"weibo/emoji_0x1f4a3.png",weibo85:"weibo/emoji_0x1f4a9.png",weibo86:"weibo/emoji_0x1f631.png",weibo87:"weibo/emoji_0x1f643.png",weibo88:"weibo/emoji_0x1f645.png",weibo89:"weibo/emoji_0x1f648.png",weibo90:"weibo/emoji_0x1f649.png",weibo91:"weibo/emoji_0x1f64a.png",weibo92:"weibo/emoji_0x1f64b.png",weibo93:"weibo/emoji_0x1f64f.png",weibo94:"weibo/emoji_0x1f913.png",weibo95:"weibo/emoji_0x1f917.png",weibo96:"weibo/emoji_0x26a1.png",weibo97:"weibo/h_buyao.png",weibo98:"weibo/h_good.png",weibo99:"weibo/h_haha.png",weibo100:"weibo/h_jiayou.png",weibo101:"weibo/h_lai.png",weibo102:"weibo/h_ok.png",weibo103:"weibo/h_quantou.png",weibo105:"weibo/h_woshou.png",weibo106:"weibo/h_ye.png",weibo107:"weibo/h_zan.png",weibo108:"weibo/h_zuoyi.png","HONKAI3-Star1":"HONKAI3-Star/1.gif","HONKAI3-Star2":"HONKAI3-Star/10.gif","HONKAI3-Star3":"HONKAI3-Star/11.gif","HONKAI3-Star4":"HONKAI3-Star/12.gif","HONKAI3-Star5":"HONKAI3-Star/13.gif","HONKAI3-Star6":"HONKAI3-Star/14.gif","HONKAI3-Star7":"HONKAI3-Star/15.gif","HONKAI3-Star8":"HONKAI3-Star/16.gif","HONKAI3-Star9":"HONKAI3-Star/2.gif","HONKAI3-Star10":"HONKAI3-Star/3.gif","HONKAI3-Star11":"HONKAI3-Star/4.gif","HONKAI3-Star12":"HONKAI3-Star/5.gif","HONKAI3-Star13":"HONKAI3-Star/6.gif","HONKAI3-Star14":"HONKAI3-Star/7.gif","HONKAI3-Star15":"HONKAI3-Star/8.gif","HONKAI3-Star16":"HONKAI3-Star/9.gif","HONKAI3-Daily1":"HONKAI3-Daily/1.gif","HONKAI3-Daily2":"HONKAI3-Daily/10.gif","HONKAI3-Daily3":"HONKAI3-Daily/11.gif","HONKAI3-Daily4":"HONKAI3-Daily/12.gif","HONKAI3-Daily5":"HONKAI3-Daily/13.gif","HONKAI3-Daily6":"HONKAI3-Daily/14.gif","HONKAI3-Daily7":"HONKAI3-Daily/15.gif","HONKAI3-Daily8":"HONKAI3-Daily/16.gif","HONKAI3-Daily9":"HONKAI3-Daily/2.gif","HONKAI3-Daily10":"HONKAI3-Daily/3.gif","HONKAI3-Daily11":"HONKAI3-Daily/4.gif","HONKAI3-Daily12":"HONKAI3-Daily/5.gif","HONKAI3-Daily13":"HONKAI3-Daily/6.gif","HONKAI3-Daily14":"HONKAI3-Daily/7.gif","HONKAI3-Daily15":"HONKAI3-Daily/8.gif","HONKAI3-Daily16":"HONKAI3-Daily/9.gif","Tsuri-me-ju_mimi1":"Tsuri-me-ju_mimi/10753776_key@2x.png","Tsuri-me-ju_mimi2":"Tsuri-me-ju_mimi/10753777_key@2x.png","Tsuri-me-ju_mimi3":"Tsuri-me-ju_mimi/10753778_key@2x.png","Tsuri-me-ju_mimi12":"Tsuri-me-ju_mimi/10753787_key@2x.png","Tsuri-me-ju_mimi13":"Tsuri-me-ju_mimi/10753788_key@2x.png","Tsuri-me-ju_mimi14":"Tsuri-me-ju_mimi/10753789_key@2x.png","Tsuri-me-ju_mimi15":"Tsuri-me-ju_mimi/10753790_key@2x.png","Tsuri-me-ju_mimi16":"Tsuri-me-ju_mimi/10753791_key@2x.png","Tsuri-me-ju_mimi36":"Tsuri-me-ju_mimi/10753811_key@2x.png","Tsuri-me-ju_mimi37":"Tsuri-me-ju_mimi/10753812_key@2x.png","Tsuri-me-ju_mimi38":"Tsuri-me-ju_mimi/10753813_key@2x.png","Tsuri-me-ju_mimi40":"Tsuri-me-ju_mimi/10753815_key@2x.png"},enableQQ:!0,avatar_cdn:"https://cravatar.cn/avatar/",visitor:!0},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vemoji",(()=>{const i=document.createElement("style");i.innerHTML="\n            #valine .vemoji {\n              width: 40px !important;  /* 根据需要调整宽度 */\n              height: 40px !important; /* 根据需要调整高度 */\n            }\n          ",document.head.appendChild(i)})),Fluid.utils.waitElementVisible("#valine .vcontent",(()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)}))}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a><div style="font-size:.85rem"><span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span><script src="/js/duration.js"></script></div></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div><span>79k</span></div></footer><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach((t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")}))},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicRibbon.min.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/star.min.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/love.min.js"></script><script src="//cdn.jsdelivr.net/npm/highlight.js@11.5.1/styles/monokai.min.css.js"></script><script src="/js/scrollAnimation.js"></script><script src="/js/loading.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>
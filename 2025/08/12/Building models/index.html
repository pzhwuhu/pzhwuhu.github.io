<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="light"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/avatar_1.jpg"><link rel="icon" href="/img/avatar_1.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="pzhwuhu"><meta name="keywords" content=""><meta name="description" content="è¿™æ˜¯æˆ‘å­¦ä¹ Pytorchæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š             torch.nn.Module &amp; torch.nn.ParameterIn this section, weâ€™ll be discussing some of the tools PyTorch makes available for building deep lear"><meta property="og:type" content="article"><meta property="og:title" content="Pytorch Tutorial-Building models"><meta property="og:url" content="http://pzhwuhu.github.io/2025/08/12/Building%20models/index.html"><meta property="og:site_name" content="é¹å“¥"><meta property="og:description" content="è¿™æ˜¯æˆ‘å­¦ä¹ Pytorchæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š             torch.nn.Module &amp; torch.nn.ParameterIn this section, weâ€™ll be discussing some of the tools PyTorch makes available for building deep lear"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://pzhwuhu.github.io/myimg/%E5%A4%A7%E9%B1%BC%E6%B5%B7%E6%A3%A00.jpg"><meta property="article:published_time" content="2025-08-12T07:41:00.000Z"><meta property="article:modified_time" content="2025-08-12T07:57:28.717Z"><meta property="article:author" content="pzhwuhu"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Python"><meta property="article:tag" content="ML"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://pzhwuhu.github.io/myimg/%E5%A4%A7%E9%B1%BC%E6%B5%B7%E6%A3%A00.jpg"><title>Pytorch Tutorial-Building models - é¹å“¥</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/custom_dark_mode.css"><link rel="stylesheet" href="/css/scroll_animation.css"><link rel="stylesheet" href="/css/mac.css"><link rel="stylesheet" href="/css/loading.css"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/gradient.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"pzhwuhu.github.io",root:"/",version:"1.9.8",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!1,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:{key:null},google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"cp3oK6SJdvk7QCx0TiMpNSdI-gzGzoHsz",app_key:"B9u5rSWtXwzrQl56fuCE0o9M",server_url:"https://cp3ok6sj.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2024-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>if(!Fluid.ctx.dnt){var _hmt=_hmt||[];!function(){var t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?[object Object]";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()}</script><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:90vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>èº«å¦‚èŠ¥å­ï¼Œå¿ƒè—é¡»å¼¥</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>é¦–é¡µ</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>å½’æ¡£</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>åˆ†ç±»</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>æ ‡ç­¾</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>å…³äº</span></a></li><li class="nav-item"><a class="nav-link" href="/links/" target="_self"><i class="iconfont icon-link-fill"></i> <span>å‹é“¾</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url('/myimg/%E5%93%88%E5%B0%94%E7%9A%84%E7%A7%BB%E5%8A%A8%E5%9F%8E%E5%A0%A12.png') no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="Pytorch Tutorial-Building models"></span></div><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> pzhwuhu </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-08-12 15:41" pubdate>2025å¹´8æœˆ12æ—¥ ä¸‹åˆ</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2.4k å­— </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 20 åˆ†é’Ÿ </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> æ¬¡</span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Pytorch" id="heading-edf7dd0dd9dbc3b022a1ef1fdb134ea1" role="tab" data-toggle="collapse" href="#collapse-edf7dd0dd9dbc3b022a1ef1fdb134ea1" aria-expanded="true">Pytorch <span class="list-group-count">(5)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-edf7dd0dd9dbc3b022a1ef1fdb134ea1" role="tabpanel" aria-labelledby="heading-edf7dd0dd9dbc3b022a1ef1fdb134ea1"><div class="category-post-list"><a href="/2025/08/12/Autograd/" title="Pytorch Tutorial-Autograd" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-Autograd</span> </a><a href="/2025/08/12/Building%20models/" title="Pytorch Tutorial-Building models" class="list-group-item list-group-item-action active"><span class="category-post">Pytorch Tutorial-Building models</span> </a><a href="/2025/08/12/TensorBoard/" title="Pytorch Tutorial-TensorBoard" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-TensorBoard</span> </a><a href="/2025/08/03/Tensors/" title="Pytorch Tutorial-Tensors" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-Tensors</span> </a><a href="/2025/08/12/Training%20Models/" title="Pytorch Tutorial-Training Models" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-Training Models</span></a></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">Pytorch Tutorial-Building models</h1><div class="markdown-body"><div class="note note-info"><p>è¿™æ˜¯æˆ‘å­¦ä¹ Pytorchæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š</p></div><h1 id="torch-nn-Module-torch-nn-Parameter"><a href="#torch-nn-Module-torch-nn-Parameter" class="headerlink" title="torch.nn.Module &amp; torch.nn.Parameter"></a>torch.nn.Module &amp; torch.nn.Parameter</h1><p>In this section, weâ€™ll be discussing some of the tools PyTorch makes available for building deep learning networks.</p><p>Except forÂ <code>Parameter</code>, the classes we discuss in this section are all subclasses ofÂ <code>torch.nn.Module</code>. This is the PyTorch base class meant to encapsulate(å°è£…) behaviors specific to PyTorch Models and their components.</p><p>One important behavior ofÂ <code>torch.nn.Module</code>Â is <strong>registering parameters</strong>. If a particularÂ <code>Module</code>Â subclass has learning weights, these weights are expressed as instances ofÂ <code>torch.nn.Parameter</code>. TheÂ <code>Parameter</code>Â class is a subclass ofÂ <code>torch.Tensor</code>, with the special behavior that when they are assigned as attributes of aÂ <code>Module</code>, they are added to the list of that modules parameters. These parameters may be accessed through theÂ <code>parameters()</code>Â method on theÂ <code>Module</code>Â class.</p><p>As a simple example, hereâ€™s a very simple model with two linear layers and an activation function. Weâ€™ll create an instance of it and ask it to report on its parameters:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TinyModel</span>(torch.nn.Module):<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(TinyModel, <span class="hljs-variable language_">self</span>).__init__()<br>        <br>        <span class="hljs-variable language_">self</span>.linear1 = torch.nn.Linear(<span class="hljs-number">100</span>, <span class="hljs-number">200</span>)<br>        <span class="hljs-variable language_">self</span>.activation = torch.nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.linear2 = torch.nn.Linear(<span class="hljs-number">200</span>, <span class="hljs-number">10</span>)<br>        <span class="hljs-variable language_">self</span>.softmax = torch.nn.Softmax()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.linear1(x)<br>        x = <span class="hljs-variable language_">self</span>.activation(x)<br>        x = <span class="hljs-variable language_">self</span>.linear2(x)<br>        x = <span class="hljs-variable language_">self</span>.softmax(x)<br>        <span class="hljs-keyword">return</span> x<br><br>tinymodel = TinyModel()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The model:&#x27;</span>)<br><span class="hljs-built_in">print</span>(tinymodel)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nJust one layer:&#x27;</span>)<br><span class="hljs-built_in">print</span>(tinymodel.linear2)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nModel params:&#x27;</span>)<br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> tinymodel.parameters():<br>    <span class="hljs-built_in">print</span>(param)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nLayer params:&#x27;</span>)<br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> tinymodel.linear2.parameters():<br>    <span class="hljs-built_in">print</span>(param)<br><br>The model:<br>TinyModel(<br>  (linear1): Linear(in_features=<span class="hljs-number">100</span>, out_features=<span class="hljs-number">200</span>, bias=<span class="hljs-literal">True</span>)<br>  (activation): ReLU()<br>  (linear2): Linear(in_features=<span class="hljs-number">200</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br>  (softmax): Softmax(dim=<span class="hljs-literal">None</span>)<br>)<br><br>Just one layer:<br>Linear(in_features=<span class="hljs-number">200</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br><br>Model params:<br>Parameter containing:<br>tensor([[-<span class="hljs-number">0.0897</span>,  <span class="hljs-number">0.0595</span>,  <span class="hljs-number">0.0172</span>,  ...,  <span class="hljs-number">0.0947</span>, -<span class="hljs-number">0.0384</span>, -<span class="hljs-number">0.0024</span>],<br>        [ <span class="hljs-number">0.0102</span>, -<span class="hljs-number">0.0393</span>, -<span class="hljs-number">0.0865</span>,  ...,  <span class="hljs-number">0.0961</span>,  <span class="hljs-number">0.0694</span>,  <span class="hljs-number">0.0555</span>],<br>        [-<span class="hljs-number">0.0251</span>, -<span class="hljs-number">0.0372</span>,  <span class="hljs-number">0.0264</span>,  ...,  <span class="hljs-number">0.0535</span>, -<span class="hljs-number">0.0535</span>,  <span class="hljs-number">0.0745</span>],<br>        ...,<br>        [-<span class="hljs-number">0.0554</span>, -<span class="hljs-number">0.0434</span>, -<span class="hljs-number">0.0032</span>,  ..., -<span class="hljs-number">0.0441</span>, -<span class="hljs-number">0.0671</span>,  <span class="hljs-number">0.0100</span>],<br>        [ <span class="hljs-number">0.0469</span>, -<span class="hljs-number">0.0174</span>,  <span class="hljs-number">0.0883</span>,  ..., -<span class="hljs-number">0.0825</span>, -<span class="hljs-number">0.0478</span>,  <span class="hljs-number">0.0232</span>],<br>        [ <span class="hljs-number">0.0877</span>, -<span class="hljs-number">0.0416</span>, -<span class="hljs-number">0.0567</span>,  ..., -<span class="hljs-number">0.0455</span>, -<span class="hljs-number">0.0185</span>, -<span class="hljs-number">0.0559</span>]],<br>       requires_grad=<span class="hljs-literal">True</span>)<br>Parameter containing:<br>tensor([ <span class="hljs-number">0.0692</span>, -<span class="hljs-number">0.0213</span>,  <span class="hljs-number">0.0033</span>,  <span class="hljs-number">0.0528</span>,  <span class="hljs-number">0.0394</span>, -<span class="hljs-number">0.0518</span>, -<span class="hljs-number">0.0535</span>, -<span class="hljs-number">0.0269</span>,<br>         <span class="hljs-number">0.0172</span>, -<span class="hljs-number">0.0897</span>,  <span class="hljs-number">0.0809</span>,  <span class="hljs-number">0.0125</span>, -<span class="hljs-number">0.0566</span>, -<span class="hljs-number">0.0490</span>, -<span class="hljs-number">0.0566</span>,  <span class="hljs-number">0.0478</span>,<br>        -<span class="hljs-number">0.0488</span>,  <span class="hljs-number">0.0989</span>, -<span class="hljs-number">0.0641</span>, -<span class="hljs-number">0.0068</span>,  <span class="hljs-number">0.0420</span>,  <span class="hljs-number">0.0358</span>,  <span class="hljs-number">0.0186</span>,  <span class="hljs-number">0.0748</span>,<br>        -<span class="hljs-number">0.0308</span>,  <span class="hljs-number">0.0472</span>,  <span class="hljs-number">0.0568</span>,  <span class="hljs-number">0.0026</span>, -<span class="hljs-number">0.0920</span>, -<span class="hljs-number">0.0553</span>,  <span class="hljs-number">0.0737</span>,  <span class="hljs-number">0.0881</span>,<br>        -<span class="hljs-number">0.0992</span>,  <span class="hljs-number">0.0300</span>, -<span class="hljs-number">0.0234</span>, -<span class="hljs-number">0.0443</span>,  <span class="hljs-number">0.0221</span>, -<span class="hljs-number">0.0552</span>, -<span class="hljs-number">0.0067</span>,  <span class="hljs-number">0.0612</span>,<br>         <span class="hljs-number">0.0281</span>, -<span class="hljs-number">0.0199</span>, -<span class="hljs-number">0.0818</span>,  <span class="hljs-number">0.0608</span>,  <span class="hljs-number">0.0975</span>, -<span class="hljs-number">0.0069</span>,  <span class="hljs-number">0.0923</span>, -<span class="hljs-number">0.0741</span>,<br>         <span class="hljs-number">0.0516</span>, -<span class="hljs-number">0.0787</span>, -<span class="hljs-number">0.0593</span>, -<span class="hljs-number">0.0303</span>,  <span class="hljs-number">0.0115</span>,  <span class="hljs-number">0.0701</span>, -<span class="hljs-number">0.0171</span>,  <span class="hljs-number">0.0291</span>,<br>         <span class="hljs-number">0.0152</span>,  <span class="hljs-number">0.0424</span>, -<span class="hljs-number">0.0106</span>, -<span class="hljs-number">0.0568</span>,  <span class="hljs-number">0.0689</span>,  <span class="hljs-number">0.0308</span>,  <span class="hljs-number">0.0863</span>, -<span class="hljs-number">0.0436</span>,<br>         <span class="hljs-number">0.0061</span>,  <span class="hljs-number">0.0822</span>, -<span class="hljs-number">0.0556</span>, -<span class="hljs-number">0.0668</span>,  <span class="hljs-number">0.0828</span>,  <span class="hljs-number">0.0758</span>,  <span class="hljs-number">0.0888</span>, -<span class="hljs-number">0.0535</span>,<br>         <span class="hljs-number">0.0648</span>,  <span class="hljs-number">0.0160</span>, -<span class="hljs-number">0.0932</span>,  <span class="hljs-number">0.0787</span>,  <span class="hljs-number">0.0546</span>, -<span class="hljs-number">0.0973</span>,  <span class="hljs-number">0.0973</span>,  <span class="hljs-number">0.0908</span>,<br>         <span class="hljs-number">0.0108</span>, -<span class="hljs-number">0.0090</span>,  <span class="hljs-number">0.0644</span>,  <span class="hljs-number">0.0990</span>,  <span class="hljs-number">0.0384</span>,  <span class="hljs-number">0.0852</span>,  <span class="hljs-number">0.0864</span>,  <span class="hljs-number">0.0565</span>,<br>        -<span class="hljs-number">0.0974</span>,  <span class="hljs-number">0.0768</span>,  <span class="hljs-number">0.0337</span>,  <span class="hljs-number">0.0590</span>, -<span class="hljs-number">0.0362</span>,  <span class="hljs-number">0.0914</span>,  <span class="hljs-number">0.0038</span>,  <span class="hljs-number">0.0516</span>,<br>        -<span class="hljs-number">0.0632</span>, -<span class="hljs-number">0.0569</span>, -<span class="hljs-number">0.0475</span>, -<span class="hljs-number">0.0564</span>, -<span class="hljs-number">0.0192</span>,  <span class="hljs-number">0.0279</span>, -<span class="hljs-number">0.0243</span>, -<span class="hljs-number">0.0621</span>,<br>        -<span class="hljs-number">0.0559</span>,  <span class="hljs-number">0.0921</span>, -<span class="hljs-number">0.0583</span>, -<span class="hljs-number">0.0508</span>,  <span class="hljs-number">0.0401</span>,  <span class="hljs-number">0.0414</span>, -<span class="hljs-number">0.0770</span>, -<span class="hljs-number">0.0378</span>,<br>        -<span class="hljs-number">0.0786</span>, -<span class="hljs-number">0.0110</span>, -<span class="hljs-number">0.0289</span>, -<span class="hljs-number">0.0778</span>,  <span class="hljs-number">0.0427</span>, -<span class="hljs-number">0.0105</span>,  <span class="hljs-number">0.0680</span>,  <span class="hljs-number">0.0146</span>,<br>        -<span class="hljs-number">0.0859</span>,  <span class="hljs-number">0.0440</span>, -<span class="hljs-number">0.0420</span>,  <span class="hljs-number">0.0613</span>,  <span class="hljs-number">0.0321</span>,  <span class="hljs-number">0.0289</span>,  <span class="hljs-number">0.0668</span>, -<span class="hljs-number">0.0028</span>,<br>        -<span class="hljs-number">0.0421</span>, -<span class="hljs-number">0.0372</span>,  <span class="hljs-number">0.0391</span>,  <span class="hljs-number">0.0479</span>, -<span class="hljs-number">0.0232</span>, -<span class="hljs-number">0.0610</span>, -<span class="hljs-number">0.0355</span>, -<span class="hljs-number">0.0896</span>,<br>         <span class="hljs-number">0.0864</span>,  <span class="hljs-number">0.0345</span>, -<span class="hljs-number">0.0252</span>, -<span class="hljs-number">0.0385</span>,  <span class="hljs-number">0.0832</span>,  <span class="hljs-number">0.0868</span>, -<span class="hljs-number">0.0514</span>,  <span class="hljs-number">0.0178</span>,<br>         <span class="hljs-number">0.0716</span>,  <span class="hljs-number">0.0796</span>, -<span class="hljs-number">0.0794</span>, -<span class="hljs-number">0.0538</span>, -<span class="hljs-number">0.0163</span>, -<span class="hljs-number">0.0929</span>, -<span class="hljs-number">0.0643</span>,  <span class="hljs-number">0.0782</span>,<br>        -<span class="hljs-number">0.0047</span>,  <span class="hljs-number">0.0024</span>, -<span class="hljs-number">0.0610</span>, -<span class="hljs-number">0.0259</span>,  <span class="hljs-number">0.0719</span>,  <span class="hljs-number">0.0840</span>,  <span class="hljs-number">0.0946</span>, -<span class="hljs-number">0.0291</span>,<br>         <span class="hljs-number">0.0131</span>, -<span class="hljs-number">0.0157</span>,  <span class="hljs-number">0.0309</span>, -<span class="hljs-number">0.0375</span>, -<span class="hljs-number">0.0800</span>, -<span class="hljs-number">0.0594</span>, -<span class="hljs-number">0.0233</span>, -<span class="hljs-number">0.0928</span>,<br>        -<span class="hljs-number">0.0028</span>, -<span class="hljs-number">0.0729</span>,  <span class="hljs-number">0.0889</span>, -<span class="hljs-number">0.0377</span>, -<span class="hljs-number">0.0685</span>,  <span class="hljs-number">0.0974</span>, -<span class="hljs-number">0.0860</span>, -<span class="hljs-number">0.0819</span>,<br>        -<span class="hljs-number">0.0918</span>, -<span class="hljs-number">0.0750</span>, -<span class="hljs-number">0.0327</span>, -<span class="hljs-number">0.0245</span>, -<span class="hljs-number">0.0058</span>, -<span class="hljs-number">0.0875</span>, -<span class="hljs-number">0.0667</span>, -<span class="hljs-number">0.0569</span>,<br>         <span class="hljs-number">0.0075</span>,  <span class="hljs-number">0.0986</span>,  <span class="hljs-number">0.0977</span>, -<span class="hljs-number">0.0291</span>,  <span class="hljs-number">0.0081</span>,  <span class="hljs-number">0.0127</span>,  <span class="hljs-number">0.0544</span>,  <span class="hljs-number">0.0711</span>,<br>         <span class="hljs-number">0.0910</span>,  <span class="hljs-number">0.0522</span>, -<span class="hljs-number">0.0874</span>, -<span class="hljs-number">0.0217</span>,  <span class="hljs-number">0.0454</span>, -<span class="hljs-number">0.0726</span>,  <span class="hljs-number">0.0791</span>, -<span class="hljs-number">0.0459</span>],<br>       requires_grad=<span class="hljs-literal">True</span>)<br>Parameter containing:<br>tensor([[-<span class="hljs-number">0.0508</span>,  <span class="hljs-number">0.0529</span>,  <span class="hljs-number">0.0234</span>,  ..., -<span class="hljs-number">0.0385</span>,  <span class="hljs-number">0.0078</span>, -<span class="hljs-number">0.0030</span>],<br>        [ <span class="hljs-number">0.0281</span>,  <span class="hljs-number">0.0437</span>, -<span class="hljs-number">0.0461</span>,  ..., -<span class="hljs-number">0.0655</span>, -<span class="hljs-number">0.0253</span>, -<span class="hljs-number">0.0222</span>],<br>        [ <span class="hljs-number">0.0243</span>,  <span class="hljs-number">0.0178</span>, -<span class="hljs-number">0.0009</span>,  ...,  <span class="hljs-number">0.0383</span>, -<span class="hljs-number">0.0507</span>, -<span class="hljs-number">0.0083</span>],<br>        ...,<br>        [-<span class="hljs-number">0.0700</span>, -<span class="hljs-number">0.0090</span>,  <span class="hljs-number">0.0153</span>,  ...,  <span class="hljs-number">0.0161</span>,  <span class="hljs-number">0.0610</span>,  <span class="hljs-number">0.0687</span>],<br>        [-<span class="hljs-number">0.0509</span>, -<span class="hljs-number">0.0291</span>, -<span class="hljs-number">0.0591</span>,  ...,  <span class="hljs-number">0.0173</span>, -<span class="hljs-number">0.0191</span>, -<span class="hljs-number">0.0705</span>],<br>        [-<span class="hljs-number">0.0090</span>,  <span class="hljs-number">0.0428</span>, -<span class="hljs-number">0.0528</span>,  ...,  <span class="hljs-number">0.0278</span>, -<span class="hljs-number">0.0153</span>, -<span class="hljs-number">0.0266</span>]],<br>       requires_grad=<span class="hljs-literal">True</span>)<br>Parameter containing:<br>tensor([-<span class="hljs-number">0.0357</span>, -<span class="hljs-number">0.0617</span>,  <span class="hljs-number">0.0027</span>, -<span class="hljs-number">0.0098</span>, -<span class="hljs-number">0.0083</span>, -<span class="hljs-number">0.0461</span>, -<span class="hljs-number">0.0076</span>,  <span class="hljs-number">0.0510</span>,<br>        -<span class="hljs-number">0.0564</span>,  <span class="hljs-number">0.0298</span>], requires_grad=<span class="hljs-literal">True</span>)<br><br>Layer params:<br>Parameter containing:<br>tensor([[-<span class="hljs-number">0.0508</span>,  <span class="hljs-number">0.0529</span>,  <span class="hljs-number">0.0234</span>,  ..., -<span class="hljs-number">0.0385</span>,  <span class="hljs-number">0.0078</span>, -<span class="hljs-number">0.0030</span>],<br>        [ <span class="hljs-number">0.0281</span>,  <span class="hljs-number">0.0437</span>, -<span class="hljs-number">0.0461</span>,  ..., -<span class="hljs-number">0.0655</span>, -<span class="hljs-number">0.0253</span>, -<span class="hljs-number">0.0222</span>],<br>        [ <span class="hljs-number">0.0243</span>,  <span class="hljs-number">0.0178</span>, -<span class="hljs-number">0.0009</span>,  ...,  <span class="hljs-number">0.0383</span>, -<span class="hljs-number">0.0507</span>, -<span class="hljs-number">0.0083</span>],<br>        ...,<br>        [-<span class="hljs-number">0.0700</span>, -<span class="hljs-number">0.0090</span>,  <span class="hljs-number">0.0153</span>,  ...,  <span class="hljs-number">0.0161</span>,  <span class="hljs-number">0.0610</span>,  <span class="hljs-number">0.0687</span>],<br>        [-<span class="hljs-number">0.0509</span>, -<span class="hljs-number">0.0291</span>, -<span class="hljs-number">0.0591</span>,  ...,  <span class="hljs-number">0.0173</span>, -<span class="hljs-number">0.0191</span>, -<span class="hljs-number">0.0705</span>],<br>        [-<span class="hljs-number">0.0090</span>,  <span class="hljs-number">0.0428</span>, -<span class="hljs-number">0.0528</span>,  ...,  <span class="hljs-number">0.0278</span>, -<span class="hljs-number">0.0153</span>, -<span class="hljs-number">0.0266</span>]],<br>       requires_grad=<span class="hljs-literal">True</span>)<br>Parameter containing:<br>tensor([-<span class="hljs-number">0.0357</span>, -<span class="hljs-number">0.0617</span>,  <span class="hljs-number">0.0027</span>, -<span class="hljs-number">0.0098</span>, -<span class="hljs-number">0.0083</span>, -<span class="hljs-number">0.0461</span>, -<span class="hljs-number">0.0076</span>,  <span class="hljs-number">0.0510</span>,<br>        -<span class="hljs-number">0.0564</span>,  <span class="hljs-number">0.0298</span>], requires_grad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>This shows the fundamental structure of a PyTorch model: there is anÂ <code>__init__()</code>Â method that defines the layers and other components of a model, and aÂ <code>forward()</code>Â method where the computation gets done. Note that we can print the model, or any of its submodules, to learn about its structure.</strong></li></ul><h1 id="Common-Layer-Types"><a href="#Common-Layer-Types" class="headerlink" title="Common Layer Types"></a>Common Layer Types</h1><h2 id="Linear-Layers"><a href="#Linear-Layers" class="headerlink" title="Linear Layers"></a>Linear Layers</h2><p>The most basic type of neural network layer is aÂ <em>linear</em>Â orÂ <em>fully connected</em>Â layer. This is a layer where <strong>every input influences every output</strong> of the layer to a degree specified by the layerâ€™s <code>weights</code>. If a model hasÂ <em>m</em>Â inputs andÂ <em>n</em>Â outputs, the weights will be anÂ <em>m *Â n</em>Â matrix. For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python">lin = torch.nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>x = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Input:&#x27;</span>)<br><span class="hljs-built_in">print</span>(x)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nWeight and Bias parameters:&#x27;</span>)<br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> lin.parameters():<br>    <span class="hljs-built_in">print</span>(param)<br><br>y = lin(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nOutput:&#x27;</span>)<br><span class="hljs-built_in">print</span>(y)<br><br>Input:<br>tensor([[<span class="hljs-number">0.2807</span>, <span class="hljs-number">0.5842</span>, <span class="hljs-number">0.7967</span>]])<br><br>Weight <span class="hljs-keyword">and</span> Bias parameters:<br>Parameter containing:<br>tensor([[-<span class="hljs-number">0.1719</span>,  <span class="hljs-number">0.4691</span>, -<span class="hljs-number">0.0654</span>],<br>        [-<span class="hljs-number">0.2522</span>,  <span class="hljs-number">0.5453</span>, -<span class="hljs-number">0.5438</span>]], requires_grad=<span class="hljs-literal">True</span>)<br>Parameter containing:<br>tensor([<span class="hljs-number">0.2956</span>, <span class="hljs-number">0.2001</span>], requires_grad=<span class="hljs-literal">True</span>)<br><br>Output:<br>tensor([[<span class="hljs-number">0.4693</span>, <span class="hljs-number">0.0146</span>]], grad_fn=&lt;AddmmBackward&gt;)<br></code></pre></td></tr></table></figure><ul><li>Parameterä¼šè‡ªåŠ¨å¼€å¯<code>autograd</code></li><li>Linear layers are used widely in deep learning models. One of the most common places youâ€™ll see them is in <code>classifier models</code></li></ul><hr><h2 id="Convolutional-Layers"><a href="#Convolutional-Layers" class="headerlink" title="Convolutional Layers"></a>Convolutional Layers</h2><ul><li><em>Convolutional</em>Â layers are built to handle data with <strong>a high degree of spatial correlation</strong>. They are very commonly used in computer vision, where they detect close groupings of features which the compose into higher-level features. They pop up in other contexts too - for example, in NLP applications, where the a wordâ€™s immediate context (that is, the other words nearby in the sequence) can affect the meaning of a sentence.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LeNet</span>(torch.nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(LeNet, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 1 input image channel (black &amp; white), 6 output channels, 3x3 square convolution</span><br>        <span class="hljs-comment"># kernel</span><br>        <span class="hljs-variable language_">self</span>.conv1 = torch.nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = torch.nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>)<br>        <span class="hljs-comment"># an affine operation: y = Wx + b</span><br>        <span class="hljs-variable language_">self</span>.fc1 = torch.nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">6</span> * <span class="hljs-number">6</span>, <span class="hljs-number">120</span>)  <span class="hljs-comment"># 6*6 from image dimension</span><br>        <span class="hljs-variable language_">self</span>.fc2 = torch.nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = torch.nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># Max pooling over a (2, 2) window</span><br>        x = F.max_pool2d(F.relu(<span class="hljs-variable language_">self</span>.conv1(x)), (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>        <span class="hljs-comment"># If the size is a square you can only specify a single number</span><br>        x = F.max_pool2d(F.relu(<span class="hljs-variable language_">self</span>.conv2(x)), <span class="hljs-number">2</span>)<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.num_flat_features(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">num_flat_features</span>(<span class="hljs-params">self, x</span>):<br>        size = x.size()[<span class="hljs-number">1</span>:]  <span class="hljs-comment"># all dimensions except the batch dimension</span><br>        num_features = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> size:<br>            num_features *= s<br>        <span class="hljs-keyword">return</span> num_features<br></code></pre></td></tr></table></figure><ul><li>å·ç§¯å±‚æ„é€ å‡½æ•°çš„<strong>ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¾“å…¥é€šé“çš„æ•°é‡</strong>ï¼Œ<strong>ç¬¬äºŒä¸ªå‚æ•°æ˜¯è¾“å‡ºç‰¹å¾çš„æ•°é‡</strong>ï¼Œ<strong>ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯çª—å£æˆ–<code>kernel</code>å†…æ ¸å¤§å°</strong></li></ul><hr><p>å…³äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­ <strong>å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰</strong>ã€<strong>ReLUæ¿€æ´»å‡½æ•°</strong> å’Œ <strong>æœ€å¤§æ± åŒ–å±‚ï¼ˆMax Pooling Layerï¼‰</strong> çš„å¤„ç†æµç¨‹åŠå…¶ä½œç”¨ã€‚ä»¥ä¸‹æ˜¯é€æ­¥è§£æï¼š</p><h3 id="1-å·ç§¯å±‚ï¼ˆConvolutional-Layerï¼‰çš„è¾“å‡º"><a href="#1-å·ç§¯å±‚ï¼ˆConvolutional-Layerï¼‰çš„è¾“å‡º" class="headerlink" title="1. å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰çš„è¾“å‡º"></a>1. å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰çš„è¾“å‡º</h3><ul><li><strong>è¾“å…¥å‡è®¾</strong>ï¼šå‡è®¾è¾“å…¥æ˜¯ä¸€ä¸ªå•é€šé“ï¼ˆç°åº¦ï¼‰çš„ <code>32x32</code> å›¾åƒï¼Œç»è¿‡ç¬¬ä¸€å±‚å·ç§¯æ“ä½œï¼š<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.conv1 = torch.nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)  <span class="hljs-comment"># è¾“å…¥é€šé“1ï¼Œè¾“å‡ºé€šé“6ï¼Œå·ç§¯æ ¸5x5</span><br></code></pre></td></tr></table></figure></li><li><strong>è¾“å‡ºå°ºå¯¸</strong>ï¼š<br>å·ç§¯åè¾“å‡ºçš„å°ºå¯¸è®¡ç®—å…¬å¼ä¸ºï¼š<br>$$<br>\text{è¾“å‡ºå°ºå¯¸} &#x3D; \left\lfloor \frac{\text{è¾“å…¥å°ºå¯¸} - \text{å·ç§¯æ ¸å°ºå¯¸} + 2 \times \text{å¡«å……}}{\text{æ­¥é•¿}} \right\rfloor + 1<br>$$<br>é»˜è®¤æƒ…å†µä¸‹ï¼Œ<code>padding=0</code>ï¼ˆæ— å¡«å……ï¼‰ï¼Œ<code>stride=1</code>ï¼ˆæ­¥é•¿ä¸º1ï¼‰ï¼Œå› æ­¤ï¼š32 - 5 + 1 &#x3D; 28<ul><li><strong>è¾“å‡ºå¼ é‡å½¢çŠ¶</strong>ï¼š<code>[batch_size, 6, 28, 28]</code><br>ï¼ˆ6ä¸ªé€šé“ï¼Œæ¯ä¸ªé€šé“çš„æ¿€æ´»å›¾å¤§å°ä¸º <code>28x28</code>ï¼‰ã€‚</li></ul></li></ul><h3 id="2-ReLUæ¿€æ´»å‡½æ•°çš„ä½œç”¨"><a href="#2-ReLUæ¿€æ´»å‡½æ•°çš„ä½œç”¨" class="headerlink" title="2. ReLUæ¿€æ´»å‡½æ•°çš„ä½œç”¨"></a>2. ReLUæ¿€æ´»å‡½æ•°çš„ä½œç”¨</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">F.relu(<span class="hljs-variable language_">self</span>.conv1(x))<br></code></pre></td></tr></table></figure><ul><li><strong>ReLUï¼ˆRectified Linear Unitï¼‰</strong>ï¼šå®šä¹‰ä¸º $\text{ReLU}(x) &#x3D; \max(0, x)$ ã€‚</li><li><strong>åŠŸèƒ½</strong>ï¼š<ol><li><strong>å¼•å…¥éçº¿æ€§</strong>ï¼šä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚</li><li><strong>ç¨€ç–æ¿€æ´»</strong>ï¼šå°†è´Ÿå€¼ç½®é›¶ï¼Œä¿ç•™æ­£å€¼ï¼Œå¢å¼ºæ¨¡å‹çš„ç¨€ç–æ€§ã€‚</li><li><strong>ç¼“è§£æ¢¯åº¦æ¶ˆå¤±</strong>ï¼šç›¸æ¯” Sigmoid&#x2F;Tanhï¼ŒReLU çš„æ¢¯åº¦åœ¨æ­£åŒºé—´æ’ä¸º1ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚</li></ol></li><li><strong>è¾“å‡ºå½¢çŠ¶</strong>ï¼šä¸è¾“å…¥ç›¸åŒï¼Œä»ä¸º <code>[batch_size, 6, 28, 28]</code>ã€‚</li></ul><h3 id="3-æœ€å¤§æ± åŒ–å±‚ï¼ˆMax-Poolingï¼‰çš„ç»†èŠ‚"><a href="#3-æœ€å¤§æ± åŒ–å±‚ï¼ˆMax-Poolingï¼‰çš„ç»†èŠ‚" class="headerlink" title="3. æœ€å¤§æ± åŒ–å±‚ï¼ˆMax Poolingï¼‰çš„ç»†èŠ‚"></a>3. æœ€å¤§æ± åŒ–å±‚ï¼ˆMax Poolingï¼‰çš„ç»†èŠ‚</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">F.max_pool2d(..., (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))  <span class="hljs-comment"># 2x2çš„æ± åŒ–çª—å£</span><br></code></pre></td></tr></table></figure><ul><li><strong>ç›®çš„</strong>ï¼šé™ä½ç©ºé—´ç»´åº¦ï¼ˆä¸‹é‡‡æ ·ï¼‰ï¼Œå‡å°‘è®¡ç®—é‡å¹¶å¢å¼ºå¹³ç§»ä¸å˜æ€§ã€‚</li><li><strong>æ“ä½œè§„åˆ™</strong>ï¼š<ul><li>å°†è¾“å…¥æ¿€æ´»å›¾åˆ’åˆ†ä¸ºä¸é‡å çš„ <code>2x2</code> åŒºåŸŸã€‚</li><li>å¯¹æ¯ä¸ªåŒºåŸŸå–æœ€å¤§å€¼ï¼Œä½œä¸ºè¾“å‡ºã€‚</li><li><strong>æ­¥é•¿é»˜è®¤ç­‰äºæ± åŒ–çª—å£å¤§å°</strong>ï¼ˆå³ <code>stride=2</code>ï¼‰ï¼Œå› æ­¤è¾“å‡ºå°ºå¯¸å‡åŠã€‚</li></ul></li><li><strong>è®¡ç®—ç¤ºä¾‹</strong>ï¼š<ul><li>è¾“å…¥å°ºå¯¸ï¼š<code>[batch_size, 6, 28, 28]</code>ã€‚</li><li>è¾“å‡ºå°ºå¯¸ï¼š<br>$\left\lfloor \frac{28 - 2}{2} \right\rfloor + 1 &#x3D; 14$</li><li><strong>è¾“å‡ºå¼ é‡å½¢çŠ¶</strong>ï¼š<code>[batch_size, 6, 14, 14]</code>ã€‚</li></ul></li></ul><h4 id="ä¸ºä»€ä¹ˆé€‰æ‹©æœ€å¤§å€¼ï¼Ÿ"><a href="#ä¸ºä»€ä¹ˆé€‰æ‹©æœ€å¤§å€¼ï¼Ÿ" class="headerlink" title="ä¸ºä»€ä¹ˆé€‰æ‹©æœ€å¤§å€¼ï¼Ÿ"></a>ä¸ºä»€ä¹ˆé€‰æ‹©æœ€å¤§å€¼ï¼Ÿ</h4><ul><li><strong>ä¿ç•™æœ€æ˜¾è‘—ç‰¹å¾</strong>ï¼šæœ€å¤§å€¼ä»£è¡¨è¯¥åŒºåŸŸæœ€å¼ºçƒˆçš„æ¿€æ´»å“åº”ï¼Œæœ‰åŠ©äºä¿ç•™é‡è¦ç‰¹å¾ï¼ˆå¦‚è¾¹ç¼˜ã€çº¹ç†ï¼‰ã€‚</li><li><strong>æŠ‘åˆ¶å™ªå£°</strong>ï¼šå¿½ç•¥éæœ€å¤§å€¼ï¼Œé™ä½å™ªå£°å¹²æ‰°ã€‚</li></ul><h3 id="4-ç»´åº¦å˜åŒ–çš„ç›´è§‚ç†è§£"><a href="#4-ç»´åº¦å˜åŒ–çš„ç›´è§‚ç†è§£" class="headerlink" title="4. ç»´åº¦å˜åŒ–çš„ç›´è§‚ç†è§£"></a>4. ç»´åº¦å˜åŒ–çš„ç›´è§‚ç†è§£</h3><table><thead><tr><th>æ“ä½œ</th><th>è¾“å…¥å½¢çŠ¶</th><th>è¾“å‡ºå½¢çŠ¶</th><th>å…³é”®ä½œç”¨</th></tr></thead><tbody><tr><td>å·ç§¯ï¼ˆConv1ï¼‰</td><td><code>[1, 1, 32, 32]</code></td><td><code>[1, 6, 28, 28]</code></td><td>æå–å±€éƒ¨ç‰¹å¾ï¼Œå¢åŠ é€šé“æ•°</td></tr><tr><td>ReLU</td><td><code>[1, 6, 28, 28]</code></td><td><code>[1, 6, 28, 28]</code></td><td>å¼•å…¥éçº¿æ€§ï¼Œè¿‡æ»¤è´Ÿå€¼</td></tr><tr><td>æœ€å¤§æ± åŒ–</td><td><code>[1, 6, 28, 28]</code></td><td><code>[1, 6, 14, 14]</code></td><td>é™ä½åˆ†è¾¨ç‡ï¼Œå¢å¼ºé²æ£’æ€§</td></tr></tbody></table><h3 id="5-ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›æ­¥éª¤ï¼Ÿ"><a href="#5-ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›æ­¥éª¤ï¼Ÿ" class="headerlink" title="5. ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›æ­¥éª¤ï¼Ÿ"></a>5. ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›æ­¥éª¤ï¼Ÿ</h3><ol><li><strong>å·ç§¯å±‚</strong>ï¼š<ul><li>é€šè¿‡å±€éƒ¨æ„Ÿå—é‡<strong>æå–ç©ºé—´ç‰¹å¾</strong>ï¼ˆå¦‚è¾¹ç¼˜ã€è§’ç‚¹ï¼‰ã€‚</li><li>ä½¿ç”¨å¤šä¸ªå·ç§¯æ ¸ï¼ˆé€šé“ï¼‰<strong>æ•æ‰ä¸åŒç‰¹å¾æ¨¡å¼</strong>ã€‚</li></ul></li><li><strong>ReLU</strong>ï¼š<ul><li>è§£å†³çº¿æ€§æ¨¡å‹çš„å±€é™æ€§ï¼Œä½¿ç½‘ç»œèƒ½<strong>æ‹Ÿåˆå¤æ‚å‡½æ•°</strong>ã€‚</li></ul></li><li><strong>æ± åŒ–å±‚</strong>ï¼š<ul><li>å‡å°‘å‚æ•°æ•°é‡ï¼Œ<strong>é˜²æ­¢è¿‡æ‹Ÿåˆ</strong></li><li>ä½¿æ¨¡å‹å¯¹è¾“å…¥çš„å°å¹³ç§»&#x2F;å½¢å˜æ›´é²æ£’ï¼ˆâ€œè¿‘ä¼¼ä¸å˜æ€§â€ï¼‰ã€‚</li></ul></li></ol><ul><li>There are convolutional layers for addressing 1D, 2D, and 3D tensors. There are also many more optional arguments for a conv layer constructor, including <code>stride length</code>(e.g., only <strong>scanning every second or every third position</strong>) in the input, <code>padding</code> (so you can <strong>scan out to the edges of the input</strong>), and more. See theÂ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#convolution-layers">documentation</a>Â for more information.</li></ul><hr><h2 id="Recurrent-Layers"><a href="#Recurrent-Layers" class="headerlink" title="Recurrent Layers"></a>Recurrent Layers</h2><ul><li><em>Recurrent neural networks</em>Â (orÂ <em>RNNs)</em>Â are used for <code>sequential data</code> - anything from <strong>time-series measurements</strong> from a scientific instrument to natural language sentences to DNA nucleotides. An RNN does this by maintaining aÂ <em>hidden state</em>Â that acts as a sort of memory for what it has seen in the sequence so far.</li><li>The internal structure of an RNN layer - or its variants, the <code>LSTM</code> (long short-term memory) and <code>GRU</code> (gated recurrent unit) - is moderately complex and beyond the scope of this video, but weâ€™ll show you what one looks like in action with an <code>LSTM-based part-of-speech tagger</code> (a type of classifier that tells you if a word is a noun, verb, etc.):</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LSTMTagger</span>(torch.nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, embedding_dim, hidden_dim, vocab_size, tagset_size</span>):<br>        <span class="hljs-built_in">super</span>(LSTMTagger, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.hidden_dim = hidden_dim<br><br>        <span class="hljs-variable language_">self</span>.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)<br><br>        <span class="hljs-comment"># The LSTM takes word embeddings as inputs, and outputs hidden states</span><br>        <span class="hljs-comment"># with dimensionality hidden_dim.</span><br>        <span class="hljs-variable language_">self</span>.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)<br><br>        <span class="hljs-comment"># The linear layer that maps from hidden state space to tag space</span><br>        <span class="hljs-variable language_">self</span>.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sentence</span>):<br>        embeds = <span class="hljs-variable language_">self</span>.word_embeddings(sentence)<br>        lstm_out, _ = <span class="hljs-variable language_">self</span>.lstm(embeds.view(<span class="hljs-built_in">len</span>(sentence), <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))<br>        tag_space = <span class="hljs-variable language_">self</span>.hidden2tag(lstm_out.view(<span class="hljs-built_in">len</span>(sentence), -<span class="hljs-number">1</span>))<br>        tag_scores = F.log_softmax(tag_space, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> tag_scores<br></code></pre></td></tr></table></figure><ul><li><p>The constructor has four arguments:</p></li><li><p><code>vocab_size</code>Â is the number of words in the input vocabulary. Each word is a <code>one-hot vector</code> (or unit vector) in aÂ <code>vocab_size</code>-dimensional space.</p></li><li><p><code>tagset_size</code>Â is the number of tags in the output set.</p></li><li><p><code>embedding_dim</code>Â is theÂ sizeÂ of theÂ <em>embedding</em>Â space for the vocabulary. An embedding <strong>maps a vocabulary onto a low-dimensional space</strong>, where words with similar meanings are close together in the space.</p></li><li><p><code>hidden_dim</code>Â is theÂ sizeÂ of the LSTMâ€™s memory.</p></li></ul><p>The input will be a sentence with the <strong>words represented as indices of one-hot vectors</strong>. The embedding layer will then map these down to anÂ <code>embedding_dim</code>-dimensional space. The LSTM takes this sequence of embeddings and iterates over it, fielding an output vector of lengthÂ <code>hidden_dim</code>. The final linear layer <strong>acts as a classifier</strong>; applyingÂ <code>log_softmax()</code>Â to the output of the final layer converts the output into <strong>a normalized set of estimated probabilities</strong> that a given word maps to a given tag.</p><p>If youâ€™d like to see this network in action, check out theÂ <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html">Sequence Models and LSTM Networks</a>Â tutorial on pytorch.org.</p><hr><h2 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h2><p><em>Transformers</em>Â are multi-purpose networks that have taken over the state of the art in NLP with models like <code>BERT</code>. A discussion of transformer architecture is beyond the scope of this video, but PyTorch has aÂ <code>Transformer</code>Â class that allows you to <strong>define the overall parameters of a transformer model</strong> - the number of attention heads, the number of encoder &amp; decoder layers, dropout and activation functions, etc. (You can even build the BERT model from this single class, with the right parameters!) TheÂ <code>torch.nn.Transformer</code>Â class also has classes to encapsulate the individual components (<code>TransformerEncoder</code>,Â <code>TransformerDecoder</code>) and subcomponents (<code>TransformerEncoderLayer</code>,Â <code>TransformerDecoderLayer</code>). For details, check out theÂ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#transformer">documentation</a>Â on transformer classes, and the relevantÂ <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">tutorial</a>Â on pytorch.org.</p><h1 id="Other-Layers-and-Functions"><a href="#Other-Layers-and-Functions" class="headerlink" title="Other Layers and Functions"></a>Other Layers and Functions</h1><h2 id="Data-Manipulation-Layers"><a href="#Data-Manipulation-Layers" class="headerlink" title="Data Manipulation Layers"></a>Data Manipulation Layers</h2><p>There are other layer types that perform important functions in models, but donâ€™t participate in the learning process themselves.</p><h3 id="Max-pooling"><a href="#Max-pooling" class="headerlink" title="Max pooling"></a>Max pooling</h3><p><code>Max pooling</code>Â (and its twin, <code>min pooling</code>) <strong>reduce a tensor by combining cells, and assigning the maximum value</strong> of the input cells to the output cell. (We saw this ) For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">my_tensor = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>)<br><span class="hljs-built_in">print</span>(my_tensor)<br>maxpool_layer = torch.nn.MaxPool2d(<span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(maxpool_layer(my_tensor))<br><br>tensor([[[<span class="hljs-number">0.8160</span>, <span class="hljs-number">0.1406</span>, <span class="hljs-number">0.5950</span>, <span class="hljs-number">0.0883</span>, <span class="hljs-number">0.5464</span>, <span class="hljs-number">0.3993</span>],<br>         [<span class="hljs-number">0.0623</span>, <span class="hljs-number">0.6626</span>, <span class="hljs-number">0.3991</span>, <span class="hljs-number">0.4878</span>, <span class="hljs-number">0.7548</span>, <span class="hljs-number">0.2426</span>],<br>         [<span class="hljs-number">0.9081</span>, <span class="hljs-number">0.4207</span>, <span class="hljs-number">0.8590</span>, <span class="hljs-number">0.3784</span>, <span class="hljs-number">0.6931</span>, <span class="hljs-number">0.5609</span>],<br>         [<span class="hljs-number">0.6182</span>, <span class="hljs-number">0.8588</span>, <span class="hljs-number">0.3766</span>, <span class="hljs-number">0.9734</span>, <span class="hljs-number">0.9662</span>, <span class="hljs-number">0.9880</span>],<br>         [<span class="hljs-number">0.0599</span>, <span class="hljs-number">0.8338</span>, <span class="hljs-number">0.6750</span>, <span class="hljs-number">0.0829</span>, <span class="hljs-number">0.3554</span>, <span class="hljs-number">0.3998</span>],<br>         [<span class="hljs-number">0.6159</span>, <span class="hljs-number">0.7129</span>, <span class="hljs-number">0.8945</span>, <span class="hljs-number">0.8717</span>, <span class="hljs-number">0.9930</span>, <span class="hljs-number">0.9059</span>]]])<br>tensor([[[<span class="hljs-number">0.9081</span>, <span class="hljs-number">0.7548</span>],<br>         [<span class="hljs-number">0.8945</span>, <span class="hljs-number">0.9930</span>]]])<br></code></pre></td></tr></table></figure><p>If you look closely at the values above, youâ€™ll see that each of the values in the maxpooled output is the maximum value of each quadrant of the 6x6 input.</p><h3 id="Normalization-layers"><a href="#Normalization-layers" class="headerlink" title="Normalization layers"></a>Normalization layers</h3><p><code>Normalization layers</code>Â <strong>re-center and normalize the output</strong> of one layer before feeding it to another. <strong>Centering and scaling</strong> the intermediate tensors has a number of beneficial effects, such as letting you <strong>use higher learning rates without exploding&#x2F;vanishing gradients</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">my_tensor = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>) * <span class="hljs-number">20</span> + <span class="hljs-number">5</span><br><span class="hljs-built_in">print</span>(my_tensor)<br><span class="hljs-built_in">print</span>(my_tensor.mean())<br>norm_layer = torch.nn.BatchNorm1d(<span class="hljs-number">4</span>)<br>normed_tensor = norm_layer(my_tensor)<br><span class="hljs-built_in">print</span>(normed_tensor)<br><span class="hljs-built_in">print</span>(normed_tensor.mean())<br><br>tensor([[[<span class="hljs-number">18.0634</span>,  <span class="hljs-number">5.6720</span>,  <span class="hljs-number">5.7805</span>, <span class="hljs-number">12.3243</span>],<br>         [ <span class="hljs-number">9.3712</span>, <span class="hljs-number">19.7366</span>,  <span class="hljs-number">6.4853</span>, <span class="hljs-number">22.8629</span>],<br>         [<span class="hljs-number">14.6223</span>, <span class="hljs-number">21.5803</span>, <span class="hljs-number">17.8267</span>, <span class="hljs-number">20.3997</span>],<br>         [<span class="hljs-number">21.7664</span>,  <span class="hljs-number">5.0936</span>, <span class="hljs-number">19.5952</span>, <span class="hljs-number">11.8554</span>]]])<br>tensor(<span class="hljs-number">14.5647</span>)<br>tensor([[[ <span class="hljs-number">1.4762</span>, -<span class="hljs-number">0.9296</span>, -<span class="hljs-number">0.9086</span>,  <span class="hljs-number">0.3619</span>],<br>         [-<span class="hljs-number">0.7650</span>,  <span class="hljs-number">0.7475</span>, -<span class="hljs-number">1.1862</span>,  <span class="hljs-number">1.2037</span>],<br>         [-<span class="hljs-number">1.4918</span>,  <span class="hljs-number">1.1130</span>, -<span class="hljs-number">0.2922</span>,  <span class="hljs-number">0.6710</span>],<br>         [ <span class="hljs-number">1.0893</span>, -<span class="hljs-number">1.4371</span>,  <span class="hljs-number">0.7603</span>, -<span class="hljs-number">0.4125</span>]]],<br>       grad_fn=&lt;NativeBatchNormBackward&gt;)<br>tensor(<span class="hljs-number">1.3039e-08</span>, grad_fn=&lt;MeanBackward0&gt;)<br></code></pre></td></tr></table></figure><p>Running the cell above, weâ€™ve added <strong>a large scaling factor and offset</strong> to an input tensor; you should see the input tensorâ€™sÂ <code>mean()</code>Â somewhere in the neighborhood of 15. After running it through the normalization layer, you can see that <strong>the values are smaller, and grouped around zero</strong> - in fact, the mean should be very small (&gt; 1e-8).</p><p>This is beneficial because many activation functions (discussed below) have their strongest gradients near 0, but sometimes <strong>suffer from</strong> vanishing or exploding gradients for inputs that <strong>drive them far away from zero</strong>. Keeping the data centered around the area of steepest gradient will tend to mean <strong>faster, better learning and higher feasible learning rates</strong>.</p><h3 id="Dropout-layers"><a href="#Dropout-layers" class="headerlink" title="Dropout layers"></a>Dropout layers</h3><p><strong>Dropout layers</strong>Â are a tool for encouragingÂ <em>sparse representations</em> ç¨€ç–è¡¨ç¤ºÂ in your model - that is, pushing it to do inference with less data.</p><p>Dropout layers work by <strong>randomly setting</strong> parts of the input tensorÂ <strong>zero</strong> <em>during training</em>Â - dropout layers are always turned off for inference æ¨ç†. This forces the model to learn against this <strong>masked or reduced dataset</strong>. For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">my_tensor = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br>dropout = torch.nn.Dropout(p=<span class="hljs-number">0.4</span>)<br><span class="hljs-built_in">print</span>(dropout(my_tensor))<br><span class="hljs-built_in">print</span>(dropout(my_tensor))<br><br>tensor([[[<span class="hljs-number">0.0000</span>, <span class="hljs-number">1.1702</span>, <span class="hljs-number">0.5911</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.1932</span>, <span class="hljs-number">1.4928</span>, <span class="hljs-number">1.2912</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.1236</span>, <span class="hljs-number">1.3672</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>]]])<br>tensor([[[<span class="hljs-number">1.5033</span>, <span class="hljs-number">1.1702</span>, <span class="hljs-number">0.5911</span>, <span class="hljs-number">0.9341</span>],<br>         [<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">1.5020</span>],<br>         [<span class="hljs-number">0.1236</span>, <span class="hljs-number">1.3672</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],<br>         [<span class="hljs-number">0.4993</span>, <span class="hljs-number">0.9576</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">1.6664</span>]]])<br></code></pre></td></tr></table></figure><p>Above, you can see the effect of dropout on a sample tensor. You can use the optionalÂ <code>p</code>Â argument to set the probability of an individual weight dropping out; if you donâ€™t it defaults to 0.5.</p><hr><h2 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h2><p>Activation functions make deep learning possible. <strong>A neural network is really a program</strong> - with many parameters - thatÂ <em>simulatesÂ a mathematical function</em>. If all we did was multiple tensors by layer weights repeatedly, we could onlyÂ simulateÂ <em>linear functions;</em>Â further, there would be no point to having many layers, as the whole network could be reduced to a single matrix multiplication. InsertingÂ <em>non-linear</em>Â activation functions between layers is what allows a deep learning model toÂ <strong>simulateÂ any function, rather than just linear ones.</strong></p><p><code>torch.nn.Module</code>Â has objects encapsulating å°è£… all of the major activation functions including <code>ReLU</code> and its many variants, <code>Tanh</code>, <code>Hardtanh</code>, <code>sigmoid</code>, and more. It also includes other functions, such as <code>Softmax</code>, that are most useful at the output stage of a model.</p><hr><h2 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h2><p>Loss functions tell us how far a modelâ€™s prediction is from the correct answer. PyTorch contains a variety of loss functions, including common <code>MSE</code> (mean squared error &#x3D; L2 norm), <code>Cross Entropy Loss</code> and <code>Negative Likelihood Loss</code> (useful for classifiers), and others.</p><h1 id="Advanced-Replacing-Layers"><a href="#Advanced-Replacing-Layers" class="headerlink" title="Advanced: Replacing Layers"></a>Advanced: Replacing Layers</h1><ul><li>waiting to be updatedğŸ˜€</li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Pytorch/" class="category-chain-item">Pytorch</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Pytorch/" class="print-no-link">#Pytorch</a> <a href="/tags/Python/" class="print-no-link">#Python</a> <a href="/tags/ML/" class="print-no-link">#ML</a></div></div><div class="license-box my-3"><div class="license-title"><div>Pytorch Tutorial-Building models</div><div>http://pzhwuhu.github.io/2025/08/12/Building models/</div></div><div class="license-meta"><div class="license-meta-item"><div>æœ¬æ–‡ä½œè€…</div><div>pzhwuhu</div></div><div class="license-meta-item license-meta-date"><div>å‘å¸ƒäº</div><div>2025å¹´8æœˆ12æ—¥</div></div><div class="license-meta-item license-meta-date"><div>æ›´æ–°äº</div><div>2025å¹´8æœˆ12æ—¥</div></div><div class="license-meta-item"><div>è®¸å¯åè®®</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - ç½²å"><i class="iconfont icon-cc-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - éå•†ä¸šæ€§ä½¿ç”¨"><i class="iconfont icon-cc-nc"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - ç›¸åŒæ–¹å¼å…±äº«"><i class="iconfont icon-cc-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2025/08/12/TensorBoard/" title="Pytorch Tutorial-TensorBoard"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Pytorch Tutorial-TensorBoard</span> <span class="visible-mobile">ä¸Šä¸€ç¯‡</span></a></article><article class="post-next col-6"><a href="/2025/08/12/Autograd/" title="Pytorch Tutorial-Autograd"><span class="hidden-mobile">Pytorch Tutorial-Autograd</span> <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script>Fluid.utils.loadComments("#valine",(function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",(function(){var i=Object.assign({appId:"cp3oK6SJdvk7QCx0TiMpNSdI-gzGzoHsz",appKey:"B9u5rSWtXwzrQl56fuCE0o9M",path:"window.location.pathname",placeholder:"ç•™ä¸‹ä½ çš„è¶³è¿¹å§ï¼Œè¶…å¤šemojiå¯ç”¨å™¢ï¼Œqqé‚®ç®±è‡ªåŠ¨è·å–å¤´åƒ",avatar:"robohash",meta:["nick","mail","link"],requiredFields:["nick"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://valine-emoji.bili33.top/",emojiMaps:{bilibilitv2:"bilibilitv/[tv_doge].png",bilibilitv3:"bilibilitv/[tv_äº²äº²].png",bilibilitv4:"bilibilitv/[tv_å·ç¬‘].png",bilibilitv5:"bilibilitv/[tv_å†è§].png",bilibilitv6:"bilibilitv/[tv_å†·æ¼ ].png",bilibilitv7:"bilibilitv/[tv_å‘æ€’].png",bilibilitv8:"bilibilitv/[tv_å‘è´¢].png",bilibilitv9:"bilibilitv/[tv_å¯çˆ±].png",bilibilitv10:"bilibilitv/[tv_åè¡€].png",bilibilitv11:"bilibilitv/[tv_å‘†].png",bilibilitv12:"bilibilitv/[tv_å‘•å].png",bilibilitv13:"bilibilitv/[tv_å›°].png",bilibilitv14:"bilibilitv/[tv_åç¬‘].png",bilibilitv15:"bilibilitv/[tv_å¤§ä½¬].png",bilibilitv16:"bilibilitv/[tv_å¤§å“­].png",bilibilitv17:"bilibilitv/[tv_å§”å±ˆ].png",bilibilitv18:"bilibilitv/[tv_å®³ç¾].png",bilibilitv19:"bilibilitv/[tv_å°´å°¬].png",bilibilitv20:"bilibilitv/[tv_å¾®ç¬‘].png",bilibilitv21:"bilibilitv/[tv_æ€è€ƒ].png",bilibilitv22:"bilibilitv/[tv_æƒŠå“].png",bilibilitv23:"bilibilitv/[tv_æ‰“è„¸].png",bilibilitv24:"bilibilitv/[tv_æŠ“ç‹‚].png",bilibilitv25:"bilibilitv/[tv_æŠ é¼»].png",bilibilitv26:"bilibilitv/[tv_æ–œçœ¼ç¬‘].png",bilibilitv27:"bilibilitv/[tv_æ— å¥ˆ].png",bilibilitv28:"bilibilitv/[tv_æ™•].png",bilibilitv29:"bilibilitv/[tv_æµæ±—].png",bilibilitv30:"bilibilitv/[tv_æµæ³ª].png",bilibilitv31:"bilibilitv/[tv_æµé¼»è¡€].png",bilibilitv32:"bilibilitv/[tv_ç‚¹èµ].png",bilibilitv33:"bilibilitv/[tv_ç”Ÿæ°”].png",bilibilitv34:"bilibilitv/[tv_ç”Ÿç—…].png",bilibilitv35:"bilibilitv/[tv_ç–‘é—®].png",bilibilitv36:"bilibilitv/[tv_ç™½çœ¼].png",bilibilitv37:"bilibilitv/[tv_çš±çœ‰].png",bilibilitv38:"bilibilitv/[tv_ç›®çªå£å‘†].png",bilibilitv39:"bilibilitv/[tv_ç¡ç€].png",bilibilitv40:"bilibilitv/[tv_ç¬‘å“­].png",bilibilitv41:"bilibilitv/[tv_è…¼è…†].png",bilibilitv42:"bilibilitv/[tv_è‰²].png",bilibilitv43:"bilibilitv/[tv_è°ƒä¾ƒ].png",bilibilitv44:"bilibilitv/[tv_è°ƒçš®].png",bilibilitv45:"bilibilitv/[tv_é„™è§†].png",bilibilitv46:"bilibilitv/[tv_é—­å˜´].png",bilibilitv47:"bilibilitv/[tv_éš¾è¿‡].png",bilibilitv48:"bilibilitv/[tv_é¦‹].png",bilibilitv49:"bilibilitv/[tv_é¬¼è„¸].png",bilibilitv50:"bilibilitv/[tv_é»‘äººé—®å·].png",bilibilitv51:"bilibilitv/[tv_é¼“æŒ].png",bilibili22332:"bilibili2233/[2233å¨˜_å–èŒ].png",bilibili22333:"bilibili2233/[2233å¨˜_åƒæƒŠ].png",bilibili22334:"bilibili2233/[2233å¨˜_åé­‚].png",bilibili22335:"bilibili2233/[2233å¨˜_å–æ°´].png",bilibili22336:"bilibili2233/[2233å¨˜_å›°æƒ‘].png",bilibili22337:"bilibili2233/[2233å¨˜_å¤§å“­].png",bilibili22338:"bilibili2233/[2233å¨˜_å¤§ç¬‘].png",bilibili22339:"bilibili2233/[2233å¨˜_å§”å±ˆ].png",bilibili223310:"bilibili2233/[2233å¨˜_æ€’].png",bilibili223311:"bilibili2233/[2233å¨˜_æ— è¨€].png",bilibili223312:"bilibili2233/[2233å¨˜_æ±—].png",bilibili223313:"bilibili2233/[2233å¨˜_ç–‘é—®].png",bilibili223314:"bilibili2233/[2233å¨˜_ç¬¬ä¸€].png",bilibili223315:"bilibili2233/[2233å¨˜_è€¶].png",bilibili223316:"bilibili2233/[2233å¨˜_éƒé—·].png","Tieba-New2":"Tieba-New/image_emoticon.png","Tieba-New3":"Tieba-New/image_emoticon10.png","Tieba-New4":"Tieba-New/image_emoticon100.png","Tieba-New14":"Tieba-New/image_emoticon11.png","Tieba-New31":"Tieba-New/image_emoticon13.png","Tieba-New32":"Tieba-New/image_emoticon14.png","Tieba-New33":"Tieba-New/image_emoticon15.png","Tieba-New34":"Tieba-New/image_emoticon16.png","Tieba-New35":"Tieba-New/image_emoticon17.png","Tieba-New36":"Tieba-New/image_emoticon18.png","Tieba-New37":"Tieba-New/image_emoticon19.png","Tieba-New38":"Tieba-New/image_emoticon2.png","Tieba-New39":"Tieba-New/image_emoticon20.png","Tieba-New40":"Tieba-New/image_emoticon21.png","Tieba-New41":"Tieba-New/image_emoticon22.png","Tieba-New42":"Tieba-New/image_emoticon23.png","Tieba-New43":"Tieba-New/image_emoticon24.png","Tieba-New44":"Tieba-New/image_emoticon25.png","Tieba-New45":"Tieba-New/image_emoticon26.png","Tieba-New46":"Tieba-New/image_emoticon27.png","Tieba-New47":"Tieba-New/image_emoticon28.png","Tieba-New48":"Tieba-New/image_emoticon29.png","Tieba-New49":"Tieba-New/image_emoticon3.png","Tieba-New50":"Tieba-New/image_emoticon30.png","Tieba-New51":"Tieba-New/image_emoticon31.png","Tieba-New52":"Tieba-New/image_emoticon32.png","Tieba-New53":"Tieba-New/image_emoticon33.png","Tieba-New54":"Tieba-New/image_emoticon34.png","Tieba-New55":"Tieba-New/image_emoticon35.png","Tieba-New56":"Tieba-New/image_emoticon36.png","Tieba-New57":"Tieba-New/image_emoticon37.png","Tieba-New58":"Tieba-New/image_emoticon38.png","Tieba-New59":"Tieba-New/image_emoticon39.png","Tieba-New60":"Tieba-New/image_emoticon4.png","Tieba-New61":"Tieba-New/image_emoticon40.png","Tieba-New62":"Tieba-New/image_emoticon41.png","Tieba-New63":"Tieba-New/image_emoticon42.png","Tieba-New64":"Tieba-New/image_emoticon43.png","Tieba-New65":"Tieba-New/image_emoticon44.png","Tieba-New66":"Tieba-New/image_emoticon45.png","Tieba-New67":"Tieba-New/image_emoticon46.png","Tieba-New68":"Tieba-New/image_emoticon47.png","Tieba-New69":"Tieba-New/image_emoticon48.png","Tieba-New70":"Tieba-New/image_emoticon49.png","Tieba-New71":"Tieba-New/image_emoticon5.png","Tieba-New72":"Tieba-New/image_emoticon50.png","Tieba-New73":"Tieba-New/image_emoticon6.png","Tieba-New74":"Tieba-New/image_emoticon66.png","Tieba-New75":"Tieba-New/image_emoticon67.png","Tieba-New76":"Tieba-New/image_emoticon68.png","Tieba-New77":"Tieba-New/image_emoticon69.png","Tieba-New78":"Tieba-New/image_emoticon7.png","Tieba-New79":"Tieba-New/image_emoticon70.png","Tieba-New80":"Tieba-New/image_emoticon71.png","Tieba-New81":"Tieba-New/image_emoticon72.png","Tieba-New82":"Tieba-New/image_emoticon73.png","Tieba-New83":"Tieba-New/image_emoticon74.png","Tieba-New84":"Tieba-New/image_emoticon75.png","Tieba-New85":"Tieba-New/image_emoticon76.png","Tieba-New86":"Tieba-New/image_emoticon77.png","Tieba-New87":"Tieba-New/image_emoticon78.png","Tieba-New88":"Tieba-New/image_emoticon79.png","Tieba-New89":"Tieba-New/image_emoticon8.png","Tieba-New90":"Tieba-New/image_emoticon80.png","Tieba-New91":"Tieba-New/image_emoticon81.png","Tieba-New92":"Tieba-New/image_emoticon82.png","Tieba-New93":"Tieba-New/image_emoticon83.png","Tieba-New94":"Tieba-New/image_emoticon84.png","Tieba-New95":"Tieba-New/image_emoticon85.png","Tieba-New96":"Tieba-New/image_emoticon86.png","Tieba-New97":"Tieba-New/image_emoticon87.png","Tieba-New98":"Tieba-New/image_emoticon88.png","Tieba-New99":"Tieba-New/image_emoticon89.png","Tieba-New100":"Tieba-New/image_emoticon9.png","Tieba-New101":"Tieba-New/image_emoticon90.png","Tieba-New102":"Tieba-New/image_emoticon91.png","Tieba-New103":"Tieba-New/image_emoticon92.png","Tieba-New104":"Tieba-New/image_emoticon93.png","Tieba-New105":"Tieba-New/image_emoticon94.png","Tieba-New106":"Tieba-New/image_emoticon95.png","Tieba-New107":"Tieba-New/image_emoticon96.png","Tieba-New108":"Tieba-New/image_emoticon97.png","Tieba-New109":"Tieba-New/image_emoticon98.png","Tieba-New110":"Tieba-New/image_emoticon99.png",weibo2:"weibo/d_aoteman.png",weibo15:"weibo/d_doge.png",weibo16:"weibo/d_erha.png",weibo40:"weibo/d_miao.png",weibo49:"weibo/d_shenshou.png",weibo65:"weibo/d_xiongmao.png",weibo74:"weibo/d_zhutou.png",weibo75:"weibo/d_zuiyou.png",weibo76:"weibo/emoji_0x1f31f.png",weibo77:"weibo/emoji_0x1f349.png",weibo78:"weibo/emoji_0x1f357.png",weibo79:"weibo/emoji_0x1f384.png",weibo80:"weibo/emoji_0x1f44f.png",weibo81:"weibo/emoji_0x1f47b.png",weibo82:"weibo/emoji_0x1f47f.png",weibo83:"weibo/emoji_0x1f48a.png",weibo84:"weibo/emoji_0x1f4a3.png",weibo85:"weibo/emoji_0x1f4a9.png",weibo86:"weibo/emoji_0x1f631.png",weibo87:"weibo/emoji_0x1f643.png",weibo88:"weibo/emoji_0x1f645.png",weibo89:"weibo/emoji_0x1f648.png",weibo90:"weibo/emoji_0x1f649.png",weibo91:"weibo/emoji_0x1f64a.png",weibo92:"weibo/emoji_0x1f64b.png",weibo93:"weibo/emoji_0x1f64f.png",weibo94:"weibo/emoji_0x1f913.png",weibo95:"weibo/emoji_0x1f917.png",weibo96:"weibo/emoji_0x26a1.png",weibo97:"weibo/h_buyao.png",weibo98:"weibo/h_good.png",weibo99:"weibo/h_haha.png",weibo100:"weibo/h_jiayou.png",weibo101:"weibo/h_lai.png",weibo102:"weibo/h_ok.png",weibo103:"weibo/h_quantou.png",weibo105:"weibo/h_woshou.png",weibo106:"weibo/h_ye.png",weibo107:"weibo/h_zan.png",weibo108:"weibo/h_zuoyi.png","HONKAI3-Star1":"HONKAI3-Star/1.gif","HONKAI3-Star2":"HONKAI3-Star/10.gif","HONKAI3-Star3":"HONKAI3-Star/11.gif","HONKAI3-Star4":"HONKAI3-Star/12.gif","HONKAI3-Star5":"HONKAI3-Star/13.gif","HONKAI3-Star6":"HONKAI3-Star/14.gif","HONKAI3-Star7":"HONKAI3-Star/15.gif","HONKAI3-Star8":"HONKAI3-Star/16.gif","HONKAI3-Star9":"HONKAI3-Star/2.gif","HONKAI3-Star10":"HONKAI3-Star/3.gif","HONKAI3-Star11":"HONKAI3-Star/4.gif","HONKAI3-Star12":"HONKAI3-Star/5.gif","HONKAI3-Star13":"HONKAI3-Star/6.gif","HONKAI3-Star14":"HONKAI3-Star/7.gif","HONKAI3-Star15":"HONKAI3-Star/8.gif","HONKAI3-Star16":"HONKAI3-Star/9.gif","HONKAI3-Daily1":"HONKAI3-Daily/1.gif","HONKAI3-Daily2":"HONKAI3-Daily/10.gif","HONKAI3-Daily3":"HONKAI3-Daily/11.gif","HONKAI3-Daily4":"HONKAI3-Daily/12.gif","HONKAI3-Daily5":"HONKAI3-Daily/13.gif","HONKAI3-Daily6":"HONKAI3-Daily/14.gif","HONKAI3-Daily7":"HONKAI3-Daily/15.gif","HONKAI3-Daily8":"HONKAI3-Daily/16.gif","HONKAI3-Daily9":"HONKAI3-Daily/2.gif","HONKAI3-Daily10":"HONKAI3-Daily/3.gif","HONKAI3-Daily11":"HONKAI3-Daily/4.gif","HONKAI3-Daily12":"HONKAI3-Daily/5.gif","HONKAI3-Daily13":"HONKAI3-Daily/6.gif","HONKAI3-Daily14":"HONKAI3-Daily/7.gif","HONKAI3-Daily15":"HONKAI3-Daily/8.gif","HONKAI3-Daily16":"HONKAI3-Daily/9.gif","Tsuri-me-ju_mimi1":"Tsuri-me-ju_mimi/10753776_key@2x.png","Tsuri-me-ju_mimi2":"Tsuri-me-ju_mimi/10753777_key@2x.png","Tsuri-me-ju_mimi3":"Tsuri-me-ju_mimi/10753778_key@2x.png","Tsuri-me-ju_mimi12":"Tsuri-me-ju_mimi/10753787_key@2x.png","Tsuri-me-ju_mimi13":"Tsuri-me-ju_mimi/10753788_key@2x.png","Tsuri-me-ju_mimi14":"Tsuri-me-ju_mimi/10753789_key@2x.png","Tsuri-me-ju_mimi15":"Tsuri-me-ju_mimi/10753790_key@2x.png","Tsuri-me-ju_mimi16":"Tsuri-me-ju_mimi/10753791_key@2x.png","Tsuri-me-ju_mimi36":"Tsuri-me-ju_mimi/10753811_key@2x.png","Tsuri-me-ju_mimi37":"Tsuri-me-ju_mimi/10753812_key@2x.png","Tsuri-me-ju_mimi38":"Tsuri-me-ju_mimi/10753813_key@2x.png","Tsuri-me-ju_mimi40":"Tsuri-me-ju_mimi/10753815_key@2x.png"},enableQQ:!0,avatar_cdn:"https://cravatar.cn/avatar/",visitor:!0},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vemoji",(()=>{const i=document.createElement("style");i.innerHTML="\n            #valine .vemoji {\n              width: 40px !important;  /* æ ¹æ®éœ€è¦è°ƒæ•´å®½åº¦ */\n              height: 40px !important; /* æ ¹æ®éœ€è¦è°ƒæ•´é«˜åº¦ */\n            }\n          ",document.head.appendChild(i)})),Fluid.utils.waitElementVisible("#valine .vcontent",(()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)}))}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>ç›®å½•</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><script>Fluid.utils.createScript("https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js",(function(){mermaid.initialize({theme:"default"}),Fluid.utils.listenDOMLoaded((function(){Fluid.events.registerRefreshCallback((function(){"mermaid"in window&&mermaid.init()}))}))}))</script><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a><div style="font-size:.85rem"><span id="timeDate">è½½å…¥å¤©æ•°...</span> <span id="times">è½½å…¥æ—¶åˆ†ç§’...</span><script src="/js/duration.js"></script></div></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">æ€»è®¿é—®é‡ <span id="leancloud-site-pv"></span> æ¬¡ </span><span id="leancloud-site-uv-container" style="display:none">æ€»è®¿å®¢æ•° <span id="leancloud-site-uv"></span> äºº</span></div><span>69k</span></div></footer><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach((t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")}))},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicRibbon.min.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/star.min.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/love.min.js"></script><script src="//cdn.jsdelivr.net/npm/highlight.js@11.5.1/styles/monokai.min.css.js"></script><script src="/js/scrollAnimation.js"></script><script src="/js/loading.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div></noscript><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>
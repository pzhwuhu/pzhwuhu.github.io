<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="light"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/avatar_1.jpg"><link rel="icon" href="/img/avatar_1.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="pzhwuhu"><meta name="keywords" content=""><meta name="description" content="è¿™æ˜¯æˆ‘å­¦ä¹ Pytorchæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š             IntroductionIn past sections, weâ€™ve discussed and demonstrated:  Building models with the neural network layers and functions of the torch.nn"><meta property="og:type" content="article"><meta property="og:title" content="Pytorch Tutorial-Training Models"><meta property="og:url" content="http://pzhwuhu.github.io/2025/08/12/Training%20Models/index.html"><meta property="og:site_name" content="é¹å“¥"><meta property="og:description" content="è¿™æ˜¯æˆ‘å­¦ä¹ Pytorchæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š             IntroductionIn past sections, weâ€™ve discussed and demonstrated:  Building models with the neural network layers and functions of the torch.nn"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://pzhwuhu.github.io/myimg/%E5%A4%A7%E9%B1%BC%E6%B5%B7%E6%A3%A03.jpg"><meta property="article:published_time" content="2025-08-12T07:43:00.000Z"><meta property="article:modified_time" content="2025-08-16T02:09:38.879Z"><meta property="article:author" content="pzhwuhu"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Python"><meta property="article:tag" content="ML"><meta property="article:tag" content="SGD"><meta property="article:tag" content="Optimizer"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://pzhwuhu.github.io/myimg/%E5%A4%A7%E9%B1%BC%E6%B5%B7%E6%A3%A03.jpg"><title>Pytorch Tutorial-Training Models - é¹å“¥</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/custom_dark_mode.css"><link rel="stylesheet" href="/css/scroll_animation.css"><link rel="stylesheet" href="/css/mac.css"><link rel="stylesheet" href="/css/loading.css"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/gradient.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"pzhwuhu.github.io",root:"/",version:"1.9.8",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!1,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:{key:null},google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"cp3oK6SJdvk7QCx0TiMpNSdI-gzGzoHsz",app_key:"B9u5rSWtXwzrQl56fuCE0o9M",server_url:"https://cp3ok6sj.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2024-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>if(!Fluid.ctx.dnt){var _hmt=_hmt||[];!function(){var t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?[object Object]";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()}</script><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:90vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>èº«å¦‚èŠ¥å­ï¼Œå¿ƒè—é¡»å¼¥</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>é¦–é¡µ</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>å½’æ¡£</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>åˆ†ç±»</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>æ ‡ç­¾</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>å…³äº</span></a></li><li class="nav-item"><a class="nav-link" href="/links/" target="_self"><i class="iconfont icon-link-fill"></i> <span>å‹é“¾</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url('/myimg/%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%9711.jpg') no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="Pytorch Tutorial-Training Models"></span></div><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> pzhwuhu </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-08-12 15:43" pubdate>2025å¹´8æœˆ12æ—¥ ä¸‹åˆ</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.4k å­— </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 12 åˆ†é’Ÿ </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> æ¬¡</span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Pytorch" id="heading-edf7dd0dd9dbc3b022a1ef1fdb134ea1" role="tab" data-toggle="collapse" href="#collapse-edf7dd0dd9dbc3b022a1ef1fdb134ea1" aria-expanded="true">Pytorch <span class="list-group-count">(5)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-edf7dd0dd9dbc3b022a1ef1fdb134ea1" role="tabpanel" aria-labelledby="heading-edf7dd0dd9dbc3b022a1ef1fdb134ea1"><div class="category-post-list"><a href="/2025/08/12/Autograd/" title="Pytorch Tutorial-Autograd" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-Autograd</span> </a><a href="/2025/08/12/Building%20models/" title="Pytorch Tutorial-Building models" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-Building models</span> </a><a href="/2025/08/12/TensorBoard/" title="Pytorch Tutorial-TensorBoard" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-TensorBoard</span> </a><a href="/2025/08/03/Tensors/" title="Pytorch Tutorial-Tensors" class="list-group-item list-group-item-action"><span class="category-post">Pytorch Tutorial-Tensors</span> </a><a href="/2025/08/12/Training%20Models/" title="Pytorch Tutorial-Training Models" class="list-group-item list-group-item-action active"><span class="category-post">Pytorch Tutorial-Training Models</span></a></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">Pytorch Tutorial-Training Models</h1><div class="markdown-body"><div class="note note-info"><p>è¿™æ˜¯æˆ‘å­¦ä¹ Pytorchæ—¶è®°å½•çš„ä¸€äº›ç¬”è®° ï¼Œå¸Œæœ›èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ğŸ˜Š</p></div><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>In past sections, weâ€™ve discussed and demonstrated:</p><ul><li>Building models with the <code>neural network layers</code> and functions of the <code>torch.nn module</code></li><li>The mechanics of <code>automated gradient computation</code>, which is central to gradient-based model training</li><li>Using <code>TensorBoard</code> to visualize training progress and other activities</li></ul><p>In this section, weâ€™ll be adding some new tools to your inventory:</p><ul><li>Weâ€™ll get familiar with the <code>dataset and dataloader</code> abstractions, and how they ease the process of feeding data to your model during a training loop</li><li>Weâ€™ll discuss specific <code>loss functions</code> and when to use them</li><li>Weâ€™ll look at <code>PyTorch optimizers</code>, which implement algorithms to adjust model weights based on the outcome of a loss function</li></ul><p>Finally, weâ€™ll pull all of these together and see a full PyTorch training loop in action.</p><h1 id="Dataset-and-DataLoader"><a href="#Dataset-and-DataLoader" class="headerlink" title="Dataset and DataLoader"></a>Dataset and DataLoader<a target="_blank" rel="noopener" href="http://localhost:8888/notebooks/Video+6+-+Model+Training+with+PyTorch.ipynb#Dataset-and-DataLoader"></a></h1><p>TheÂ <code>Dataset</code>Â andÂ <code>DataLoader</code>Â classes encapsulate the process of pulling your data from storage and exposing it to your training loop in batches.</p><p>TheÂ <code>Dataset</code>Â is responsible for accessing and processing single instances of data.</p><p>TheÂ <code>DataLoader</code>Â pulls instances of data from theÂ <code>Dataset</code>Â (either automatically or with a sampler that you define), collects them in batches, and returns them <strong>for consumption by your training</strong> loop. TheÂ <code>DataLoader</code>Â works with all kinds of datasets, regardless of the type of data they contain.</p><p>For this tutorial, weâ€™ll be using the Fashion-MNIST dataset provided by TorchVision.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><br><span class="hljs-comment"># PyTorch TensorBoard support</span><br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime<br><br>transform = transforms.Compose(<br>    [transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.5</span>,), (<span class="hljs-number">0.5</span>,))])<br><br><span class="hljs-comment"># Create datasets for training &amp; validation, download if necessary</span><br>training_set = torchvision.datasets.FashionMNIST(<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">True</span>, transform=transform, download=<span class="hljs-literal">True</span>)<br>validation_set = torchvision.datasets.FashionMNIST(<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=transform, download=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># Create data loaders for our datasets; shuffle for training, not for validation</span><br>training_loader = torch.utils.data.DataLoader(training_set, batch_size=<span class="hljs-number">4</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=<span class="hljs-number">4</span>, shuffle=<span class="hljs-literal">False</span>, num_workers=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># Class labels</span><br>classes = (<span class="hljs-string">&#x27;T-shirt/top&#x27;</span>, <span class="hljs-string">&#x27;Trouser&#x27;</span>, <span class="hljs-string">&#x27;Pullover&#x27;</span>, <span class="hljs-string">&#x27;Dress&#x27;</span>, <span class="hljs-string">&#x27;Coat&#x27;</span>,<br>        <span class="hljs-string">&#x27;Sandal&#x27;</span>, <span class="hljs-string">&#x27;Shirt&#x27;</span>, <span class="hljs-string">&#x27;Sneaker&#x27;</span>, <span class="hljs-string">&#x27;Bag&#x27;</span>, <span class="hljs-string">&#x27;Ankle Boot&#x27;</span>)<br><br><span class="hljs-comment"># Report split sizes</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Training set has &#123;&#125; instances&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(training_set)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Validation set has &#123;&#125; instances&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(validation_set)))<br>Training <span class="hljs-built_in">set</span> has <span class="hljs-number">60000</span> instances<br>Validation <span class="hljs-built_in">set</span> has <span class="hljs-number">10000</span> instances<br></code></pre></td></tr></table></figure><p><strong>As always, letâ€™s visualize the data as a sanity check:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Helper function for inline image display</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">matplotlib_imshow</span>(<span class="hljs-params">img, one_channel=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-keyword">if</span> one_channel:<br>        img = img.mean(dim=<span class="hljs-number">0</span>)<br>    img = img / <span class="hljs-number">2</span> + <span class="hljs-number">0.5</span>     <span class="hljs-comment"># unnormalize</span><br>    npimg = img.numpy()<br>    <span class="hljs-keyword">if</span> one_channel:<br>        plt.imshow(npimg, cmap=<span class="hljs-string">&quot;Greys&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        plt.imshow(np.transpose(npimg, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<br><br>dataiter = <span class="hljs-built_in">iter</span>(training_loader)<br>images, labels = dataiter.<span class="hljs-built_in">next</span>()<br><br><span class="hljs-comment"># Create a grid from the images and show them</span><br>img_grid = torchvision.utils.make_grid(images)<br>matplotlib_imshow(img_grid, one_channel=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;  &#x27;</span>.join(classes[labels[j]] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)))<br></code></pre></td></tr></table></figure><ul><li><code>make_grid</code> æ˜¯ PyTorch ä¸­ç”¨äºå°†å¤šå¼ å›¾åƒæ‹¼æ¥æˆç½‘æ ¼çš„å·¥å…·ï¼Œå¸¸ç”¨äºå¯è§†åŒ–æ‰¹æ¬¡æ•°æ®æˆ–ç‰¹å¾å›¾</li></ul><h1 id="The-Model"><a href="#The-Model" class="headerlink" title="The Model"></a>The Model</h1><p>The model weâ€™ll use in this example is a <code>variant of LeNet-5</code> - it should be familiar if youâ€™ve watched the previous videos in this series.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-comment"># PyTorch models inherit from torch.nn.Module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GarmentClassifier</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(GarmentClassifier, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.pool = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">120</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv1(x)))<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv2(x)))<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">16</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>)<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br>    <br><br>model = GarmentClassifier()<br></code></pre></td></tr></table></figure><h1 id="Loss-Funtion"><a href="#Loss-Funtion" class="headerlink" title="Loss Funtion"></a>Loss Funtion</h1><p>For this example, weâ€™ll be using a <code>cross-entropy loss</code>. For demonstration purposes, weâ€™ll create batches of <strong>dummy output and label values</strong>, run them through the loss function, and examine the result.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_fn = torch.nn.CrossEntropyLoss()<br><br><span class="hljs-comment"># NB: Loss functions expect data in batches, so we&#x27;re creating batches of 4</span><br><span class="hljs-comment"># Represents the model&#x27;s confidence in each of the 10 classes for a given input</span><br>dummy_outputs = torch.rand(<span class="hljs-number">4</span>, <span class="hljs-number">10</span>)<br><span class="hljs-comment"># Represents the correct class among the 10 being tested</span><br>dummy_labels = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>])<br>    <br><span class="hljs-built_in">print</span>(dummy_outputs)<br><span class="hljs-built_in">print</span>(dummy_labels)<br><br>loss = loss_fn(dummy_outputs, dummy_labels)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Total loss for this batch: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(loss.item()))<br><br>tensor([[<span class="hljs-number">0.7915</span>, <span class="hljs-number">0.4766</span>, <span class="hljs-number">0.3735</span>, <span class="hljs-number">0.5340</span>, <span class="hljs-number">0.0799</span>, <span class="hljs-number">0.9948</span>, <span class="hljs-number">0.1870</span>, <span class="hljs-number">0.0507</span>, <span class="hljs-number">0.1183</span>,<br>         <span class="hljs-number">0.9106</span>],<br>        [<span class="hljs-number">0.9666</span>, <span class="hljs-number">0.3765</span>, <span class="hljs-number">0.4324</span>, <span class="hljs-number">0.7354</span>, <span class="hljs-number">0.1953</span>, <span class="hljs-number">0.8906</span>, <span class="hljs-number">0.6882</span>, <span class="hljs-number">0.1925</span>, <span class="hljs-number">0.7076</span>,<br>         <span class="hljs-number">0.8777</span>],<br>        [<span class="hljs-number">0.4412</span>, <span class="hljs-number">0.0325</span>, <span class="hljs-number">0.4886</span>, <span class="hljs-number">0.9350</span>, <span class="hljs-number">0.9792</span>, <span class="hljs-number">0.5580</span>, <span class="hljs-number">0.6199</span>, <span class="hljs-number">0.2478</span>, <span class="hljs-number">0.3619</span>,<br>         <span class="hljs-number">0.8307</span>],<br>        [<span class="hljs-number">0.3287</span>, <span class="hljs-number">0.8571</span>, <span class="hljs-number">0.6046</span>, <span class="hljs-number">0.6719</span>, <span class="hljs-number">0.5982</span>, <span class="hljs-number">0.0540</span>, <span class="hljs-number">0.7193</span>, <span class="hljs-number">0.4764</span>, <span class="hljs-number">0.7451</span>,<br>         <span class="hljs-number">0.8345</span>]])<br>tensor([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>])<br>Total loss <span class="hljs-keyword">for</span> this batch: <span class="hljs-number">2.196722984313965</span><br></code></pre></td></tr></table></figure><h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><p>For this example, weâ€™ll be using simpleÂ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">stochastic gradient descent</a>Â with momentum.</p><p>It can be instructive to try some variations on this optimization scheme:</p><ul><li><code>Learning rate</code> determines theÂ sizeÂ of the steps the optimizer takes. What does a different learning rate do to the your training results, in terms of <strong>accuracy and convergence time</strong>?</li><li><code>Momentum</code> nudges the optimizer in the direction of strongest gradient over multiple steps. What does changing this value do to your results?</li><li>Try some different optimization algorithms, such as <code>averaged SGD</code>, <code>Adagrad</code>, or <code>Adam</code>. How do your results differ?</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Optimizers specified in the torch.optim package</span><br>optimizer = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><p><code>torch.optim.SGD</code> æ˜¯ PyTorch ä¸­å®ç° <strong>éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descentï¼‰</strong> çš„ä¼˜åŒ–å™¨ï¼Œæ”¯æŒåŸºç¡€çš„ SGD å’Œå¸¦åŠ¨é‡çš„ SGDï¼ˆMomentum SGDï¼‰ã€‚ä»¥ä¸‹æ˜¯å…¶æ ¸å¿ƒå‚æ•°ã€æ•°å­¦åŸç†å’Œä½¿ç”¨æ–¹æ³•ã€‚</p><hr><h2 id="1-æ ¸å¿ƒå‚æ•°"><a href="#1-æ ¸å¿ƒå‚æ•°" class="headerlink" title="1. æ ¸å¿ƒå‚æ•°"></a>1. æ ¸å¿ƒå‚æ•°</h2><table><thead><tr><th>å‚æ•°å</th><th>ç±»å‹</th><th>é»˜è®¤å€¼</th><th>è¯´æ˜</th></tr></thead><tbody><tr><td><code>params</code></td><td>iterable</td><td>-</td><td>å¾…ä¼˜åŒ–çš„æ¨¡å‹å‚æ•°ï¼ˆé€šå¸¸ä¸º <code>model.parameters()</code>ï¼‰ã€‚</td></tr><tr><td><code>lr</code></td><td>float</td><td>-</td><td><strong>å­¦ä¹ ç‡</strong>ï¼ˆå¿…é¡»æŒ‡å®šï¼‰ï¼Œæ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿ã€‚</td></tr><tr><td><code>momentum</code></td><td>float</td><td>0</td><td>åŠ¨é‡å› å­ï¼ˆ0 è¡¨ç¤ºæ™®é€š SGDï¼‰ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘éœ‡è¡ã€‚</td></tr><tr><td><code>dampening</code></td><td>float</td><td>0</td><td>åŠ¨é‡é˜»å°¼ï¼ˆé€šå¸¸ä¸ <code>momentum</code> é…åˆä½¿ç”¨ï¼‰ã€‚</td></tr><tr><td><code>weight_decay</code></td><td>float</td><td>0</td><td>L2 æ­£åˆ™åŒ–ç³»æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ã€‚</td></tr><tr><td><code>nesterov</code></td><td>bool</td><td>False</td><td>æ˜¯å¦å¯ç”¨ <strong>Nesterov åŠ¨é‡</strong>ï¼ˆéœ€ <code>momentum &gt; 0</code>ï¼‰ã€‚</td></tr></tbody></table><h2 id="2-æ•°å­¦åŸç†"><a href="#2-æ•°å­¦åŸç†" class="headerlink" title="2. æ•°å­¦åŸç†"></a>2. æ•°å­¦åŸç†</h2><h3 id="1-æ™®é€š-SGDï¼ˆæ— åŠ¨é‡ï¼‰"><a href="#1-æ™®é€š-SGDï¼ˆæ— åŠ¨é‡ï¼‰" class="headerlink" title="(1) æ™®é€š SGDï¼ˆæ— åŠ¨é‡ï¼‰"></a>(1) æ™®é€š SGDï¼ˆæ— åŠ¨é‡ï¼‰</h3><p>å‚æ•°æ›´æ–°å…¬å¼ï¼š<br>$$<br>\theta_{t+1} &#x3D; \theta_t - \eta \cdot \nabla_\theta J(\theta_t)<br>$$</p><ul><li>$\theta_t$ï¼šç¬¬ $t$ æ­¥çš„å‚æ•°ã€‚</li><li>$\eta$ï¼šå­¦ä¹ ç‡ï¼ˆ<code>lr</code>ï¼‰ã€‚</li><li>$\nabla_\theta J(\theta_t)$ï¼šæŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦ã€‚</li></ul><h3 id="2-å¸¦åŠ¨é‡çš„-SGD"><a href="#2-å¸¦åŠ¨é‡çš„-SGD" class="headerlink" title="(2) å¸¦åŠ¨é‡çš„ SGD"></a>(2) å¸¦åŠ¨é‡çš„ SGD</h3><p>å¼•å…¥åŠ¨é‡é¡¹ $v_t$ï¼š<br>$$<br>\begin{align}v_{t+1} &amp;&#x3D; \mu \cdot v_t + \nabla_\theta J(\theta_t) \<br>\theta_{t+1} &amp;&#x3D; \theta_t - \eta \cdot v_{t+1}\end{align}<br>$$</p><ul><li>$\mu$ï¼šåŠ¨é‡ç³»æ•°ï¼ˆ<code>momentum</code>ï¼Œé€šå¸¸è®¾ä¸º 0.9ï¼‰ã€‚</li><li><strong>åŠ¨é‡é€šè¿‡ç´¯ç§¯å†å²æ¢¯åº¦æ–¹å‘ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶æŠ‘åˆ¶éœ‡è¡</strong>ã€‚</li></ul><h3 id="3-Nesterov-åŠ¨é‡"><a href="#3-Nesterov-åŠ¨é‡" class="headerlink" title="(3) Nesterov åŠ¨é‡"></a>(3) Nesterov åŠ¨é‡</h3><p>åœ¨è®¡ç®—æ¢¯åº¦æ—¶å…ˆè¿›è¡Œâ€œè¯•æ¢æ€§â€æ›´æ–°ï¼š<br>$$<br>v_{t+1} &#x3D; \mu \cdot v_t + \nabla_\theta J(\theta_t - \eta \mu v_t) \<br>\theta_{t+1} &#x3D; \theta_t - \eta \cdot v_{t+1}<br>$$</p><ul><li>ç›¸æ¯”æ™®é€šåŠ¨é‡ï¼ŒNesterov åŠ¨é‡å¯¹æ¢¯åº¦æ–¹å‘æ›´æ•æ„Ÿï¼Œæ”¶æ•›æ›´å¿«ã€‚</li></ul><hr><h2 id="3-é€‚ç”¨åœºæ™¯"><a href="#3-é€‚ç”¨åœºæ™¯" class="headerlink" title="3. é€‚ç”¨åœºæ™¯"></a>3. é€‚ç”¨åœºæ™¯</h2><table><thead><tr><th>åœºæ™¯</th><th>æ¨èé…ç½®</th><th>è¯´æ˜</th></tr></thead><tbody><tr><td><strong>ç®€å•ä»»åŠ¡</strong></td><td><code>lr=0.01</code>, <code>momentum=0</code></td><td>æ•°æ®é‡å°ã€æ¨¡å‹ç®€å•æ—¶ï¼Œæ™®é€š SGD è¶³å¤Ÿã€‚</td></tr><tr><td><strong>æ·±å±‚ç½‘ç»œè®­ç»ƒ</strong></td><td><code>lr=0.1</code>, <code>momentum=0.9</code></td><td>åŠ¨é‡å¸®åŠ©åŠ é€Ÿæ”¶æ•›ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚</td></tr><tr><td><strong>å¯¹æŠ—è®­ç»ƒ</strong></td><td><code>lr=0.01</code>, <code>momentum=0.9</code>, <code>nesterov=True</code></td><td>Nesterov åŠ¨é‡æå‡å¯¹æŠ—æ ·æœ¬ç”Ÿæˆæ•ˆæœã€‚</td></tr><tr><td><strong>ç¨€ç–æ•°æ®</strong></td><td><code>lr=0.001</code>, <code>weight_decay=1e-4</code></td><td>L2 æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆã€‚</td></tr></tbody></table><hr><h2 id="6-ä¸å…¶ä»–ä¼˜åŒ–å™¨å¯¹æ¯”"><a href="#6-ä¸å…¶ä»–ä¼˜åŒ–å™¨å¯¹æ¯”" class="headerlink" title="6. ä¸å…¶ä»–ä¼˜åŒ–å™¨å¯¹æ¯”"></a>6. ä¸å…¶ä»–ä¼˜åŒ–å™¨å¯¹æ¯”</h2><table><thead><tr><th>ä¼˜åŒ–å™¨</th><th>ä¼˜ç‚¹</th><th>ç¼ºç‚¹</th></tr></thead><tbody><tr><td><strong>SGD</strong></td><td>ç†è®ºæ”¶æ•›æ€§å¥½ï¼Œè°ƒå‚ç®€å•ã€‚</td><td>éœ€æ‰‹åŠ¨è°ƒå­¦ä¹ ç‡ï¼Œå¯èƒ½æ”¶æ•›æ…¢ã€‚</td></tr><tr><td><strong>Adam</strong></td><td>è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆå¤§å¤šæ•°ä»»åŠ¡ã€‚</td><td>å¯èƒ½åœ¨æŸäº›ä»»åŠ¡ä¸Šæ³›åŒ–æ€§å·®ã€‚</td></tr><tr><td><strong>RMSprop</strong></td><td>é€‚åˆéå¹³ç¨³ç›®æ ‡ï¼ˆå¦‚ RNNï¼‰ã€‚</td><td>å¯¹è¶…å‚æ•°æ•æ„Ÿã€‚</td></tr></tbody></table><hr><p>é€šè¿‡åˆç†é…ç½® <code>torch.optim.SGD</code>ï¼Œä½ å¯ä»¥åœ¨<strong>è®­ç»ƒé€Ÿåº¦å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡</strong>ã€‚å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œå»ºè®®å°è¯• <code>Adam</code> æˆ– <code>SGD + åŠ¨é‡</code> å¹¶å¯¹æ¯”æ•ˆæœã€‚</p><h1 id="The-Training-Loop"><a href="#The-Training-Loop" class="headerlink" title="The Training Loop"></a>The Training Loop</h1><p>Below, we have a function that performs one training epoch. It enumerates data from the DataLoader, and on each pass of the loop does the following:</p><ul><li>Gets a batch of training data from the DataLoader</li><li>Zeros the optimizerâ€™s gradients</li><li>Performs an inference - that is, gets predictions from the model for an input batch</li><li>Calculates the loss for that set of predictions vs. the labels on the dataset</li><li>Calculates the backward gradients over the learning weights</li><li>Tells the optimizer to perform one learning step - that is, adjust the modelâ€™s learning weights based on the observed gradients for this batch, according to the optimization algorithm we chose</li><li>It reports on the loss for every 1000 batches.</li><li>Finally, it reports the average per-batch loss for the last 1000 batches, for comparison with a validation run</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_one_epoch</span>(<span class="hljs-params">epoch_index, tb_writer</span>):<br>    running_loss = <span class="hljs-number">0.</span><br>    last_loss = <span class="hljs-number">0.</span><br>    <br>    <span class="hljs-comment"># Here, we use enumerate(training_loader) instead of</span><br>    <span class="hljs-comment"># iter(training_loader) so that we can track the batch</span><br>    <span class="hljs-comment"># index and do some intra-epoch reporting</span><br>    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(training_loader):<br>        <span class="hljs-comment"># Every data instance is an input + label pair</span><br>        inputs, labels = data<br>        <br>        <span class="hljs-comment"># Zero your gradients for every batch!</span><br>        optimizer.zero_grad()<br>        <br>        <span class="hljs-comment"># Make predictions for this batch</span><br>        outputs = model(inputs)<br>        <br>        <span class="hljs-comment"># Compute the loss and its gradients</span><br>        loss = loss_fn(outputs, labels)<br>        loss.backward()<br>        <br>        <span class="hljs-comment"># Adjust learning weights</span><br>        optimizer.step()<br>        <br>        <span class="hljs-comment"># Gather data and report</span><br>        running_loss += loss.item()<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">1000</span> == <span class="hljs-number">999</span>:<br>            last_loss = running_loss / <span class="hljs-number">1000</span> <span class="hljs-comment"># loss per batch</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;  batch &#123;&#125; loss: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i + <span class="hljs-number">1</span>, last_loss))<br>            tb_x = epoch_index * <span class="hljs-built_in">len</span>(training_loader) + i + <span class="hljs-number">1</span><br>            tb_writer.add_scalar(<span class="hljs-string">&#x27;Loss/train&#x27;</span>, last_loss, tb_x)<br>            running_loss = <span class="hljs-number">0.</span><br>            <br>    <span class="hljs-keyword">return</span> last_loss<br></code></pre></td></tr></table></figure><h2 id="Per-Epoch-Activity"><a href="#Per-Epoch-Activity" class="headerlink" title="Per-Epoch Activity"></a>Per-Epoch Activity</h2><p>There are a couple of things weâ€™ll want to do once per epoch:</p><ul><li><strong>Perform validation</strong> by checking our relative loss on a set of data that was not used for training, and report this</li><li><strong>Save a copy of the model</strong></li></ul><p>Here, weâ€™ll do our reporting in TensorBoard. This will require going to the command line to start TensorBoard, and opening it in another browser tab.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Initializing in a separate cell so we can easily add more epochs to the same run</span><br>timestamp = datetime.now().strftime(<span class="hljs-string">&#x27;%Y%m%d_%H%M%S&#x27;</span>)<br>writer = SummaryWriter(<span class="hljs-string">&#x27;runs/fashion_trainer_&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(timestamp))<br>epoch_number = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python">EPOCHS = <span class="hljs-number">5</span><br><br>best_vloss = <span class="hljs-number">1_000_000.</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;EPOCH &#123;&#125;:&#x27;</span>.<span class="hljs-built_in">format</span>(epoch_number + <span class="hljs-number">1</span>))<br>    <br>    <span class="hljs-comment"># Make sure gradient tracking is on, and do a pass over the data</span><br>    model.train(<span class="hljs-literal">True</span>)<br>    avg_loss = train_one_epoch(epoch_number, writer)<br>    <br>    <span class="hljs-comment"># We don&#x27;t need gradients on to do reporting</span><br>    model.train(<span class="hljs-literal">False</span>)<br>    <br>    running_vloss = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> i, vdata <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(validation_loader):<br>        vinputs, vlabels = vdata<br>        voutputs = model(vinputs)<br>        vloss = loss_fn(voutputs, vlabels)<br>        running_vloss += vloss<br>    <br>    avg_vloss = running_vloss / (i + <span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;LOSS train &#123;&#125; valid &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(avg_loss, avg_vloss))<br>    <br>    <span class="hljs-comment"># Log the running loss averaged per batch</span><br>    <span class="hljs-comment"># for both training and validation</span><br>    writer.add_scalars(<span class="hljs-string">&#x27;Training vs. Validation Loss&#x27;</span>,<br>                    &#123; <span class="hljs-string">&#x27;Training&#x27;</span> : avg_loss, <span class="hljs-string">&#x27;Validation&#x27;</span> : avg_vloss &#125;,<br>                    epoch_number + <span class="hljs-number">1</span>)<br>    writer.flush()<br>    <br>    <span class="hljs-comment"># Track best performance, and save the model&#x27;s state</span><br>    <span class="hljs-keyword">if</span> avg_vloss &lt; best_vloss:<br>        best_vloss = avg_vloss<br>        model_path = <span class="hljs-string">&#x27;model_&#123;&#125;_&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(timestamp, epoch_number)<br>        torch.save(model.state_dict(), model_path)<br>    <br>    epoch_number += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>To load a saved version of the model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">saved_model = GarmentClassifier()<br>saved_model.load_state_dict(torch.load(PATH))<br></code></pre></td></tr></table></figure><p>Once youâ€™ve loaded the model, itâ€™s ready for whatever you need it for - more training, inference, or analysis.</p><p>Note that if your model has constructor parameters that affect model structure, youâ€™ll need to provide them and configure the model identically to the state in which it was saved.</p><h1 id="Other-Resources"><a href="#Other-Resources" class="headerlink" title="Other Resources"></a>Other Resources</h1><ul><li>Docs on theÂ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html">data utilities</a>, including Dataset and DataLoader, at pytorch.org</li><li>AÂ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-pinning">note on the use of pinned memory</a>Â for GPU training</li><li>Documentation on the datasets available inÂ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torchvision/datasets.html">TorchVision</a>,Â <a target="_blank" rel="noopener" href="https://pytorch.org/text/datasets.html">TorchText</a>, andÂ <a target="_blank" rel="noopener" href="https://pytorch.org/audio/datasets.html">TorchAudio</a></li><li>Documentation on theÂ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#loss-functions">loss functions</a>Â available in PyTorch</li><li>Documentation on theÂ <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch.optim package</a>, which includes optimizers and related tools, such as learning rate scheduling</li><li>A detailedÂ <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">tutorial on saving and loading models</a></li><li>TheÂ <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/">Tutorials section of pytorch.org</a>Â contains tutorials on a broad variety of training tasks, including classification in different domains, generative adversarial networks, reinforcement learning, and more</li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Pytorch/" class="category-chain-item">Pytorch</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Pytorch/" class="print-no-link">#Pytorch</a> <a href="/tags/Python/" class="print-no-link">#Python</a> <a href="/tags/ML/" class="print-no-link">#ML</a> <a href="/tags/SGD/" class="print-no-link">#SGD</a> <a href="/tags/Optimizer/" class="print-no-link">#Optimizer</a></div></div><div class="license-box my-3"><div class="license-title"><div>Pytorch Tutorial-Training Models</div><div>http://pzhwuhu.github.io/2025/08/12/Training Models/</div></div><div class="license-meta"><div class="license-meta-item"><div>æœ¬æ–‡ä½œè€…</div><div>pzhwuhu</div></div><div class="license-meta-item license-meta-date"><div>å‘å¸ƒäº</div><div>2025å¹´8æœˆ12æ—¥</div></div><div class="license-meta-item license-meta-date"><div>æ›´æ–°äº</div><div>2025å¹´8æœˆ16æ—¥</div></div><div class="license-meta-item"><div>è®¸å¯åè®®</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - ç½²å"><i class="iconfont icon-cc-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - éå•†ä¸šæ€§ä½¿ç”¨"><i class="iconfont icon-cc-nc"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - ç›¸åŒæ–¹å¼å…±äº«"><i class="iconfont icon-cc-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2025/08/16/%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%B3%A8%E6%84%8F/" title="æå®æ¯…æœºå™¨å­¦ä¹ -ç±»ç¥ç»ç½‘ç»œè®­ç»ƒ"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">æå®æ¯…æœºå™¨å­¦ä¹ -ç±»ç¥ç»ç½‘ç»œè®­ç»ƒ</span> <span class="visible-mobile">ä¸Šä¸€ç¯‡</span></a></article><article class="post-next col-6"><a href="/2025/08/12/TensorBoard/" title="Pytorch Tutorial-TensorBoard"><span class="hidden-mobile">Pytorch Tutorial-TensorBoard</span> <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script>Fluid.utils.loadComments("#valine",(function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",(function(){var i=Object.assign({appId:"cp3oK6SJdvk7QCx0TiMpNSdI-gzGzoHsz",appKey:"B9u5rSWtXwzrQl56fuCE0o9M",path:"window.location.pathname",placeholder:"ç•™ä¸‹ä½ çš„è¶³è¿¹å§ï¼Œè¶…å¤šemojiå¯ç”¨å™¢ï¼Œgravataré‚®ç®±æˆ–qqé‚®ç®±è‡ªåŠ¨è·å–å¤´åƒ",avatar:"retro",meta:["nick","mail","link"],requiredFields:["nick"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://valine-emoji.bili33.top/",emojiMaps:{bilibilitv2:"bilibilitv/[tv_doge].png",bilibilitv3:"bilibilitv/[tv_äº²äº²].png",bilibilitv4:"bilibilitv/[tv_å·ç¬‘].png",bilibilitv5:"bilibilitv/[tv_å†è§].png",bilibilitv6:"bilibilitv/[tv_å†·æ¼ ].png",bilibilitv7:"bilibilitv/[tv_å‘æ€’].png",bilibilitv8:"bilibilitv/[tv_å‘è´¢].png",bilibilitv9:"bilibilitv/[tv_å¯çˆ±].png",bilibilitv10:"bilibilitv/[tv_åè¡€].png",bilibilitv11:"bilibilitv/[tv_å‘†].png",bilibilitv12:"bilibilitv/[tv_å‘•å].png",bilibilitv13:"bilibilitv/[tv_å›°].png",bilibilitv14:"bilibilitv/[tv_åç¬‘].png",bilibilitv15:"bilibilitv/[tv_å¤§ä½¬].png",bilibilitv16:"bilibilitv/[tv_å¤§å“­].png",bilibilitv17:"bilibilitv/[tv_å§”å±ˆ].png",bilibilitv18:"bilibilitv/[tv_å®³ç¾].png",bilibilitv19:"bilibilitv/[tv_å°´å°¬].png",bilibilitv20:"bilibilitv/[tv_å¾®ç¬‘].png",bilibilitv21:"bilibilitv/[tv_æ€è€ƒ].png",bilibilitv22:"bilibilitv/[tv_æƒŠå“].png",bilibilitv23:"bilibilitv/[tv_æ‰“è„¸].png",bilibilitv24:"bilibilitv/[tv_æŠ“ç‹‚].png",bilibilitv25:"bilibilitv/[tv_æŠ é¼»].png",bilibilitv26:"bilibilitv/[tv_æ–œçœ¼ç¬‘].png",bilibilitv27:"bilibilitv/[tv_æ— å¥ˆ].png",bilibilitv28:"bilibilitv/[tv_æ™•].png",bilibilitv29:"bilibilitv/[tv_æµæ±—].png",bilibilitv30:"bilibilitv/[tv_æµæ³ª].png",bilibilitv31:"bilibilitv/[tv_æµé¼»è¡€].png",bilibilitv32:"bilibilitv/[tv_ç‚¹èµ].png",bilibilitv33:"bilibilitv/[tv_ç”Ÿæ°”].png",bilibilitv34:"bilibilitv/[tv_ç”Ÿç—…].png",bilibilitv35:"bilibilitv/[tv_ç–‘é—®].png",bilibilitv36:"bilibilitv/[tv_ç™½çœ¼].png",bilibilitv37:"bilibilitv/[tv_çš±çœ‰].png",bilibilitv38:"bilibilitv/[tv_ç›®çªå£å‘†].png",bilibilitv39:"bilibilitv/[tv_ç¡ç€].png",bilibilitv40:"bilibilitv/[tv_ç¬‘å“­].png",bilibilitv41:"bilibilitv/[tv_è…¼è…†].png",bilibilitv42:"bilibilitv/[tv_è‰²].png",bilibilitv43:"bilibilitv/[tv_è°ƒä¾ƒ].png",bilibilitv44:"bilibilitv/[tv_è°ƒçš®].png",bilibilitv45:"bilibilitv/[tv_é„™è§†].png",bilibilitv46:"bilibilitv/[tv_é—­å˜´].png",bilibilitv47:"bilibilitv/[tv_éš¾è¿‡].png",bilibilitv48:"bilibilitv/[tv_é¦‹].png",bilibilitv49:"bilibilitv/[tv_é¬¼è„¸].png",bilibilitv50:"bilibilitv/[tv_é»‘äººé—®å·].png",bilibilitv51:"bilibilitv/[tv_é¼“æŒ].png",bilibili22332:"bilibili2233/[2233å¨˜_å–èŒ].png",bilibili22333:"bilibili2233/[2233å¨˜_åƒæƒŠ].png",bilibili22334:"bilibili2233/[2233å¨˜_åé­‚].png",bilibili22335:"bilibili2233/[2233å¨˜_å–æ°´].png",bilibili22336:"bilibili2233/[2233å¨˜_å›°æƒ‘].png",bilibili22337:"bilibili2233/[2233å¨˜_å¤§å“­].png",bilibili22338:"bilibili2233/[2233å¨˜_å¤§ç¬‘].png",bilibili22339:"bilibili2233/[2233å¨˜_å§”å±ˆ].png",bilibili223310:"bilibili2233/[2233å¨˜_æ€’].png",bilibili223311:"bilibili2233/[2233å¨˜_æ— è¨€].png",bilibili223312:"bilibili2233/[2233å¨˜_æ±—].png",bilibili223313:"bilibili2233/[2233å¨˜_ç–‘é—®].png",bilibili223314:"bilibili2233/[2233å¨˜_ç¬¬ä¸€].png",bilibili223315:"bilibili2233/[2233å¨˜_è€¶].png",bilibili223316:"bilibili2233/[2233å¨˜_éƒé—·].png","Tieba-New2":"Tieba-New/image_emoticon.png","Tieba-New3":"Tieba-New/image_emoticon10.png","Tieba-New4":"Tieba-New/image_emoticon100.png","Tieba-New14":"Tieba-New/image_emoticon11.png","Tieba-New31":"Tieba-New/image_emoticon13.png","Tieba-New32":"Tieba-New/image_emoticon14.png","Tieba-New33":"Tieba-New/image_emoticon15.png","Tieba-New34":"Tieba-New/image_emoticon16.png","Tieba-New35":"Tieba-New/image_emoticon17.png","Tieba-New36":"Tieba-New/image_emoticon18.png","Tieba-New37":"Tieba-New/image_emoticon19.png","Tieba-New38":"Tieba-New/image_emoticon2.png","Tieba-New39":"Tieba-New/image_emoticon20.png","Tieba-New40":"Tieba-New/image_emoticon21.png","Tieba-New41":"Tieba-New/image_emoticon22.png","Tieba-New42":"Tieba-New/image_emoticon23.png","Tieba-New43":"Tieba-New/image_emoticon24.png","Tieba-New44":"Tieba-New/image_emoticon25.png","Tieba-New45":"Tieba-New/image_emoticon26.png","Tieba-New46":"Tieba-New/image_emoticon27.png","Tieba-New47":"Tieba-New/image_emoticon28.png","Tieba-New48":"Tieba-New/image_emoticon29.png","Tieba-New49":"Tieba-New/image_emoticon3.png","Tieba-New50":"Tieba-New/image_emoticon30.png","Tieba-New51":"Tieba-New/image_emoticon31.png","Tieba-New52":"Tieba-New/image_emoticon32.png","Tieba-New53":"Tieba-New/image_emoticon33.png","Tieba-New54":"Tieba-New/image_emoticon34.png","Tieba-New55":"Tieba-New/image_emoticon35.png","Tieba-New56":"Tieba-New/image_emoticon36.png","Tieba-New57":"Tieba-New/image_emoticon37.png","Tieba-New58":"Tieba-New/image_emoticon38.png","Tieba-New59":"Tieba-New/image_emoticon39.png","Tieba-New60":"Tieba-New/image_emoticon4.png","Tieba-New61":"Tieba-New/image_emoticon40.png","Tieba-New62":"Tieba-New/image_emoticon41.png","Tieba-New63":"Tieba-New/image_emoticon42.png","Tieba-New64":"Tieba-New/image_emoticon43.png","Tieba-New65":"Tieba-New/image_emoticon44.png","Tieba-New66":"Tieba-New/image_emoticon45.png","Tieba-New67":"Tieba-New/image_emoticon46.png","Tieba-New68":"Tieba-New/image_emoticon47.png","Tieba-New69":"Tieba-New/image_emoticon48.png","Tieba-New70":"Tieba-New/image_emoticon49.png","Tieba-New71":"Tieba-New/image_emoticon5.png","Tieba-New72":"Tieba-New/image_emoticon50.png","Tieba-New73":"Tieba-New/image_emoticon6.png","Tieba-New74":"Tieba-New/image_emoticon66.png","Tieba-New75":"Tieba-New/image_emoticon67.png","Tieba-New76":"Tieba-New/image_emoticon68.png","Tieba-New77":"Tieba-New/image_emoticon69.png","Tieba-New78":"Tieba-New/image_emoticon7.png","Tieba-New79":"Tieba-New/image_emoticon70.png","Tieba-New80":"Tieba-New/image_emoticon71.png","Tieba-New81":"Tieba-New/image_emoticon72.png","Tieba-New82":"Tieba-New/image_emoticon73.png","Tieba-New83":"Tieba-New/image_emoticon74.png","Tieba-New84":"Tieba-New/image_emoticon75.png","Tieba-New85":"Tieba-New/image_emoticon76.png","Tieba-New86":"Tieba-New/image_emoticon77.png","Tieba-New87":"Tieba-New/image_emoticon78.png","Tieba-New88":"Tieba-New/image_emoticon79.png","Tieba-New89":"Tieba-New/image_emoticon8.png","Tieba-New90":"Tieba-New/image_emoticon80.png","Tieba-New91":"Tieba-New/image_emoticon81.png","Tieba-New92":"Tieba-New/image_emoticon82.png","Tieba-New93":"Tieba-New/image_emoticon83.png","Tieba-New94":"Tieba-New/image_emoticon84.png","Tieba-New95":"Tieba-New/image_emoticon85.png","Tieba-New96":"Tieba-New/image_emoticon86.png","Tieba-New97":"Tieba-New/image_emoticon87.png","Tieba-New98":"Tieba-New/image_emoticon88.png","Tieba-New99":"Tieba-New/image_emoticon89.png","Tieba-New100":"Tieba-New/image_emoticon9.png","Tieba-New101":"Tieba-New/image_emoticon90.png","Tieba-New102":"Tieba-New/image_emoticon91.png","Tieba-New103":"Tieba-New/image_emoticon92.png","Tieba-New104":"Tieba-New/image_emoticon93.png","Tieba-New105":"Tieba-New/image_emoticon94.png","Tieba-New106":"Tieba-New/image_emoticon95.png","Tieba-New107":"Tieba-New/image_emoticon96.png","Tieba-New108":"Tieba-New/image_emoticon97.png","Tieba-New109":"Tieba-New/image_emoticon98.png","Tieba-New110":"Tieba-New/image_emoticon99.png",weibo2:"weibo/d_aoteman.png",weibo15:"weibo/d_doge.png",weibo16:"weibo/d_erha.png",weibo40:"weibo/d_miao.png",weibo49:"weibo/d_shenshou.png",weibo65:"weibo/d_xiongmao.png",weibo74:"weibo/d_zhutou.png",weibo75:"weibo/d_zuiyou.png",weibo76:"weibo/emoji_0x1f31f.png",weibo77:"weibo/emoji_0x1f349.png",weibo78:"weibo/emoji_0x1f357.png",weibo79:"weibo/emoji_0x1f384.png",weibo80:"weibo/emoji_0x1f44f.png",weibo81:"weibo/emoji_0x1f47b.png",weibo82:"weibo/emoji_0x1f47f.png",weibo83:"weibo/emoji_0x1f48a.png",weibo84:"weibo/emoji_0x1f4a3.png",weibo85:"weibo/emoji_0x1f4a9.png",weibo86:"weibo/emoji_0x1f631.png",weibo87:"weibo/emoji_0x1f643.png",weibo88:"weibo/emoji_0x1f645.png",weibo89:"weibo/emoji_0x1f648.png",weibo90:"weibo/emoji_0x1f649.png",weibo91:"weibo/emoji_0x1f64a.png",weibo92:"weibo/emoji_0x1f64b.png",weibo93:"weibo/emoji_0x1f64f.png",weibo94:"weibo/emoji_0x1f913.png",weibo95:"weibo/emoji_0x1f917.png",weibo96:"weibo/emoji_0x26a1.png",weibo97:"weibo/h_buyao.png",weibo98:"weibo/h_good.png",weibo99:"weibo/h_haha.png",weibo100:"weibo/h_jiayou.png",weibo101:"weibo/h_lai.png",weibo102:"weibo/h_ok.png",weibo103:"weibo/h_quantou.png",weibo105:"weibo/h_woshou.png",weibo106:"weibo/h_ye.png",weibo107:"weibo/h_zan.png",weibo108:"weibo/h_zuoyi.png","HONKAI3-Star1":"HONKAI3-Star/1.gif","HONKAI3-Star2":"HONKAI3-Star/10.gif","HONKAI3-Star3":"HONKAI3-Star/11.gif","HONKAI3-Star4":"HONKAI3-Star/12.gif","HONKAI3-Star5":"HONKAI3-Star/13.gif","HONKAI3-Star6":"HONKAI3-Star/14.gif","HONKAI3-Star7":"HONKAI3-Star/15.gif","HONKAI3-Star8":"HONKAI3-Star/16.gif","HONKAI3-Star9":"HONKAI3-Star/2.gif","HONKAI3-Star10":"HONKAI3-Star/3.gif","HONKAI3-Star11":"HONKAI3-Star/4.gif","HONKAI3-Star12":"HONKAI3-Star/5.gif","HONKAI3-Star13":"HONKAI3-Star/6.gif","HONKAI3-Star14":"HONKAI3-Star/7.gif","HONKAI3-Star15":"HONKAI3-Star/8.gif","HONKAI3-Star16":"HONKAI3-Star/9.gif","HONKAI3-Daily1":"HONKAI3-Daily/1.gif","HONKAI3-Daily2":"HONKAI3-Daily/10.gif","HONKAI3-Daily3":"HONKAI3-Daily/11.gif","HONKAI3-Daily4":"HONKAI3-Daily/12.gif","HONKAI3-Daily5":"HONKAI3-Daily/13.gif","HONKAI3-Daily6":"HONKAI3-Daily/14.gif","HONKAI3-Daily7":"HONKAI3-Daily/15.gif","HONKAI3-Daily8":"HONKAI3-Daily/16.gif","HONKAI3-Daily9":"HONKAI3-Daily/2.gif","HONKAI3-Daily10":"HONKAI3-Daily/3.gif","HONKAI3-Daily11":"HONKAI3-Daily/4.gif","HONKAI3-Daily12":"HONKAI3-Daily/5.gif","HONKAI3-Daily13":"HONKAI3-Daily/6.gif","HONKAI3-Daily14":"HONKAI3-Daily/7.gif","HONKAI3-Daily15":"HONKAI3-Daily/8.gif","HONKAI3-Daily16":"HONKAI3-Daily/9.gif","Tsuri-me-ju_mimi1":"Tsuri-me-ju_mimi/10753776_key@2x.png","Tsuri-me-ju_mimi2":"Tsuri-me-ju_mimi/10753777_key@2x.png","Tsuri-me-ju_mimi3":"Tsuri-me-ju_mimi/10753778_key@2x.png","Tsuri-me-ju_mimi12":"Tsuri-me-ju_mimi/10753787_key@2x.png","Tsuri-me-ju_mimi13":"Tsuri-me-ju_mimi/10753788_key@2x.png","Tsuri-me-ju_mimi14":"Tsuri-me-ju_mimi/10753789_key@2x.png","Tsuri-me-ju_mimi15":"Tsuri-me-ju_mimi/10753790_key@2x.png","Tsuri-me-ju_mimi16":"Tsuri-me-ju_mimi/10753791_key@2x.png","Tsuri-me-ju_mimi36":"Tsuri-me-ju_mimi/10753811_key@2x.png","Tsuri-me-ju_mimi37":"Tsuri-me-ju_mimi/10753812_key@2x.png","Tsuri-me-ju_mimi38":"Tsuri-me-ju_mimi/10753813_key@2x.png","Tsuri-me-ju_mimi40":"Tsuri-me-ju_mimi/10753815_key@2x.png"},enableQQ:!0,avatar_cdn:"https://cravatar.cn/avatar/",visitor:!0},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vemoji",(()=>{const i=document.createElement("style");i.innerHTML="\n            #valine .vemoji {\n              width: 40px !important;  /* æ ¹æ®éœ€è¦è°ƒæ•´å®½åº¦ */\n              height: 40px !important; /* æ ¹æ®éœ€è¦è°ƒæ•´é«˜åº¦ */\n            }\n          ",document.head.appendChild(i)})),Fluid.utils.waitElementVisible("#valine .vcontent",(()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)}))}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>ç›®å½•</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a><div style="font-size:.85rem"><span id="timeDate">è½½å…¥å¤©æ•°...</span> <span id="times">è½½å…¥æ—¶åˆ†ç§’...</span><script src="/js/duration.js"></script></div></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">æ€»è®¿é—®é‡ <span id="leancloud-site-pv"></span> æ¬¡ </span><span id="leancloud-site-uv-container" style="display:none">æ€»è®¿å®¢æ•° <span id="leancloud-site-uv"></span> äºº</span></div><span>91k</span></div></footer><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach((t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")}))},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicRibbon.min.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/star.min.js"></script><script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/love.min.js"></script><script src="//cdn.jsdelivr.net/npm/highlight.js@11.5.1/styles/monokai.min.css.js"></script><script src="/js/scrollAnimation.js"></script><script src="/js/loading.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div></noscript><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html>